{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM76kssdPK9qhBORkvS8xTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanpharamut/data_viz/blob/main/Project01%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpQybCvMEsri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b70e5b-6cf6-4f13-c66e-56e5bf4ed945"
      },
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/facebookresearch/consistent_depth.git\n",
        "%cd consistent_depth\n",
        "!git submodule update --init --recursive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'consistent_depth'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 57 (delta 0), reused 52 (delta 0), pack-reused 5\u001b[K\n",
            "Unpacking objects: 100% (57/57), done.\n",
            "/content/consistent_depth\n",
            "Submodule 'monodepth/mannequin_challenge' (https://github.com/roxanneluo/mannequinchallenge.git) registered for path 'monodepth/mannequin_challenge'\n",
            "Submodule 'monodepth/midas_v2' (https://github.com/roxanneluo/MiDaS-1.git) registered for path 'monodepth/midas_v2'\n",
            "Submodule 'monodepth/monodepth2' (https://github.com/roxanneluo/monodepth2.git) registered for path 'monodepth/monodepth2'\n",
            "Submodule 'third_party/OpticalFlowToolkit' (https://github.com/roxanneluo/OpticalFlowToolkit.git) registered for path 'third_party/OpticalFlowToolkit'\n",
            "Submodule 'third_party/colmap' (https://github.com/colmap/colmap.git) registered for path 'third_party/colmap'\n",
            "Submodule 'flownet2' (https://github.com/roxanneluo/flownet2-pytorch.git) registered for path 'third_party/flownet2'\n",
            "Cloning into '/content/consistent_depth/monodepth/mannequin_challenge'...\n",
            "Cloning into '/content/consistent_depth/monodepth/midas_v2'...\n",
            "Cloning into '/content/consistent_depth/monodepth/monodepth2'...\n",
            "Cloning into '/content/consistent_depth/third_party/OpticalFlowToolkit'...\n",
            "Cloning into '/content/consistent_depth/third_party/colmap'...\n",
            "Cloning into '/content/consistent_depth/third_party/flownet2'...\n",
            "Submodule path 'monodepth/mannequin_challenge': checked out '826c33d4a9f50eb9bfd4b0eba731d893ccbbfa31'\n",
            "Submodule path 'monodepth/midas_v2': checked out '6cae4a6d3198efae398f07220aae4804c2f517ed'\n",
            "Submodule path 'monodepth/monodepth2': checked out '22d073ab881c1e3a8cfcdd147467d80b1d103327'\n",
            "Submodule path 'third_party/OpticalFlowToolkit': checked out '21e7eccd4f979f56ed4dc6e3ba03a5d90edb9bd7'\n",
            "Submodule path 'third_party/colmap': checked out '64d625a996a052cdf4f3f9151e9a5af7b8d6a5e1'\n",
            "Submodule 'doc/_build/html' (https://github.com/colmap/colmap.github.io.git) registered for path 'third_party/colmap/doc/_build/html'\n",
            "Cloning into '/content/consistent_depth/third_party/colmap/doc/_build/html'...\n",
            "Submodule path 'third_party/colmap/doc/_build/html': checked out '751fd602f81d3bb52b45bc3f4757113c18f3cd00'\n",
            "Submodule path 'third_party/flownet2': checked out 'e99b5cc2b8dde129cb0273ff43501da59fb9731e'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cleNUVwFS0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b87643f4-7f18-48b7-9682-5f7792117036"
      },
      "source": [
        "!./scripts/download_model.sh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "allexport      \toff\n",
            "braceexpand    \ton\n",
            "emacs          \toff\n",
            "errexit        \ton\n",
            "errtrace       \toff\n",
            "functrace      \toff\n",
            "hashall        \ton\n",
            "histexpand     \toff\n",
            "history        \toff\n",
            "ignoreeof      \toff\n",
            "interactive-comments\ton\n",
            "keyword        \toff\n",
            "monitor        \toff\n",
            "noclobber      \toff\n",
            "noexec         \toff\n",
            "noglob         \toff\n",
            "nolog          \toff\n",
            "notify         \toff\n",
            "nounset        \toff\n",
            "onecmd         \toff\n",
            "physical       \toff\n",
            "pipefail       \toff\n",
            "posix          \toff\n",
            "privileged     \toff\n",
            "verbose        \toff\n",
            "vi             \toff\n",
            "xtrace         \ton\n",
            "++ mkdir -p checkpoints\n",
            "++ gdown 'https://drive.google.com/uc?id=1hF8vS6YeHkx3j2pfCeQqqZGwA_PJq_Da' -O checkpoints/flownet2.pth\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hF8vS6YeHkx3j2pfCeQqqZGwA_PJq_Da\n",
            "To: /content/consistent_depth/checkpoints/flownet2.pth\n",
            "650MB [00:06, 108MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA4WsYaYlaah",
        "outputId": "1c89d757-8415-4ceb-9bcd-db710fbdd31a"
      },
      "source": [
        "!./scripts/install.sh"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.17.2)\n",
            "Collecting opencv-contrib-python==3.4.2.16\n",
            "  Downloading opencv_contrib_python-3.4.2.16-cp36-cp36m-manylinux1_x86_64.whl (30.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 30.6 MB 22 kB/s \n",
            "\u001b[?25hCollecting torch==1.4.0+cu100\n",
            "  Downloading https://download.pytorch.org/whl/cu100/torch-1.4.0%2Bcu100-cp36-cp36m-linux_x86_64.whl (723.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 723.9 MB 14 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.5.0+cu100\n",
            "  Downloading https://download.pytorch.org/whl/cu100/torchvision-0.5.0%2Bcu100-cp36-cp36m-linux_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (1.19.2)\n",
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 44.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboardX in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 12)) (1.4)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 14)) (4.59.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.2-cp36-cp36m-manylinux1_x86_64.whl (35 kB)\n",
            "Collecting wget\n",
            "  Using cached wget-3.2.zip (10 kB)\n",
            "Collecting gdown\n",
            "  Downloading gdown-3.13.0.tar.gz (9.3 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pypng\n",
            "  Using cached pypng-0.0.20.tar.gz (649 kB)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/site-packages (from torchvision==0.5.0+cu100->-r requirements.txt (line 7)) (8.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from torchvision==0.5.0+cu100->-r requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.6/site-packages (from h5py->-r requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/site-packages (from scikit-image->-r requirements.txt (line 2)) (2.9.0)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/site-packages (from scikit-image->-r requirements.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.6/site-packages (from scikit-image->-r requirements.txt (line 2)) (2020.9.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.6/site-packages (from scikit-image->-r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/site-packages (from scikit-image->-r requirements.txt (line 2)) (2.5.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/site-packages (from scikit-image->-r requirements.txt (line 2)) (3.3.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 2)) (0.10.0)\n",
            "Collecting decorator<5,>=4.3\n",
            "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 9)) (52.0.0.post20210125)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 9)) (0.36.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 9)) (2.25.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 9)) (3.17.3)\n",
            "Collecting grpcio>=1.24.3\n",
            "  Downloading grpcio-1.38.0-cp36-cp36m-manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 45.9 MB/s \n",
            "\u001b[?25hCollecting absl-py>=0.4\n",
            "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 75.8 MB/s \n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 67.3 MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 64.8 MB/s \n",
            "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.31.0-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 76.2 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 68.3 MB/s \n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 71.7 MB/s \n",
            "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata\n",
            "  Downloading importlib_metadata-4.5.0-py3-none-any.whl (17 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 9)) (1.26.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 9)) (2021.5.30)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 9)) (4.0.0)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 74.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard->-r requirements.txt (line 9)) (0.8)\n",
            "Collecting filelock\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 9)) (1.7.1)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->-r requirements.txt (line 9)) (3.10.0.0)\n",
            "Building wheels for collected packages: wget, gdown, pypng\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=bbd20903d2867b12541721677c159c8f778b42de3597f9b099c72234004ceb41\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/1d/93/c863ee832230df5cfc25ca497b3e88e0ee3ea9e44adc46ac62\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.13.0-py3-none-any.whl size=9034 sha256=e30328c2b0b5ea30c5d826464e58c78527999ce4c99e047c6a00a4b81f3fac61\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/87/bd/09b16161b149fd6711ac76b5420d78ed58bd6a320e892117c3\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-py3-none-any.whl size=67162 sha256=3552c82f0a3886634a9024ca80fa7b36133228d0069fae432f364e864c27d968\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/d3/cf/b82186f85e4c9d159bc4233fbd37607e766c241b78b09f1e8f\n",
            "Successfully built wget gdown pypng\n",
            "Installing collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, decorator, werkzeug, torch, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, filelock, absl-py, wget, torchvision, tensorboard, setproctitle, pypng, opencv-contrib-python, gdown, colorama\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 5.0.9\n",
            "    Uninstalling decorator-5.0.9:\n",
            "      Successfully uninstalled decorator-5.0.9\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0\n",
            "    Uninstalling torch-1.9.0:\n",
            "      Successfully uninstalled torch-1.9.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.2.1\n",
            "    Uninstalling torchvision-0.2.1:\n",
            "      Successfully uninstalled torchvision-0.2.1\n",
            "Successfully installed absl-py-0.13.0 cachetools-4.2.2 colorama-0.4.4 decorator-4.4.2 filelock-3.0.12 gdown-3.13.0 google-auth-1.31.0 google-auth-oauthlib-0.4.4 grpcio-1.38.0 importlib-metadata-4.5.0 markdown-3.3.4 oauthlib-3.1.1 opencv-contrib-python-3.4.2.16 pyasn1-0.4.8 pyasn1-modules-0.2.8 pypng-0.0.20 requests-oauthlib-1.3.0 rsa-4.7.2 setproctitle-1.2.2 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 torch-1.4.0+cu100 torchvision-0.5.0+cu100 werkzeug-2.0.1 wget-3.2 zipp-3.4.1\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "/content/consistent_depth/third_party/flownet2 /content/consistent_depth\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating correlation_cuda.egg-info\n",
            "writing correlation_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to correlation_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to correlation_cuda.egg-info/top_level.txt\n",
            "writing manifest file 'correlation_cuda.egg-info/SOURCES.txt'\n",
            "reading manifest file 'correlation_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'correlation_cuda.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "building 'correlation_cuda' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "gcc -pthread -B /usr/local/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c correlation_cuda.cc -o build/temp.linux-x86_64-3.6/correlation_cuda.o -std=c++11 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=correlation_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c correlation_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/correlation_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=correlation_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:317:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:605:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:632:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:903:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:386:934:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channels_first_fwd_1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:317:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:605:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:632:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:903:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:393:934:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"channels_first_fwd_2\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:328:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:400:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:469:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:748:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:819:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:887:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:1177:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:1252:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:403:1324:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"correlation_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:317:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:605:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:632:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:903:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:495:934:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:317:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:605:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:632:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:903:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:507:934:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:343:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:415:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:487:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:781:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:852:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:923:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:1228:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:1303:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:524:1378:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input2.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:100:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:344:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:416:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:488:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:782:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:853:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:924:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:1229:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:1304:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcorrelation_cuda_kernel.cu:541:1379:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(rInput1.type(), \"lltm_forward_cuda\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "g++ -pthread -shared -B /usr/local/compiler_compat -L/usr/local/lib -Wl,-rpath=/usr/local/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/correlation_cuda.o build/temp.linux-x86_64-3.6/correlation_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/correlation_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/correlation_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for correlation_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/correlation_cuda.py to correlation_cuda.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying correlation_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying correlation_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying correlation_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying correlation_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.correlation_cuda.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/correlation_cuda-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing correlation_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "creating /root/.local/lib/python3.6/site-packages/correlation_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "Extracting correlation_cuda-0.0.0-py3.6-linux-x86_64.egg to /root/.local/lib/python3.6/site-packages\n",
            "Adding correlation-cuda 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /root/.local/lib/python3.6/site-packages/correlation_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for correlation-cuda==0.0.0\n",
            "Finished processing dependencies for correlation-cuda==0.0.0\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating resample2d_cuda.egg-info\n",
            "writing resample2d_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to resample2d_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to resample2d_cuda.egg-info/top_level.txt\n",
            "writing manifest file 'resample2d_cuda.egg-info/SOURCES.txt'\n",
            "reading manifest file 'resample2d_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'resample2d_cuda.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "building 'resample2d_cuda' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "gcc -pthread -B /usr/local/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c resample2d_cuda.cc -o build/temp.linux-x86_64-3.6/resample2d_cuda.o -std=c++11 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=resample2d_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c resample2d_kernel.cu -o build/temp.linux-x86_64-3.6/resample2d_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=resample2d_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid resample2d_kernel_forward(at::Tensor&, at::Tensor&, at::Tensor&, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:221:173:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_update_output<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:221:225:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_update_output<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:221:277:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_update_output<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid resample2d_kernel_backward(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:269:175:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:269:227:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:269:283:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:269:347:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input1<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:298:175:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:298:227:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:298:283:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kresample2d_kernel.cu:298:347:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "         kernel_resample2d_backward_input2<float><<< (n + CUDA_NUM_THREADS - 1)/CUDA_NUM_THREADS, CUDA_NUM_THREADS, 0, at::cuda::getCurrentCUDAStream() >>>(\n",
            "                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "g++ -pthread -shared -B /usr/local/compiler_compat -L/usr/local/lib -Wl,-rpath=/usr/local/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/resample2d_cuda.o build/temp.linux-x86_64-3.6/resample2d_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/resample2d_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/resample2d_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for resample2d_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/resample2d_cuda.py to resample2d_cuda.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying resample2d_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying resample2d_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying resample2d_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying resample2d_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.resample2d_cuda.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "creating /root/.local/lib/python3.6/site-packages/resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "Extracting resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg to /root/.local/lib/python3.6/site-packages\n",
            "Adding resample2d-cuda 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /root/.local/lib/python3.6/site-packages/resample2d_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for resample2d-cuda==0.0.0\n",
            "Finished processing dependencies for resample2d-cuda==0.0.0\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating channelnorm_cuda.egg-info\n",
            "writing channelnorm_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to channelnorm_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to channelnorm_cuda.egg-info/top_level.txt\n",
            "writing manifest file 'channelnorm_cuda.egg-info/SOURCES.txt'\n",
            "reading manifest file 'channelnorm_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'channelnorm_cuda.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "building 'channelnorm_cuda' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "gcc -pthread -B /usr/local/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c channelnorm_cuda.cc -o build/temp.linux-x86_64-3.6/channelnorm_cuda.o -std=c++11 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=channelnorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/site-packages/torch/include -I/usr/local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/site-packages/torch/include/TH -I/usr/local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/local/include/python3.6m -c channelnorm_kernel.cu -o build/temp.linux-x86_64-3.6/channelnorm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=channelnorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:366:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:421:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:717:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:771:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:1078:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:111:1136:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_forward\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:368:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:423:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:482:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:549:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:855:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:909:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:967:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1033:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1350:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1408:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1470:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kchannelnorm_kernel.cu:152:1540:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_DISPATCH_FLOATING_TYPES_AND_HALF(input1.type(), \"channelnorm_backward_input1\", ([&] {\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "g++ -pthread -shared -B /usr/local/compiler_compat -L/usr/local/lib -Wl,-rpath=/usr/local/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/channelnorm_cuda.o build/temp.linux-x86_64-3.6/channelnorm_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/channelnorm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/channelnorm_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for channelnorm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/channelnorm_cuda.py to channelnorm_cuda.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying channelnorm_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying channelnorm_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying channelnorm_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying channelnorm_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.channelnorm_cuda.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "creating /root/.local/lib/python3.6/site-packages/channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "Extracting channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg to /root/.local/lib/python3.6/site-packages\n",
            "Adding channelnorm-cuda 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /root/.local/lib/python3.6/site-packages/channelnorm_cuda-0.0.0-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for channelnorm-cuda==0.0.0\n",
            "Finished processing dependencies for channelnorm-cuda==0.0.0\n",
            "/content/consistent_depth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJgA5KLClipM",
        "outputId": "329ba5b5-e581-4c77-af95-5c0f9f6381e2"
      },
      "source": [
        "!./scripts/download_demo.sh results/ayush"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "allexport      \toff\n",
            "braceexpand    \ton\n",
            "emacs          \toff\n",
            "errexit        \ton\n",
            "errtrace       \toff\n",
            "functrace      \toff\n",
            "hashall        \ton\n",
            "histexpand     \toff\n",
            "history        \toff\n",
            "ignoreeof      \toff\n",
            "interactive-comments\ton\n",
            "keyword        \toff\n",
            "monitor        \toff\n",
            "noclobber      \toff\n",
            "noexec         \toff\n",
            "noglob         \toff\n",
            "nolog          \toff\n",
            "notify         \toff\n",
            "nounset        \toff\n",
            "onecmd         \toff\n",
            "physical       \toff\n",
            "pipefail       \toff\n",
            "posix          \toff\n",
            "privileged     \toff\n",
            "verbose        \toff\n",
            "vi             \toff\n",
            "xtrace         \ton\n",
            "++ results_dir=results/ayush\n",
            "++ mkdir -p data/videos/\n",
            "++ wget 'https://www.dropbox.com/s/9a2kb7flg3o1eb5/ayush_color.mp4?dl=1' -O data/videos/ayush.mp4\n",
            "--2021-06-21 08:41:05--  https://www.dropbox.com/s/9a2kb7flg3o1eb5/ayush_color.mp4?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.4.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.4.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/9a2kb7flg3o1eb5/ayush_color.mp4 [following]\n",
            "--2021-06-21 08:41:06--  https://www.dropbox.com/s/dl/9a2kb7flg3o1eb5/ayush_color.mp4\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb0fe33827c332b1cc039868cba.dl.dropboxusercontent.com/cd/0/get/BQ0hw6rL8vm_rTH5FLfceGGSR4TqWiTtEi9do2p9a2BekBZR_JaZOCbsfkdes6MCTSHCaDVj2Ew8BaLVavDc7LsyRtM4_JuEU0EvJh13Momawx4IU3dD7sHC8cwZxK08ksW0nDfD72-SbYnYKgEMfdT8/file?dl=1# [following]\n",
            "--2021-06-21 08:41:06--  https://ucb0fe33827c332b1cc039868cba.dl.dropboxusercontent.com/cd/0/get/BQ0hw6rL8vm_rTH5FLfceGGSR4TqWiTtEi9do2p9a2BekBZR_JaZOCbsfkdes6MCTSHCaDVj2Ew8BaLVavDc7LsyRtM4_JuEU0EvJh13Momawx4IU3dD7sHC8cwZxK08ksW0nDfD72-SbYnYKgEMfdT8/file?dl=1\n",
            "Resolving ucb0fe33827c332b1cc039868cba.dl.dropboxusercontent.com (ucb0fe33827c332b1cc039868cba.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6032:15::a27d:520f\n",
            "Connecting to ucb0fe33827c332b1cc039868cba.dl.dropboxusercontent.com (ucb0fe33827c332b1cc039868cba.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52605435 (50M) [application/binary]\n",
            "Saving to: ‘data/videos/ayush.mp4’\n",
            "\n",
            "data/videos/ayush.m 100%[===================>]  50.17M  15.5MB/s    in 3.2s    \n",
            "\n",
            "2021-06-21 08:41:11 (15.5 MB/s) - ‘data/videos/ayush.mp4’ saved [52605435/52605435]\n",
            "\n",
            "++ mkdir -p results/ayush\n",
            "++ wget 'https://www.dropbox.com/s/7mbvu60qbs7hzod/ayush_colmap.zip?dl=1' -O results/ayush/ayush_colmap.zip\n",
            "--2021-06-21 08:41:11--  https://www.dropbox.com/s/7mbvu60qbs7hzod/ayush_colmap.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/7mbvu60qbs7hzod/ayush_colmap.zip [following]\n",
            "--2021-06-21 08:41:11--  https://www.dropbox.com/s/dl/7mbvu60qbs7hzod/ayush_colmap.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc52616e03169e18ca41ab02b9d6.dl.dropboxusercontent.com/cd/0/get/BQ3AA5wKQsnPmGQYNkLRZJuFZUaD83oB1vkLdCDl0xsNMDNy2ucEuH9bImk9SWWi59p4ifZsqUx-8-6B52s84DgJXG166Q9WtBCMx6r_F1wmj-W_9A-xNiK_ovC1Xrtgl_bg8lEdNdqRzWPoJlTYTfIH/file?dl=1# [following]\n",
            "--2021-06-21 08:41:11--  https://uc52616e03169e18ca41ab02b9d6.dl.dropboxusercontent.com/cd/0/get/BQ3AA5wKQsnPmGQYNkLRZJuFZUaD83oB1vkLdCDl0xsNMDNy2ucEuH9bImk9SWWi59p4ifZsqUx-8-6B52s84DgJXG166Q9WtBCMx6r_F1wmj-W_9A-xNiK_ovC1Xrtgl_bg8lEdNdqRzWPoJlTYTfIH/file?dl=1\n",
            "Resolving uc52616e03169e18ca41ab02b9d6.dl.dropboxusercontent.com (uc52616e03169e18ca41ab02b9d6.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc52616e03169e18ca41ab02b9d6.dl.dropboxusercontent.com (uc52616e03169e18ca41ab02b9d6.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31811430 (30M) [application/binary]\n",
            "Saving to: ‘results/ayush/ayush_colmap.zip’\n",
            "\n",
            "results/ayush/ayush 100%[===================>]  30.34M  12.8MB/s    in 2.4s    \n",
            "\n",
            "2021-06-21 08:41:15 (12.8 MB/s) - ‘results/ayush/ayush_colmap.zip’ saved [31811430/31811430]\n",
            "\n",
            "++ unzip results/ayush/ayush_colmap.zip -d results/ayush\n",
            "Archive:  results/ayush/ayush_colmap.zip\n",
            "   creating: results/ayush/colmap_dense/\n",
            "  inflating: results/ayush/colmap_dense/metadata.npz  \n",
            "   creating: results/ayush/__MACOSX/\n",
            "   creating: results/ayush/__MACOSX/colmap_dense/\n",
            "  inflating: results/ayush/__MACOSX/colmap_dense/._metadata.npz  \n",
            "  inflating: results/ayush/__MACOSX/._colmap_dense  \n",
            "   creating: results/ayush/depth_colmap_dense/\n",
            "   creating: results/ayush/depth_colmap_dense/depth/\n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000022.raw  \n",
            "   creating: results/ayush/__MACOSX/depth_colmap_dense/\n",
            "   creating: results/ayush/__MACOSX/depth_colmap_dense/depth/\n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000022.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000036.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000036.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000088.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000088.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000063.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000063.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000077.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000077.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000076.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000076.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000062.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000062.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000089.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000089.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000037.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000037.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000023.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000023.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000009.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000009.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000035.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000035.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000021.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000021.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000048.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000048.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000074.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000074.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000060.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000060.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000061.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000061.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000075.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000075.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000049.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000049.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000020.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000020.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000034.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000034.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000008.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000008.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000030.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000030.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000024.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000024.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000018.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000018.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000071.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000071.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000065.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000065.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000059.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000059.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000058.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000058.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000064.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000064.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000070.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000070.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000019.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000019.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000025.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000025.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000031.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000031.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000027.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000027.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000033.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000033.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000066.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000066.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000072.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000072.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000073.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000073.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000067.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000067.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000032.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000032.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000026.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000026.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000082.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000082.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000069.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000069.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000041.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000041.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000055.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000055.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000028.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000028.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000000.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000000.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000014.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000014.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000015.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000015.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000001.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000001.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000029.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000029.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000054.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000054.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000040.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000040.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000068.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000068.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000083.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000083.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000081.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000081.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000056.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000056.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000042.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000042.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000017.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000017.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000003.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000003.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000002.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000002.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000016.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000016.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000043.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000043.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000057.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000057.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000080.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000080.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000090.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000090.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000084.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000084.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000053.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000053.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000047.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000047.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000012.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000012.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000006.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000006.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000007.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000007.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000013.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000013.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000046.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000046.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000052.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000052.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000085.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000085.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000091.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000091.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000087.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000087.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000044.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000044.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000050.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000050.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000078.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000078.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000005.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000005.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000011.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000011.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000039.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000039.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000038.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000038.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000010.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000010.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000004.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000004.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000079.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000079.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000051.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000051.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000045.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000045.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000086.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000086.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000048.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000048.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000060.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000060.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000074.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000074.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000009.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000009.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000021.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000021.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000035.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000035.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000034.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000034.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000020.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000020.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000008.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000008.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000075.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000075.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000061.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000061.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000049.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000049.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000088.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000088.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000077.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000077.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000063.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000063.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000036.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000036.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000022.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000022.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000023.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000023.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000037.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000037.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000062.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000062.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000076.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000076.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000089.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000089.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000072.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000072.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000066.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000066.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000033.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000033.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000027.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000027.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000026.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000026.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000032.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000032.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000067.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000067.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000073.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000073.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000065.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000065.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000071.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000071.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000059.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000059.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000024.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000024.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000030.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000030.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000018.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000018.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000019.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000019.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000031.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000031.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000025.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000025.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000058.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000058.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000070.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000070.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000064.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000064.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000003.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000003.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000017.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000017.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000081.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000081.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000042.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000042.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000056.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000056.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000057.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000057.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000043.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000043.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000080.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000080.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000016.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000016.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000002.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000002.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000028.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000028.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000014.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000014.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000000.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000000.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000082.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000082.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000069.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000069.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000055.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000055.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000041.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000041.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000040.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000040.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000054.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000054.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000068.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000068.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000083.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000083.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000001.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000001.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000015.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000015.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000029.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000029.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000011.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000011.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000005.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000005.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000039.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000039.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000087.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000087.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000050.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000050.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000044.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000044.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000078.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000078.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000079.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000079.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000045.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000045.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000051.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000051.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000086.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000086.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000038.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000038.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000004.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000004.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000010.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000010.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000006.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000006.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000012.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000012.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000084.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000084.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000090.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000090.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000047.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000047.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000053.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000053.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000052.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000052.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000046.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000046.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000091.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000091.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000085.png  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000085.png  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000013.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000013.raw  \n",
            "  inflating: results/ayush/depth_colmap_dense/depth/frame_000007.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/depth/._frame_000007.raw  \n",
            "  inflating: results/ayush/__MACOSX/depth_colmap_dense/._depth  \n",
            "  inflating: results/ayush/__MACOSX/._depth_colmap_dense  \n",
            "++ rm results/ayush/ayush_colmap.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0clmFX2KmJ7T",
        "outputId": "6746e4ee-9fad-4ca4-dd0d-6f706da42f6a"
      },
      "source": [
        "!python main.py --video_file data/videos/ayush.mp4 --path results/ayush \\\n",
        "  --camera_params \"1671.770118, 540, 960\" --camera_model \"SIMPLE_PINHOLE\" \\\n",
        "  --make_video\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "(75, 77): reprojection: 0.350847, disparity: 0.069905\n",
            "(76, 77): reprojection: 0.222033, disparity: 0.060954\n",
            "(76, 78): reprojection: 0.331492, disparity: 0.066980\n",
            "(76, 80): reprojection: 0.437980, disparity: 0.083646\n",
            "(76, 84): reprojection: 0.707222, disparity: 0.093615\n",
            "(77, 78): reprojection: 0.216137, disparity: 0.059365\n",
            "(77, 79): reprojection: 0.267792, disparity: 0.068482\n",
            "(78, 79): reprojection: 0.205308, disparity: 0.064261\n",
            "(78, 80): reprojection: 0.262357, disparity: 0.072551\n",
            "(78, 82): reprojection: 0.441747, disparity: 0.078366\n",
            "(79, 80): reprojection: 0.145220, disparity: 0.062875\n",
            "(79, 81): reprojection: 0.243848, disparity: 0.064535\n",
            "(80, 81): reprojection: 0.143529, disparity: 0.063304\n",
            "(80, 82): reprojection: 0.273501, disparity: 0.080373\n",
            "(80, 84): reprojection: 0.504853, disparity: 0.083918\n",
            "(80, 88): reprojection: 0.734228, disparity: 0.098891\n",
            "(81, 82): reprojection: 0.190113, disparity: 0.061980\n",
            "(81, 83): reprojection: 0.309300, disparity: 0.069754\n",
            "(82, 83): reprojection: 0.152960, disparity: 0.057031\n",
            "(82, 84): reprojection: 0.266526, disparity: 0.065578\n",
            "(82, 86): reprojection: 0.467949, disparity: 0.078978\n",
            "(83, 84): reprojection: 0.151778, disparity: 0.057871\n",
            "(83, 85): reprojection: 0.313614, disparity: 0.069237\n",
            "(84, 85): reprojection: 0.173530, disparity: 0.057817\n",
            "(84, 86): reprojection: 0.259625, disparity: 0.064858\n",
            "(84, 88): reprojection: 0.485004, disparity: 0.071132\n",
            "(85, 86): reprojection: 0.163798, disparity: 0.055652\n",
            "(85, 87): reprojection: 0.275480, disparity: 0.061914\n",
            "(86, 87): reprojection: 0.228746, disparity: 0.059352\n",
            "(86, 88): reprojection: 0.321828, disparity: 0.066349\n",
            "(86, 90): reprojection: 0.488860, disparity: 0.070256\n",
            "(87, 88): reprojection: 0.172712, disparity: 0.057790\n",
            "(87, 89): reprojection: 0.275443, disparity: 0.064550\n",
            "(88, 89): reprojection: 0.141689, disparity: 0.057078\n",
            "(88, 90): reprojection: 0.310651, disparity: 0.061654\n",
            "(89, 90): reprojection: 0.213451, disparity: 0.053341\n",
            "(89, 91): reprojection: 0.408729, disparity: 0.069215\n",
            "(90, 91): reprojection: 0.327160, disparity: 0.060744\n",
            "Mean:     reprojection: 0.327160, disparity: 0.060744\n",
            "Done Validation for epoch 5 (1300 iterations)\n",
            "Epoch = 5, pairs = [[7, 9], [87, 88], [4, 8], [0, 4]], loss = 0.3092533349990845\n",
            "Epoch = 5, pairs = [[58, 62], [85, 86], [15, 16], [77, 78]], loss = 0.3712765574455261\n",
            "Epoch = 5, pairs = [[49, 51], [25, 26], [6, 7], [10, 11]], loss = 0.20636533200740814\n",
            "Epoch = 5, pairs = [[19, 20], [72, 88], [20, 22], [63, 65]], loss = 0.5068532228469849\n",
            "Epoch = 5, pairs = [[80, 81], [8, 24], [29, 31], [79, 80]], loss = 0.42700666189193726\n",
            "Epoch = 5, pairs = [[76, 78], [75, 76], [11, 12], [28, 32]], loss = 0.3367263078689575\n",
            "Epoch = 5, pairs = [[57, 58], [8, 9], [28, 36], [26, 27]], loss = 0.4024733304977417\n",
            "Epoch = 5, pairs = [[27, 29], [19, 21], [54, 55], [2, 3]], loss = 0.29815739393234253\n",
            "Epoch = 5, pairs = [[88, 89], [52, 53], [0, 8], [78, 79]], loss = 0.31015944480895996\n",
            "Epoch = 5, pairs = [[30, 31], [61, 62], [45, 47], [4, 5]], loss = 0.2873624861240387\n",
            "Epoch = 5, pairs = [[40, 42], [89, 90], [4, 12], [83, 85]], loss = 0.454123854637146\n",
            "Epoch = 5, pairs = [[23, 25], [56, 72], [38, 42], [67, 69]], loss = 0.7202470898628235\n",
            "Epoch = 5, pairs = [[6, 10], [7, 8], [66, 67], [83, 84]], loss = 0.2969852685928345\n",
            "Epoch = 5, pairs = [[44, 48], [12, 14], [60, 64], [34, 38]], loss = 0.4464385509490967\n",
            "Epoch = 5, pairs = [[76, 77], [41, 42], [17, 19], [74, 78]], loss = 0.4002315104007721\n",
            "Epoch = 5, pairs = [[20, 21], [3, 4], [13, 14], [36, 40]], loss = 0.2791426181793213\n",
            "Epoch = 5, pairs = [[0, 16], [26, 30], [79, 81], [27, 28]], loss = 0.4621284604072571\n",
            "Epoch = 5, pairs = [[22, 23], [58, 59], [76, 80], [23, 24]], loss = 0.28863781690597534\n",
            "Epoch = 5, pairs = [[21, 23], [5, 6], [72, 74], [40, 56]], loss = 0.4401701092720032\n",
            "Epoch = 5, pairs = [[32, 33], [85, 87], [21, 22], [1, 2]], loss = 0.26891079545021057\n",
            "Epoch = 5, pairs = [[42, 46], [0, 2], [64, 65], [66, 68]], loss = 0.4412282705307007\n",
            "Epoch = 5, pairs = [[44, 45], [86, 87], [51, 53], [1, 3]], loss = 0.29509103298187256\n",
            "Epoch = 5, pairs = [[18, 19], [40, 44], [89, 91], [70, 71]], loss = 0.4197804927825928\n",
            "Epoch = 5, pairs = [[50, 52], [32, 36], [32, 40], [44, 52]], loss = 0.5798832178115845\n",
            "Epoch = 5, pairs = [[53, 55], [5, 7], [48, 64], [35, 36]], loss = 0.5188338756561279\n",
            "Epoch = 5, pairs = [[4, 6], [12, 16], [74, 75], [37, 39]], loss = 0.3341493308544159\n",
            "Epoch = 5, pairs = [[65, 66], [84, 85], [14, 18], [28, 29]], loss = 0.31245720386505127\n",
            "Epoch = 5, pairs = [[52, 56], [82, 84], [57, 59], [81, 82]], loss = 0.4717830419540405\n",
            "Epoch = 5, pairs = [[46, 50], [39, 41], [39, 40], [16, 48]], loss = 0.7491995692253113\n",
            "Epoch = 5, pairs = [[52, 60], [69, 71], [43, 44], [8, 12]], loss = 0.5291861295700073\n",
            "Epoch = 5, pairs = [[62, 66], [55, 56], [72, 73], [76, 84]], loss = 0.49837106466293335\n",
            "Epoch = 5, pairs = [[31, 33], [24, 28], [48, 56], [36, 38]], loss = 0.48774924874305725\n",
            "Epoch = 5, pairs = [[22, 26], [40, 48], [36, 37], [26, 28]], loss = 0.46658480167388916\n",
            "Epoch = 5, pairs = [[17, 18], [68, 72], [9, 11], [10, 14]], loss = 0.4661741256713867\n",
            "Epoch = 5, pairs = [[66, 70], [60, 62], [45, 46], [22, 24]], loss = 0.3453863859176636\n",
            "Epoch = 5, pairs = [[75, 77], [59, 60], [47, 49], [74, 76]], loss = 0.33897387981414795\n",
            "Epoch = 5, pairs = [[50, 51], [51, 52], [80, 82], [32, 48]], loss = 0.4733175039291382\n",
            "Epoch = 5, pairs = [[33, 35], [54, 58], [49, 50], [12, 20]], loss = 0.4252837300300598\n",
            "Epoch = 5, pairs = [[42, 43], [55, 57], [72, 76], [47, 48]], loss = 0.357608824968338\n",
            "Epoch = 5, pairs = [[68, 69], [24, 25], [70, 72], [44, 46]], loss = 0.2956574559211731\n",
            "Epoch = 5, pairs = [[16, 32], [36, 44], [43, 45], [67, 68]], loss = 0.7487983703613281\n",
            "Epoch = 5, pairs = [[2, 6], [62, 64], [9, 10], [56, 58]], loss = 0.4682624340057373\n",
            "Epoch = 5, pairs = [[77, 79], [73, 74], [0, 32], [31, 32]], loss = 0.4859774112701416\n",
            "Epoch = 5, pairs = [[48, 49], [48, 52], [18, 20], [64, 80]], loss = 0.5841425657272339\n",
            "Epoch = 5, pairs = [[24, 32], [54, 56], [86, 88], [80, 84]], loss = 0.5629787445068359\n",
            "Epoch = 5, pairs = [[42, 44], [14, 16], [16, 18], [3, 5]], loss = 0.3868987560272217\n",
            "Epoch = 5, pairs = [[52, 54], [69, 70], [68, 76], [16, 17]], loss = 0.4247048497200012\n",
            "Epoch = 5, pairs = [[64, 66], [87, 89], [56, 60], [15, 17]], loss = 0.5062628388404846\n",
            "Epoch = 5, pairs = [[46, 47], [34, 36], [81, 83], [71, 72]], loss = 0.3062053918838501\n",
            "Epoch = 5, pairs = [[84, 88], [37, 38], [25, 27], [90, 91]], loss = 0.37699422240257263\n",
            "Epoch = 5, pairs = [[86, 90], [82, 83], [88, 90], [70, 74]], loss = 0.4106469452381134\n",
            "Epoch = 5, pairs = [[2, 4], [78, 82], [24, 26], [62, 63]], loss = 0.3052350878715515\n",
            "Epoch = 5, pairs = [[38, 40], [16, 20], [34, 35], [41, 43]], loss = 0.4336755871772766\n",
            "Epoch = 5, pairs = [[8, 16], [0, 1], [10, 12], [82, 86]], loss = 0.4555583596229553\n",
            "Epoch = 5, pairs = [[32, 34], [20, 24], [72, 80], [13, 15]], loss = 0.46118131279945374\n",
            "Epoch = 5, pairs = [[71, 73], [20, 28], [60, 61], [18, 22]], loss = 0.487826406955719\n",
            "Epoch = 5, pairs = [[28, 30], [58, 60], [12, 13], [40, 41]], loss = 0.3219262957572937\n",
            "Epoch = 5, pairs = [[63, 64], [29, 30], [11, 13], [33, 34]], loss = 0.24492518603801727\n",
            "Epoch = 5, pairs = [[84, 86], [64, 68], [48, 50], [53, 54]], loss = 0.3630802631378174\n",
            "Epoch = 5, pairs = [[59, 61], [60, 68], [6, 8], [35, 37]], loss = 0.46384596824645996\n",
            "Epoch = 5, pairs = [[16, 24], [78, 80], [24, 40], [80, 88]], loss = 0.8117191791534424\n",
            "Epoch = 5, pairs = [[14, 15], [64, 72], [61, 63], [68, 70]], loss = 0.47107893228530884\n",
            "Epoch = 5, pairs = [[48, 80], [65, 67], [30, 34], [32, 64]], loss = 1.5842230319976807\n",
            "Epoch = 5, pairs = [[56, 64], [56, 57], [38, 39], [73, 75]], loss = 0.5493847131729126\n",
            "Epoch = 5, pairs = [[50, 54], [30, 32], [46, 48], [8, 10]], loss = 0.49815669655799866\n",
            "Epoch 5 took 84.21s.\n",
            "( 0,  1): reprojection: 0.227021, disparity: 0.076297\n",
            "( 0,  2): reprojection: 0.338856, disparity: 0.084425\n",
            "( 0,  4): reprojection: 0.400163, disparity: 0.093167\n",
            "( 0,  8): reprojection: 0.583873, disparity: 0.120560\n",
            "( 0, 16): reprojection: 1.389631, disparity: 0.129390\n",
            "( 0, 32): reprojection: 3.788637, disparity: 0.176803\n",
            "( 1,  2): reprojection: 0.193965, disparity: 0.062208\n",
            "( 1,  3): reprojection: 0.260198, disparity: 0.069593\n",
            "( 2,  3): reprojection: 0.120195, disparity: 0.057500\n",
            "( 2,  4): reprojection: 0.212805, disparity: 0.066237\n",
            "( 2,  6): reprojection: 0.437528, disparity: 0.080181\n",
            "( 3,  4): reprojection: 0.114215, disparity: 0.062569\n",
            "( 3,  5): reprojection: 0.235230, disparity: 0.072366\n",
            "( 4,  5): reprojection: 0.166323, disparity: 0.059696\n",
            "( 4,  6): reprojection: 0.268406, disparity: 0.065215\n",
            "( 4,  8): reprojection: 0.372294, disparity: 0.078270\n",
            "( 4, 12): reprojection: 0.746196, disparity: 0.101689\n",
            "( 5,  6): reprojection: 0.138116, disparity: 0.060186\n",
            "( 5,  7): reprojection: 0.194754, disparity: 0.067328\n",
            "( 6,  7): reprojection: 0.096970, disparity: 0.058064\n",
            "( 6,  8): reprojection: 0.191356, disparity: 0.067307\n",
            "( 6, 10): reprojection: 0.503550, disparity: 0.082608\n",
            "( 7,  8): reprojection: 0.110323, disparity: 0.058927\n",
            "( 7,  9): reprojection: 0.253988, disparity: 0.070534\n",
            "( 8,  9): reprojection: 0.187598, disparity: 0.060927\n",
            "( 8, 10): reprojection: 0.446752, disparity: 0.069852\n",
            "( 8, 12): reprojection: 0.719206, disparity: 0.085629\n",
            "( 8, 16): reprojection: 1.130522, disparity: 0.109639\n",
            "( 8, 24): reprojection: 2.016323, disparity: 0.151284\n",
            "( 9, 10): reprojection: 0.306250, disparity: 0.060608\n",
            "( 9, 11): reprojection: 0.487371, disparity: 0.070961\n",
            "(10, 11): reprojection: 0.224719, disparity: 0.055339\n",
            "(10, 12): reprojection: 0.322296, disparity: 0.069734\n",
            "(10, 14): reprojection: 0.590827, disparity: 0.100159\n",
            "(11, 12): reprojection: 0.196952, disparity: 0.057822\n",
            "(11, 13): reprojection: 0.352683, disparity: 0.075663\n",
            "(12, 13): reprojection: 0.188379, disparity: 0.058937\n",
            "(12, 14): reprojection: 0.380721, disparity: 0.075441\n",
            "(12, 16): reprojection: 0.630508, disparity: 0.113021\n",
            "(12, 20): reprojection: 1.248529, disparity: 0.126773\n",
            "(13, 14): reprojection: 0.241395, disparity: 0.060006\n",
            "(13, 15): reprojection: 0.313838, disparity: 0.082862\n",
            "(14, 15): reprojection: 0.234156, disparity: 0.063900\n",
            "(14, 16): reprojection: 0.553405, disparity: 0.080009\n",
            "(14, 18): reprojection: 0.601812, disparity: 0.098980\n",
            "(15, 16): reprojection: 0.351179, disparity: 0.062734\n",
            "(15, 17): reprojection: 0.490892, disparity: 0.078585\n",
            "(16, 17): reprojection: 0.190348, disparity: 0.062748\n",
            "(16, 18): reprojection: 0.460065, disparity: 0.082829\n",
            "(16, 20): reprojection: 0.898224, disparity: 0.107283\n",
            "(16, 24): reprojection: 1.507691, disparity: 0.140593\n",
            "(16, 32): reprojection: 2.367283, disparity: 0.142308\n",
            "(16, 48): reprojection: 6.390317, disparity: 0.264900\n",
            "(17, 18): reprojection: 0.396961, disparity: 0.068063\n",
            "(17, 19): reprojection: 0.653994, disparity: 0.087174\n",
            "(18, 19): reprojection: 0.297384, disparity: 0.064189\n",
            "(18, 20): reprojection: 0.477926, disparity: 0.078857\n",
            "(18, 22): reprojection: 0.780344, disparity: 0.100481\n",
            "(19, 20): reprojection: 0.183938, disparity: 0.063976\n",
            "(19, 21): reprojection: 0.409673, disparity: 0.078955\n",
            "(20, 21): reprojection: 0.246664, disparity: 0.065102\n",
            "(20, 22): reprojection: 0.408056, disparity: 0.083844\n",
            "(20, 24): reprojection: 0.641411, disparity: 0.106009\n",
            "(20, 28): reprojection: 1.088709, disparity: 0.149961\n",
            "(21, 22): reprojection: 0.202665, disparity: 0.067213\n",
            "(21, 23): reprojection: 0.332847, disparity: 0.081672\n",
            "(22, 23): reprojection: 0.179888, disparity: 0.066744\n",
            "(22, 24): reprojection: 0.297262, disparity: 0.086239\n",
            "(22, 26): reprojection: 0.647069, disparity: 0.106765\n",
            "(23, 24): reprojection: 0.172473, disparity: 0.067047\n",
            "(23, 25): reprojection: 0.340116, disparity: 0.083385\n",
            "(24, 25): reprojection: 0.197077, disparity: 0.069238\n",
            "(24, 26): reprojection: 0.332941, disparity: 0.084611\n",
            "(24, 28): reprojection: 0.614622, disparity: 0.109713\n",
            "(24, 32): reprojection: 1.177838, disparity: 0.129685\n",
            "(24, 40): reprojection: 2.199216, disparity: 0.172255\n",
            "(25, 26): reprojection: 0.196006, disparity: 0.074128\n",
            "(25, 27): reprojection: 0.382858, disparity: 0.089565\n",
            "(26, 27): reprojection: 0.210350, disparity: 0.071363\n",
            "(26, 28): reprojection: 0.325536, disparity: 0.090495\n",
            "(26, 30): reprojection: 0.636085, disparity: 0.112390\n",
            "(27, 28): reprojection: 0.203800, disparity: 0.067230\n",
            "(27, 29): reprojection: 0.401875, disparity: 0.084889\n",
            "(28, 29): reprojection: 0.249501, disparity: 0.070657\n",
            "(28, 30): reprojection: 0.398263, disparity: 0.085592\n",
            "(28, 32): reprojection: 0.672880, disparity: 0.100575\n",
            "(28, 36): reprojection: 1.135450, disparity: 0.136369\n",
            "(29, 30): reprojection: 0.200557, disparity: 0.068512\n",
            "(29, 31): reprojection: 0.386057, disparity: 0.079181\n",
            "(30, 31): reprojection: 0.245538, disparity: 0.066868\n",
            "(30, 32): reprojection: 0.355204, disparity: 0.079364\n",
            "(30, 34): reprojection: 0.628697, disparity: 0.102231\n",
            "(31, 32): reprojection: 0.170345, disparity: 0.065207\n",
            "(31, 33): reprojection: 0.384752, disparity: 0.080765\n",
            "(32, 33): reprojection: 0.280427, disparity: 0.073447\n",
            "(32, 34): reprojection: 0.321842, disparity: 0.083588\n",
            "(32, 36): reprojection: 0.546351, disparity: 0.108362\n",
            "(32, 40): reprojection: 0.996332, disparity: 0.153038\n",
            "(32, 48): reprojection: 2.134748, disparity: 0.310013\n",
            "(32, 64): reprojection: 6.991949, disparity: 0.570739\n",
            "(33, 34): reprojection: 0.203003, disparity: 0.068343\n",
            "(33, 35): reprojection: 0.349620, disparity: 0.078610\n",
            "(34, 35): reprojection: 0.226997, disparity: 0.066215\n",
            "(34, 36): reprojection: 0.357271, disparity: 0.084493\n",
            "(34, 38): reprojection: 0.631048, disparity: 0.101182\n",
            "(35, 36): reprojection: 0.274975, disparity: 0.074940\n",
            "(35, 37): reprojection: 0.376594, disparity: 0.086830\n",
            "(36, 37): reprojection: 0.309223, disparity: 0.075139\n",
            "(36, 38): reprojection: 0.522739, disparity: 0.087496\n",
            "(36, 40): reprojection: 0.635935, disparity: 0.109174\n",
            "(36, 44): reprojection: 1.175979, disparity: 0.160916\n",
            "(37, 38): reprojection: 0.277098, disparity: 0.070437\n",
            "(37, 39): reprojection: 0.347464, disparity: 0.080797\n",
            "(38, 39): reprojection: 0.238283, disparity: 0.071815\n",
            "(38, 40): reprojection: 0.509020, disparity: 0.093741\n",
            "(38, 42): reprojection: 1.002653, disparity: 0.124757\n",
            "(39, 40): reprojection: 0.341987, disparity: 0.082306\n",
            "(39, 41): reprojection: 0.635644, disparity: 0.103622\n",
            "(40, 41): reprojection: 0.362784, disparity: 0.076790\n",
            "(40, 42): reprojection: 0.595498, disparity: 0.092628\n",
            "(40, 44): reprojection: 0.841130, disparity: 0.132297\n",
            "(40, 48): reprojection: 1.285168, disparity: 0.185148\n",
            "(40, 56): reprojection: 2.398769, disparity: 0.335317\n",
            "(41, 42): reprojection: 0.276423, disparity: 0.073376\n",
            "(41, 43): reprojection: 0.407786, disparity: 0.089842\n",
            "(42, 43): reprojection: 0.212222, disparity: 0.081002\n",
            "(42, 44): reprojection: 0.400536, disparity: 0.099638\n",
            "(42, 46): reprojection: 0.723415, disparity: 0.136298\n",
            "(43, 44): reprojection: 0.223386, disparity: 0.076714\n",
            "(43, 45): reprojection: 0.375548, disparity: 0.095517\n",
            "(44, 45): reprojection: 0.200193, disparity: 0.077845\n",
            "(44, 46): reprojection: 0.398049, disparity: 0.108376\n",
            "(44, 48): reprojection: 0.655631, disparity: 0.134645\n",
            "(44, 52): reprojection: 1.217592, disparity: 0.200531\n",
            "(45, 46): reprojection: 0.236929, disparity: 0.086643\n",
            "(45, 47): reprojection: 0.404448, disparity: 0.111520\n",
            "(46, 47): reprojection: 0.229802, disparity: 0.078492\n",
            "(46, 48): reprojection: 0.417552, disparity: 0.099163\n",
            "(46, 50): reprojection: 0.681554, disparity: 0.133372\n",
            "(47, 48): reprojection: 0.252063, disparity: 0.074944\n",
            "(47, 49): reprojection: 0.371592, disparity: 0.098875\n",
            "(48, 49): reprojection: 0.222056, disparity: 0.088697\n",
            "(48, 50): reprojection: 0.369865, disparity: 0.109913\n",
            "(48, 52): reprojection: 0.661402, disparity: 0.142416\n",
            "(48, 56): reprojection: 1.379553, disparity: 0.190008\n",
            "(48, 64): reprojection: 2.994695, disparity: 0.310167\n",
            "(48, 80): reprojection: 9.431359, disparity: 0.214349\n",
            "(49, 50): reprojection: 0.202351, disparity: 0.079538\n",
            "(49, 51): reprojection: 0.371055, disparity: 0.100508\n",
            "(50, 51): reprojection: 0.214974, disparity: 0.080720\n",
            "(50, 52): reprojection: 0.393856, disparity: 0.100504\n",
            "(50, 54): reprojection: 0.761152, disparity: 0.147004\n",
            "(51, 52): reprojection: 0.235065, disparity: 0.079734\n",
            "(51, 53): reprojection: 0.442262, disparity: 0.098388\n",
            "(52, 53): reprojection: 0.242326, disparity: 0.081899\n",
            "(52, 54): reprojection: 0.405212, disparity: 0.102270\n",
            "(52, 56): reprojection: 0.904595, disparity: 0.113643\n",
            "(52, 60): reprojection: 1.455491, disparity: 0.190447\n",
            "(53, 54): reprojection: 0.209282, disparity: 0.082058\n",
            "(53, 55): reprojection: 0.489575, disparity: 0.103847\n",
            "(54, 55): reprojection: 0.381812, disparity: 0.078645\n",
            "(54, 56): reprojection: 0.729449, disparity: 0.087233\n",
            "(54, 58): reprojection: 0.784726, disparity: 0.112771\n",
            "(55, 56): reprojection: 0.401594, disparity: 0.081071\n",
            "(55, 57): reprojection: 0.523754, disparity: 0.092905\n",
            "(56, 57): reprojection: 0.383383, disparity: 0.081168\n",
            "(56, 58): reprojection: 0.727753, disparity: 0.100341\n",
            "(56, 60): reprojection: 1.072034, disparity: 0.150269\n",
            "(56, 64): reprojection: 1.718155, disparity: 0.159512\n",
            "(56, 72): reprojection: 4.037075, disparity: 0.186082\n",
            "(57, 58): reprojection: 0.422230, disparity: 0.088282\n",
            "(57, 59): reprojection: 0.645498, disparity: 0.105838\n",
            "(58, 59): reprojection: 0.329059, disparity: 0.086272\n",
            "(58, 60): reprojection: 0.513324, disparity: 0.108353\n",
            "(58, 62): reprojection: 0.886424, disparity: 0.123723\n",
            "(59, 60): reprojection: 0.254372, disparity: 0.094439\n",
            "(59, 61): reprojection: 0.453104, disparity: 0.104936\n",
            "(60, 61): reprojection: 0.296645, disparity: 0.087758\n",
            "(60, 62): reprojection: 0.481158, disparity: 0.094394\n",
            "(60, 64): reprojection: 0.915732, disparity: 0.110009\n",
            "(60, 68): reprojection: 1.737345, disparity: 0.140661\n",
            "(61, 62): reprojection: 0.292143, disparity: 0.087757\n",
            "(61, 63): reprojection: 0.483614, disparity: 0.094457\n",
            "(62, 63): reprojection: 0.318333, disparity: 0.085737\n",
            "(62, 64): reprojection: 0.525106, disparity: 0.098797\n",
            "(62, 66): reprojection: 0.919755, disparity: 0.135402\n",
            "(63, 64): reprojection: 0.308556, disparity: 0.081194\n",
            "(63, 65): reprojection: 0.530745, disparity: 0.099535\n",
            "(64, 65): reprojection: 0.295835, disparity: 0.091229\n",
            "(64, 66): reprojection: 0.582443, disparity: 0.101652\n",
            "(64, 68): reprojection: 0.856126, disparity: 0.135553\n",
            "(64, 72): reprojection: 1.721891, disparity: 0.146250\n",
            "(64, 80): reprojection: 3.591402, disparity: 0.174148\n",
            "(65, 66): reprojection: 0.377434, disparity: 0.085746\n",
            "(65, 67): reprojection: 0.649114, disparity: 0.103674\n",
            "(66, 67): reprojection: 0.343678, disparity: 0.094520\n",
            "(66, 68): reprojection: 0.574696, disparity: 0.111995\n",
            "(66, 70): reprojection: 0.988744, disparity: 0.134345\n",
            "(67, 68): reprojection: 0.434610, disparity: 0.094212\n",
            "(67, 69): reprojection: 0.593794, disparity: 0.099905\n",
            "(68, 69): reprojection: 0.330218, disparity: 0.089775\n",
            "(68, 70): reprojection: 0.719470, disparity: 0.115621\n",
            "(68, 72): reprojection: 1.038823, disparity: 0.138019\n",
            "(68, 76): reprojection: 1.769636, disparity: 0.151515\n",
            "(69, 70): reprojection: 0.451182, disparity: 0.096252\n",
            "(69, 71): reprojection: 0.726301, disparity: 0.109609\n",
            "(70, 71): reprojection: 0.317665, disparity: 0.088596\n",
            "(70, 72): reprojection: 0.499867, disparity: 0.101531\n",
            "(70, 74): reprojection: 0.867462, disparity: 0.117366\n",
            "(71, 72): reprojection: 0.350190, disparity: 0.086316\n",
            "(71, 73): reprojection: 0.596755, disparity: 0.100130\n",
            "(72, 73): reprojection: 0.319644, disparity: 0.085445\n",
            "(72, 74): reprojection: 0.491092, disparity: 0.092236\n",
            "(72, 76): reprojection: 0.869148, disparity: 0.098412\n",
            "(72, 80): reprojection: 1.338302, disparity: 0.139290\n",
            "(72, 88): reprojection: 2.579071, disparity: 0.172835\n",
            "(73, 74): reprojection: 0.392824, disparity: 0.079733\n",
            "(73, 75): reprojection: 0.627405, disparity: 0.108256\n",
            "(74, 75): reprojection: 0.321890, disparity: 0.087622\n",
            "(74, 76): reprojection: 0.445938, disparity: 0.095613\n",
            "(74, 78): reprojection: 0.776655, disparity: 0.116288\n",
            "(75, 76): reprojection: 0.318636, disparity: 0.081378\n",
            "(75, 77): reprojection: 0.550674, disparity: 0.096426\n",
            "(76, 77): reprojection: 0.301625, disparity: 0.078868\n",
            "(76, 78): reprojection: 0.485507, disparity: 0.090753\n",
            "(76, 80): reprojection: 0.692811, disparity: 0.127301\n",
            "(76, 84): reprojection: 1.288502, disparity: 0.127959\n",
            "(77, 78): reprojection: 0.290283, disparity: 0.077516\n",
            "(77, 79): reprojection: 0.424381, disparity: 0.101089\n",
            "(78, 79): reprojection: 0.272690, disparity: 0.087854\n",
            "(78, 80): reprojection: 0.392935, disparity: 0.099534\n",
            "(78, 82): reprojection: 0.714900, disparity: 0.106676\n",
            "(79, 80): reprojection: 0.199002, disparity: 0.076564\n",
            "(79, 81): reprojection: 0.364064, disparity: 0.081539\n",
            "(80, 81): reprojection: 0.213041, disparity: 0.075012\n",
            "(80, 82): reprojection: 0.395692, disparity: 0.081116\n",
            "(80, 84): reprojection: 0.783032, disparity: 0.089040\n",
            "(80, 88): reprojection: 1.287491, disparity: 0.122850\n",
            "(81, 82): reprojection: 0.259578, disparity: 0.074851\n",
            "(81, 83): reprojection: 0.475649, disparity: 0.081217\n",
            "(82, 83): reprojection: 0.231875, disparity: 0.072555\n",
            "(82, 84): reprojection: 0.398275, disparity: 0.082681\n",
            "(82, 86): reprojection: 0.728575, disparity: 0.098686\n",
            "(83, 84): reprojection: 0.217613, disparity: 0.071521\n",
            "(83, 85): reprojection: 0.451416, disparity: 0.087441\n",
            "(84, 85): reprojection: 0.226932, disparity: 0.073586\n",
            "(84, 86): reprojection: 0.370943, disparity: 0.081960\n",
            "(84, 88): reprojection: 0.711301, disparity: 0.097364\n",
            "(85, 86): reprojection: 0.210988, disparity: 0.069677\n",
            "(85, 87): reprojection: 0.380013, disparity: 0.080762\n",
            "(86, 87): reprojection: 0.283612, disparity: 0.078422\n",
            "(86, 88): reprojection: 0.430275, disparity: 0.089953\n",
            "(86, 90): reprojection: 0.596934, disparity: 0.094479\n",
            "(87, 88): reprojection: 0.219721, disparity: 0.073571\n",
            "(87, 89): reprojection: 0.345802, disparity: 0.083628\n",
            "(88, 89): reprojection: 0.166726, disparity: 0.071894\n",
            "(88, 90): reprojection: 0.342617, disparity: 0.078804\n",
            "(89, 90): reprojection: 0.219689, disparity: 0.067450\n",
            "(89, 91): reprojection: 0.460208, disparity: 0.097878\n",
            "(90, 91): reprojection: 0.360873, disparity: 0.083996\n",
            "Mean:     reprojection: 0.360873, disparity: 0.083996\n",
            "Done Validation for epoch 6 (1560 iterations)\n",
            "Epoch = 6, pairs = [[47, 48], [22, 24], [2, 6], [42, 43]], loss = 0.37274664640426636\n",
            "Epoch = 6, pairs = [[18, 20], [8, 10], [45, 47], [68, 76]], loss = 1.0194220542907715\n",
            "Epoch = 6, pairs = [[50, 54], [84, 85], [60, 62], [44, 52]], loss = 0.9472360610961914\n",
            "Epoch = 6, pairs = [[58, 59], [28, 30], [62, 66], [90, 91]], loss = 0.6823161840438843\n",
            "Epoch = 6, pairs = [[20, 21], [44, 45], [10, 14], [32, 33]], loss = 0.4560336470603943\n",
            "Epoch = 6, pairs = [[63, 65], [3, 5], [2, 3], [18, 22]], loss = 0.54728102684021\n",
            "Epoch = 6, pairs = [[44, 48], [35, 36], [24, 40], [8, 24]], loss = 1.8663212060928345\n",
            "Epoch = 6, pairs = [[9, 11], [12, 16], [74, 75], [54, 55]], loss = 0.5682535171508789\n",
            "Epoch = 6, pairs = [[86, 90], [46, 50], [3, 4], [48, 52]], loss = 0.6231321096420288\n",
            "Epoch = 6, pairs = [[53, 54], [4, 6], [16, 18], [1, 3]], loss = 0.35757213830947876\n",
            "Epoch = 6, pairs = [[6, 7], [56, 64], [0, 2], [12, 20]], loss = 0.8740897178649902\n",
            "Epoch = 6, pairs = [[46, 48], [71, 73], [25, 26], [30, 32]], loss = 0.4112740159034729\n",
            "Epoch = 6, pairs = [[50, 51], [48, 64], [6, 10], [59, 61]], loss = 0.7781194448471069\n",
            "Epoch = 6, pairs = [[4, 12], [36, 37], [17, 18], [13, 14]], loss = 0.40952253341674805\n",
            "Epoch = 6, pairs = [[67, 69], [81, 83], [60, 68], [72, 80]], loss = 0.6599847078323364\n",
            "Epoch = 6, pairs = [[56, 60], [54, 58], [82, 86], [17, 19]], loss = 0.6208703517913818\n",
            "Epoch = 6, pairs = [[16, 17], [6, 8], [26, 27], [78, 82]], loss = 0.2926185429096222\n",
            "Epoch = 6, pairs = [[87, 89], [86, 88], [56, 57], [24, 26]], loss = 0.3705807328224182\n",
            "Epoch = 6, pairs = [[81, 82], [10, 11], [39, 40], [50, 52]], loss = 0.30365583300590515\n",
            "Epoch = 6, pairs = [[80, 82], [52, 60], [66, 68], [71, 72]], loss = 0.5861057043075562\n",
            "Epoch = 6, pairs = [[70, 74], [26, 28], [78, 80], [4, 5]], loss = 0.4020833969116211\n",
            "Epoch = 6, pairs = [[16, 20], [41, 43], [35, 37], [74, 78]], loss = 0.5682754516601562\n",
            "Epoch = 6, pairs = [[72, 73], [40, 56], [2, 4], [76, 80]], loss = 0.6862623691558838\n",
            "Epoch = 6, pairs = [[76, 77], [19, 20], [85, 87], [22, 23]], loss = 0.25516217947006226\n",
            "Epoch = 6, pairs = [[54, 56], [64, 80], [89, 91], [34, 38]], loss = 0.9334760308265686\n",
            "Epoch = 6, pairs = [[68, 70], [38, 39], [85, 86], [36, 44]], loss = 0.5240033864974976\n",
            "Epoch = 6, pairs = [[44, 46], [28, 36], [12, 13], [42, 46]], loss = 0.5196815729141235\n",
            "Epoch = 6, pairs = [[52, 56], [37, 39], [5, 6], [19, 21]], loss = 0.37296736240386963\n",
            "Epoch = 6, pairs = [[75, 76], [59, 60], [66, 70], [55, 57]], loss = 0.4448001980781555\n",
            "Epoch = 6, pairs = [[34, 36], [28, 29], [88, 90], [36, 40]], loss = 0.35756659507751465\n",
            "Epoch = 6, pairs = [[88, 89], [79, 81], [29, 31], [70, 72]], loss = 0.3289223313331604\n",
            "Epoch = 6, pairs = [[31, 33], [43, 45], [72, 76], [40, 48]], loss = 0.5535094141960144\n",
            "Epoch = 6, pairs = [[0, 32], [57, 58], [16, 24], [4, 8]], loss = 0.85196852684021\n",
            "Epoch = 6, pairs = [[33, 34], [9, 10], [27, 28], [75, 77]], loss = 0.2871946096420288\n",
            "Epoch = 6, pairs = [[62, 63], [48, 56], [56, 58], [8, 16]], loss = 0.6203422546386719\n",
            "Epoch = 6, pairs = [[40, 42], [58, 62], [49, 51], [7, 8]], loss = 0.39135584235191345\n",
            "Epoch = 6, pairs = [[0, 16], [61, 63], [77, 79], [12, 14]], loss = 0.48727184534072876\n",
            "Epoch = 6, pairs = [[64, 72], [28, 32], [69, 70], [83, 85]], loss = 0.5317425727844238\n",
            "Epoch = 6, pairs = [[1, 2], [5, 7], [76, 78], [26, 30]], loss = 0.3182222843170166\n",
            "Epoch = 6, pairs = [[21, 23], [72, 74], [84, 88], [39, 41]], loss = 0.45429712533950806\n",
            "Epoch = 6, pairs = [[41, 42], [14, 16], [0, 8], [83, 84]], loss = 0.3813064694404602\n",
            "Epoch = 6, pairs = [[45, 46], [18, 19], [65, 67], [34, 35]], loss = 0.3251142203807831\n",
            "Epoch = 6, pairs = [[65, 66], [38, 40], [43, 44], [15, 17]], loss = 0.38616347312927246\n",
            "Epoch = 6, pairs = [[82, 84], [89, 90], [52, 53], [29, 30]], loss = 0.261990487575531\n",
            "Epoch = 6, pairs = [[80, 81], [20, 22], [64, 66], [42, 44]], loss = 0.3460596203804016\n",
            "Epoch = 6, pairs = [[0, 4], [32, 34], [53, 55], [87, 88]], loss = 0.34225788712501526\n",
            "Epoch = 6, pairs = [[80, 84], [51, 52], [51, 53], [67, 68]], loss = 0.41980528831481934\n",
            "Epoch = 6, pairs = [[61, 62], [11, 12], [72, 88], [64, 65]], loss = 0.4841386079788208\n",
            "Epoch = 6, pairs = [[46, 47], [86, 87], [66, 67], [15, 16]], loss = 0.30035313963890076\n",
            "Epoch = 6, pairs = [[80, 88], [73, 74], [23, 25], [60, 64]], loss = 0.5212181806564331\n",
            "Epoch = 6, pairs = [[40, 41], [82, 83], [73, 75], [31, 32]], loss = 0.3077504336833954\n",
            "Epoch = 6, pairs = [[76, 84], [70, 71], [56, 72], [21, 22]], loss = 0.5595499277114868\n",
            "Epoch = 6, pairs = [[48, 49], [22, 26], [58, 60], [11, 13]], loss = 0.3516784608364105\n",
            "Epoch = 6, pairs = [[13, 15], [48, 80], [27, 29], [69, 71]], loss = 0.6489949226379395\n",
            "Epoch = 6, pairs = [[40, 44], [20, 28], [8, 9], [77, 78]], loss = 0.4657790958881378\n",
            "Epoch = 6, pairs = [[0, 1], [74, 76], [23, 24], [49, 50]], loss = 0.24323323369026184\n",
            "Epoch = 6, pairs = [[48, 50], [52, 54], [10, 12], [8, 12]], loss = 0.3895992636680603\n",
            "Epoch = 6, pairs = [[57, 59], [68, 69], [55, 56], [33, 35]], loss = 0.39667361974716187\n",
            "Epoch = 6, pairs = [[62, 64], [78, 79], [38, 42], [24, 25]], loss = 0.4055960774421692\n",
            "Epoch = 6, pairs = [[16, 48], [25, 27], [7, 9], [32, 64]], loss = 1.1616766452789307\n",
            "Epoch = 6, pairs = [[63, 64], [68, 72], [64, 68], [32, 36]], loss = 0.4897896647453308\n",
            "Epoch = 6, pairs = [[47, 49], [14, 15], [30, 31], [32, 48]], loss = 0.5070661306381226\n",
            "Epoch = 6, pairs = [[16, 32], [36, 38], [32, 40], [24, 32]], loss = 0.873929500579834\n",
            "Epoch = 6, pairs = [[84, 86], [37, 38], [60, 61], [14, 18]], loss = 0.35978448390960693\n",
            "Epoch = 6, pairs = [[30, 34], [24, 28], [20, 24], [79, 80]], loss = 0.4044489562511444\n",
            "Epoch 6 took 85.19s.\n",
            "( 0,  1): reprojection: 0.231679, disparity: 0.064448\n",
            "( 0,  2): reprojection: 0.340302, disparity: 0.064747\n",
            "( 0,  4): reprojection: 0.366391, disparity: 0.077534\n",
            "( 0,  8): reprojection: 0.448628, disparity: 0.102052\n",
            "( 0, 16): reprojection: 0.736994, disparity: 0.102028\n",
            "( 0, 32): reprojection: 1.478627, disparity: 0.117615\n",
            "( 1,  2): reprojection: 0.192797, disparity: 0.050148\n",
            "( 1,  3): reprojection: 0.243899, disparity: 0.055304\n",
            "( 2,  3): reprojection: 0.107386, disparity: 0.053133\n",
            "( 2,  4): reprojection: 0.174374, disparity: 0.057203\n",
            "( 2,  6): reprojection: 0.346332, disparity: 0.071138\n",
            "( 3,  4): reprojection: 0.106506, disparity: 0.054334\n",
            "( 3,  5): reprojection: 0.198258, disparity: 0.057860\n",
            "( 4,  5): reprojection: 0.154958, disparity: 0.052317\n",
            "( 4,  6): reprojection: 0.223419, disparity: 0.055069\n",
            "( 4,  8): reprojection: 0.277416, disparity: 0.071279\n",
            "( 4, 12): reprojection: 0.423424, disparity: 0.076729\n",
            "( 5,  6): reprojection: 0.124085, disparity: 0.050552\n",
            "( 5,  7): reprojection: 0.158355, disparity: 0.054930\n",
            "( 6,  7): reprojection: 0.091569, disparity: 0.050167\n",
            "( 6,  8): reprojection: 0.153201, disparity: 0.057515\n",
            "( 6, 10): reprojection: 0.390930, disparity: 0.068335\n",
            "( 7,  8): reprojection: 0.093946, disparity: 0.052456\n",
            "( 7,  9): reprojection: 0.200636, disparity: 0.059295\n",
            "( 8,  9): reprojection: 0.166221, disparity: 0.052234\n",
            "( 8, 10): reprojection: 0.374456, disparity: 0.059605\n",
            "( 8, 12): reprojection: 0.518526, disparity: 0.068198\n",
            "( 8, 16): reprojection: 0.663977, disparity: 0.081976\n",
            "( 8, 24): reprojection: 0.916430, disparity: 0.096922\n",
            "( 9, 10): reprojection: 0.263863, disparity: 0.053588\n",
            "( 9, 11): reprojection: 0.396521, disparity: 0.060881\n",
            "(10, 11): reprojection: 0.178059, disparity: 0.047574\n",
            "(10, 12): reprojection: 0.215025, disparity: 0.055773\n",
            "(10, 14): reprojection: 0.350742, disparity: 0.072470\n",
            "(11, 12): reprojection: 0.138411, disparity: 0.047441\n",
            "(11, 13): reprojection: 0.226640, disparity: 0.058304\n",
            "(12, 13): reprojection: 0.139874, disparity: 0.049661\n",
            "(12, 14): reprojection: 0.275520, disparity: 0.056982\n",
            "(12, 16): reprojection: 0.426026, disparity: 0.086957\n",
            "(12, 20): reprojection: 0.751755, disparity: 0.087647\n",
            "(13, 14): reprojection: 0.191020, disparity: 0.048597\n",
            "(13, 15): reprojection: 0.227870, disparity: 0.062747\n",
            "(14, 15): reprojection: 0.194411, disparity: 0.053751\n",
            "(14, 16): reprojection: 0.447270, disparity: 0.065270\n",
            "(14, 18): reprojection: 0.381137, disparity: 0.072466\n",
            "(15, 16): reprojection: 0.302535, disparity: 0.053995\n",
            "(15, 17): reprojection: 0.386357, disparity: 0.065121\n",
            "(16, 17): reprojection: 0.146803, disparity: 0.052936\n",
            "(16, 18): reprojection: 0.350550, disparity: 0.068536\n",
            "(16, 20): reprojection: 0.598367, disparity: 0.076004\n",
            "(16, 24): reprojection: 0.934395, disparity: 0.100343\n",
            "(16, 32): reprojection: 1.245194, disparity: 0.110312\n",
            "(16, 48): reprojection: 2.255577, disparity: 0.290796\n",
            "(17, 18): reprojection: 0.339118, disparity: 0.056656\n",
            "(17, 19): reprojection: 0.519689, disparity: 0.069445\n",
            "(18, 19): reprojection: 0.226778, disparity: 0.052441\n",
            "(18, 20): reprojection: 0.325482, disparity: 0.061253\n",
            "(18, 22): reprojection: 0.503030, disparity: 0.073728\n",
            "(19, 20): reprojection: 0.119121, disparity: 0.051411\n",
            "(19, 21): reprojection: 0.270709, disparity: 0.062630\n",
            "(20, 21): reprojection: 0.199023, disparity: 0.054311\n",
            "(20, 22): reprojection: 0.295071, disparity: 0.064645\n",
            "(20, 24): reprojection: 0.385859, disparity: 0.081297\n",
            "(20, 28): reprojection: 0.564666, disparity: 0.125506\n",
            "(21, 22): reprojection: 0.158716, disparity: 0.055755\n",
            "(21, 23): reprojection: 0.238100, disparity: 0.068479\n",
            "(22, 23): reprojection: 0.125437, disparity: 0.054522\n",
            "(22, 24): reprojection: 0.173999, disparity: 0.066984\n",
            "(22, 26): reprojection: 0.336906, disparity: 0.083504\n",
            "(23, 24): reprojection: 0.112719, disparity: 0.053759\n",
            "(23, 25): reprojection: 0.200977, disparity: 0.063774\n",
            "(24, 25): reprojection: 0.164732, disparity: 0.061163\n",
            "(24, 26): reprojection: 0.214379, disparity: 0.079310\n",
            "(24, 28): reprojection: 0.306614, disparity: 0.087557\n",
            "(24, 32): reprojection: 0.630404, disparity: 0.095857\n",
            "(24, 40): reprojection: 1.062463, disparity: 0.177295\n",
            "(25, 26): reprojection: 0.136774, disparity: 0.065206\n",
            "(25, 27): reprojection: 0.238699, disparity: 0.078295\n",
            "(26, 27): reprojection: 0.156195, disparity: 0.061417\n",
            "(26, 28): reprojection: 0.186322, disparity: 0.066202\n",
            "(26, 30): reprojection: 0.331658, disparity: 0.075462\n",
            "(27, 28): reprojection: 0.155942, disparity: 0.060864\n",
            "(27, 29): reprojection: 0.276695, disparity: 0.067099\n",
            "(28, 29): reprojection: 0.205625, disparity: 0.061073\n",
            "(28, 30): reprojection: 0.265381, disparity: 0.068787\n",
            "(28, 32): reprojection: 0.427316, disparity: 0.085035\n",
            "(28, 36): reprojection: 0.649184, disparity: 0.129842\n",
            "(29, 30): reprojection: 0.141068, disparity: 0.055720\n",
            "(29, 31): reprojection: 0.275978, disparity: 0.063547\n",
            "(30, 31): reprojection: 0.202048, disparity: 0.056142\n",
            "(30, 32): reprojection: 0.271786, disparity: 0.066510\n",
            "(30, 34): reprojection: 0.442802, disparity: 0.098376\n",
            "(31, 32): reprojection: 0.145393, disparity: 0.061274\n",
            "(31, 33): reprojection: 0.304133, disparity: 0.073113\n",
            "(32, 33): reprojection: 0.244895, disparity: 0.068994\n",
            "(32, 34): reprojection: 0.245920, disparity: 0.077157\n",
            "(32, 36): reprojection: 0.345244, disparity: 0.100606\n",
            "(32, 40): reprojection: 0.578822, disparity: 0.164406\n",
            "(32, 48): reprojection: 1.201833, disparity: 0.298649\n",
            "(32, 64): reprojection: 2.768085, disparity: 0.277572\n",
            "(33, 34): reprojection: 0.161170, disparity: 0.059692\n",
            "(33, 35): reprojection: 0.251090, disparity: 0.070927\n",
            "(34, 35): reprojection: 0.182741, disparity: 0.059791\n",
            "(34, 36): reprojection: 0.289970, disparity: 0.079691\n",
            "(34, 38): reprojection: 0.365519, disparity: 0.103954\n",
            "(35, 36): reprojection: 0.252987, disparity: 0.069537\n",
            "(35, 37): reprojection: 0.303772, disparity: 0.089317\n",
            "(36, 37): reprojection: 0.264314, disparity: 0.074452\n",
            "(36, 38): reprojection: 0.397504, disparity: 0.078848\n",
            "(36, 40): reprojection: 0.373247, disparity: 0.102026\n",
            "(36, 44): reprojection: 0.760441, disparity: 0.190061\n",
            "(37, 38): reprojection: 0.225432, disparity: 0.064444\n",
            "(37, 39): reprojection: 0.237933, disparity: 0.083684\n",
            "(38, 39): reprojection: 0.197385, disparity: 0.068520\n",
            "(38, 40): reprojection: 0.414651, disparity: 0.089026\n",
            "(38, 42): reprojection: 0.837787, disparity: 0.164165\n",
            "(39, 40): reprojection: 0.301108, disparity: 0.066521\n",
            "(39, 41): reprojection: 0.536827, disparity: 0.090780\n",
            "(40, 41): reprojection: 0.310043, disparity: 0.077714\n",
            "(40, 42): reprojection: 0.489622, disparity: 0.099017\n",
            "(40, 44): reprojection: 0.624663, disparity: 0.117559\n",
            "(40, 48): reprojection: 0.833134, disparity: 0.195721\n",
            "(40, 56): reprojection: 1.133039, disparity: 0.210875\n",
            "(41, 42): reprojection: 0.238948, disparity: 0.075296\n",
            "(41, 43): reprojection: 0.323300, disparity: 0.093502\n",
            "(42, 43): reprojection: 0.183945, disparity: 0.069706\n",
            "(42, 44): reprojection: 0.339118, disparity: 0.084575\n",
            "(42, 46): reprojection: 0.535299, disparity: 0.107290\n",
            "(43, 44): reprojection: 0.207736, disparity: 0.069890\n",
            "(43, 45): reprojection: 0.300255, disparity: 0.087280\n",
            "(44, 45): reprojection: 0.160517, disparity: 0.068473\n",
            "(44, 46): reprojection: 0.303185, disparity: 0.083777\n",
            "(44, 48): reprojection: 0.443565, disparity: 0.138652\n",
            "(44, 52): reprojection: 0.671748, disparity: 0.180151\n",
            "(45, 46): reprojection: 0.208064, disparity: 0.073629\n",
            "(45, 47): reprojection: 0.351158, disparity: 0.119268\n",
            "(46, 47): reprojection: 0.200610, disparity: 0.081383\n",
            "(46, 48): reprojection: 0.323479, disparity: 0.100473\n",
            "(46, 50): reprojection: 0.468044, disparity: 0.119889\n",
            "(47, 48): reprojection: 0.211848, disparity: 0.071980\n",
            "(47, 49): reprojection: 0.280414, disparity: 0.085359\n",
            "(48, 49): reprojection: 0.203915, disparity: 0.078180\n",
            "(48, 50): reprojection: 0.257975, disparity: 0.085551\n",
            "(48, 52): reprojection: 0.382074, disparity: 0.103564\n",
            "(48, 56): reprojection: 0.770592, disparity: 0.121405\n",
            "(48, 64): reprojection: 1.752240, disparity: 0.153943\n",
            "(48, 80): reprojection: 3.458382, disparity: 0.272001\n",
            "(49, 50): reprojection: 0.157323, disparity: 0.070563\n",
            "(49, 51): reprojection: 0.255107, disparity: 0.083164\n",
            "(50, 51): reprojection: 0.160031, disparity: 0.071071\n",
            "(50, 52): reprojection: 0.287882, disparity: 0.082497\n",
            "(50, 54): reprojection: 0.535893, disparity: 0.101972\n",
            "(51, 52): reprojection: 0.195254, disparity: 0.074582\n",
            "(51, 53): reprojection: 0.316010, disparity: 0.082258\n",
            "(52, 53): reprojection: 0.192232, disparity: 0.073268\n",
            "(52, 54): reprojection: 0.280608, disparity: 0.086914\n",
            "(52, 56): reprojection: 0.640485, disparity: 0.090084\n",
            "(52, 60): reprojection: 0.911015, disparity: 0.112341\n",
            "(53, 54): reprojection: 0.170127, disparity: 0.073750\n",
            "(53, 55): reprojection: 0.375411, disparity: 0.084556\n",
            "(54, 55): reprojection: 0.318847, disparity: 0.067909\n",
            "(54, 56): reprojection: 0.611209, disparity: 0.078291\n",
            "(54, 58): reprojection: 0.473127, disparity: 0.084646\n",
            "(55, 56): reprojection: 0.349353, disparity: 0.072931\n",
            "(55, 57): reprojection: 0.406345, disparity: 0.080457\n",
            "(56, 57): reprojection: 0.326156, disparity: 0.071579\n",
            "(56, 58): reprojection: 0.596439, disparity: 0.086110\n",
            "(56, 60): reprojection: 0.843185, disparity: 0.105345\n",
            "(56, 64): reprojection: 1.074388, disparity: 0.130578\n",
            "(56, 72): reprojection: 2.072290, disparity: 0.187116\n",
            "(57, 58): reprojection: 0.387718, disparity: 0.078976\n",
            "(57, 59): reprojection: 0.548634, disparity: 0.086119\n",
            "(58, 59): reprojection: 0.263154, disparity: 0.077741\n",
            "(58, 60): reprojection: 0.405903, disparity: 0.098022\n",
            "(58, 62): reprojection: 0.631051, disparity: 0.113735\n",
            "(59, 60): reprojection: 0.203422, disparity: 0.082801\n",
            "(59, 61): reprojection: 0.316562, disparity: 0.088080\n",
            "(60, 61): reprojection: 0.241309, disparity: 0.079123\n",
            "(60, 62): reprojection: 0.336287, disparity: 0.086566\n",
            "(60, 64): reprojection: 0.568134, disparity: 0.097451\n",
            "(60, 68): reprojection: 1.183145, disparity: 0.146666\n",
            "(61, 62): reprojection: 0.224158, disparity: 0.078779\n",
            "(61, 63): reprojection: 0.376555, disparity: 0.090526\n",
            "(62, 63): reprojection: 0.248572, disparity: 0.078198\n",
            "(62, 64): reprojection: 0.383733, disparity: 0.085023\n",
            "(62, 66): reprojection: 0.697068, disparity: 0.131873\n",
            "(63, 64): reprojection: 0.293482, disparity: 0.081633\n",
            "(63, 65): reprojection: 0.435011, disparity: 0.087340\n",
            "(64, 65): reprojection: 0.284347, disparity: 0.089636\n",
            "(64, 66): reprojection: 0.445616, disparity: 0.086689\n",
            "(64, 68): reprojection: 0.530780, disparity: 0.107361\n",
            "(64, 72): reprojection: 0.830297, disparity: 0.130263\n",
            "(64, 80): reprojection: 1.432047, disparity: 0.224586\n",
            "(65, 66): reprojection: 0.293178, disparity: 0.072473\n",
            "(65, 67): reprojection: 0.472755, disparity: 0.082272\n",
            "(66, 67): reprojection: 0.272432, disparity: 0.079157\n",
            "(66, 68): reprojection: 0.447215, disparity: 0.086071\n",
            "(66, 70): reprojection: 0.641815, disparity: 0.106337\n",
            "(67, 68): reprojection: 0.374595, disparity: 0.073915\n",
            "(67, 69): reprojection: 0.465648, disparity: 0.080729\n",
            "(68, 69): reprojection: 0.241784, disparity: 0.073073\n",
            "(68, 70): reprojection: 0.513237, disparity: 0.081823\n",
            "(68, 72): reprojection: 0.623132, disparity: 0.100346\n",
            "(68, 76): reprojection: 0.922194, disparity: 0.134185\n",
            "(69, 70): reprojection: 0.361999, disparity: 0.074103\n",
            "(69, 71): reprojection: 0.532710, disparity: 0.079088\n",
            "(70, 71): reprojection: 0.253093, disparity: 0.072378\n",
            "(70, 72): reprojection: 0.309983, disparity: 0.079268\n",
            "(70, 74): reprojection: 0.519581, disparity: 0.103524\n",
            "(71, 72): reprojection: 0.270108, disparity: 0.075461\n",
            "(71, 73): reprojection: 0.417748, disparity: 0.086620\n",
            "(72, 73): reprojection: 0.250130, disparity: 0.075769\n",
            "(72, 74): reprojection: 0.362613, disparity: 0.089668\n",
            "(72, 76): reprojection: 0.550592, disparity: 0.107922\n",
            "(72, 80): reprojection: 0.701848, disparity: 0.141353\n",
            "(72, 88): reprojection: 1.224520, disparity: 0.160568\n",
            "(73, 74): reprojection: 0.335413, disparity: 0.076840\n",
            "(73, 75): reprojection: 0.483805, disparity: 0.082305\n",
            "(74, 75): reprojection: 0.261532, disparity: 0.071091\n",
            "(74, 76): reprojection: 0.301687, disparity: 0.077569\n",
            "(74, 78): reprojection: 0.452633, disparity: 0.089753\n",
            "(75, 76): reprojection: 0.257058, disparity: 0.073438\n",
            "(75, 77): reprojection: 0.413530, disparity: 0.082272\n",
            "(76, 77): reprojection: 0.250055, disparity: 0.068481\n",
            "(76, 78): reprojection: 0.366023, disparity: 0.074277\n",
            "(76, 80): reprojection: 0.412934, disparity: 0.092508\n",
            "(76, 84): reprojection: 0.627198, disparity: 0.100641\n",
            "(77, 78): reprojection: 0.244441, disparity: 0.067637\n",
            "(77, 79): reprojection: 0.287620, disparity: 0.080206\n",
            "(78, 79): reprojection: 0.202890, disparity: 0.073407\n",
            "(78, 80): reprojection: 0.257546, disparity: 0.084794\n",
            "(78, 82): reprojection: 0.426563, disparity: 0.092141\n",
            "(79, 80): reprojection: 0.155798, disparity: 0.068083\n",
            "(79, 81): reprojection: 0.256394, disparity: 0.074050\n",
            "(80, 81): reprojection: 0.161317, disparity: 0.066041\n",
            "(80, 82): reprojection: 0.262933, disparity: 0.073388\n",
            "(80, 84): reprojection: 0.502151, disparity: 0.079323\n",
            "(80, 88): reprojection: 0.786815, disparity: 0.108968\n",
            "(81, 82): reprojection: 0.196006, disparity: 0.071344\n",
            "(81, 83): reprojection: 0.349899, disparity: 0.076779\n",
            "(82, 83): reprojection: 0.169801, disparity: 0.063923\n",
            "(82, 84): reprojection: 0.267587, disparity: 0.072043\n",
            "(82, 86): reprojection: 0.474408, disparity: 0.090674\n",
            "(83, 84): reprojection: 0.157050, disparity: 0.065569\n",
            "(83, 85): reprojection: 0.332020, disparity: 0.081540\n",
            "(84, 85): reprojection: 0.177389, disparity: 0.065693\n",
            "(84, 86): reprojection: 0.273031, disparity: 0.075626\n",
            "(84, 88): reprojection: 0.541700, disparity: 0.088651\n",
            "(85, 86): reprojection: 0.174663, disparity: 0.062648\n",
            "(85, 87): reprojection: 0.290581, disparity: 0.071832\n",
            "(86, 87): reprojection: 0.242560, disparity: 0.064841\n",
            "(86, 88): reprojection: 0.361113, disparity: 0.079711\n",
            "(86, 90): reprojection: 0.498532, disparity: 0.086809\n",
            "(87, 88): reprojection: 0.196922, disparity: 0.069418\n",
            "(87, 89): reprojection: 0.298147, disparity: 0.079112\n",
            "(88, 89): reprojection: 0.143757, disparity: 0.062928\n",
            "(88, 90): reprojection: 0.310638, disparity: 0.069378\n",
            "(89, 90): reprojection: 0.212623, disparity: 0.058359\n",
            "(89, 91): reprojection: 0.436917, disparity: 0.074863\n",
            "(90, 91): reprojection: 0.351695, disparity: 0.064899\n",
            "Mean:     reprojection: 0.351695, disparity: 0.064899\n",
            "Done Validation for epoch 7 (1820 iterations)\n",
            "Epoch = 7, pairs = [[65, 67], [56, 60], [17, 19], [69, 71]], loss = 0.6595931053161621\n",
            "Epoch = 7, pairs = [[41, 43], [2, 4], [39, 41], [71, 72]], loss = 0.41219276189804077\n",
            "Epoch = 7, pairs = [[58, 60], [4, 12], [62, 63], [46, 48]], loss = 0.46712571382522583\n",
            "Epoch = 7, pairs = [[16, 48], [41, 42], [38, 39], [35, 37]], loss = 1.0394843816757202\n",
            "Epoch = 7, pairs = [[16, 20], [64, 68], [80, 84], [16, 32]], loss = 0.8592019081115723\n",
            "Epoch = 7, pairs = [[19, 21], [10, 14], [54, 56], [1, 3]], loss = 0.4392528235912323\n",
            "Epoch = 7, pairs = [[48, 80], [50, 52], [34, 38], [28, 29]], loss = 0.6499238610267639\n",
            "Epoch = 7, pairs = [[89, 91], [83, 85], [0, 1], [82, 84]], loss = 0.3882453739643097\n",
            "Epoch = 7, pairs = [[64, 66], [54, 55], [48, 49], [76, 77]], loss = 0.32486385107040405\n",
            "Epoch = 7, pairs = [[55, 57], [18, 22], [64, 80], [78, 80]], loss = 0.7730579972267151\n",
            "Epoch = 7, pairs = [[13, 15], [52, 53], [72, 76], [40, 56]], loss = 0.5915555357933044\n",
            "Epoch = 7, pairs = [[84, 85], [32, 36], [18, 19], [5, 6]], loss = 0.28311991691589355\n",
            "Epoch = 7, pairs = [[50, 51], [49, 50], [56, 58], [73, 75]], loss = 0.40192219614982605\n",
            "Epoch = 7, pairs = [[75, 77], [0, 8], [81, 82], [45, 46]], loss = 0.3877713680267334\n",
            "Epoch = 7, pairs = [[21, 22], [34, 36], [87, 89], [4, 6]], loss = 0.29438477754592896\n",
            "Epoch = 7, pairs = [[8, 12], [76, 80], [16, 18], [56, 72]], loss = 0.7637947201728821\n",
            "Epoch = 7, pairs = [[60, 62], [4, 5], [46, 50], [0, 4]], loss = 0.3966163098812103\n",
            "Epoch = 7, pairs = [[61, 62], [2, 6], [66, 67], [42, 46]], loss = 0.425350159406662\n",
            "Epoch = 7, pairs = [[75, 76], [21, 23], [44, 46], [74, 78]], loss = 0.337419331073761\n",
            "Epoch = 7, pairs = [[0, 16], [29, 30], [11, 12], [78, 79]], loss = 0.3808431923389435\n",
            "Epoch = 7, pairs = [[6, 10], [12, 16], [42, 44], [7, 9]], loss = 0.41377362608909607\n",
            "Epoch = 7, pairs = [[23, 24], [59, 60], [60, 61], [84, 88]], loss = 0.3287225365638733\n",
            "Epoch = 7, pairs = [[10, 12], [73, 74], [70, 72], [20, 22]], loss = 0.3544718325138092\n",
            "Epoch = 7, pairs = [[56, 57], [27, 29], [88, 89], [34, 35]], loss = 0.2916785180568695\n",
            "Epoch = 7, pairs = [[31, 33], [33, 35], [56, 64], [20, 28]], loss = 0.5922996401786804\n",
            "Epoch = 7, pairs = [[70, 74], [8, 24], [36, 38], [90, 91]], loss = 0.563133716583252\n",
            "Epoch = 7, pairs = [[16, 17], [47, 48], [78, 82], [22, 26]], loss = 0.3525330424308777\n",
            "Epoch = 7, pairs = [[77, 78], [72, 73], [57, 59], [68, 69]], loss = 0.3530837893486023\n",
            "Epoch = 7, pairs = [[67, 68], [28, 36], [79, 80], [24, 26]], loss = 0.4402537941932678\n",
            "Epoch = 7, pairs = [[6, 7], [71, 73], [31, 32], [37, 39]], loss = 0.27034661173820496\n",
            "Epoch = 7, pairs = [[36, 44], [66, 70], [64, 72], [52, 60]], loss = 0.7681743502616882\n",
            "Epoch = 7, pairs = [[72, 80], [0, 2], [44, 45], [74, 75]], loss = 0.37490373849868774\n",
            "Epoch = 7, pairs = [[1, 2], [67, 69], [15, 17], [9, 10]], loss = 0.36051517724990845\n",
            "Epoch = 7, pairs = [[38, 42], [10, 11], [9, 11], [12, 20]], loss = 0.5954415798187256\n",
            "Epoch = 7, pairs = [[23, 25], [6, 8], [30, 32], [52, 54]], loss = 0.28172436356544495\n",
            "Epoch = 7, pairs = [[48, 56], [28, 32], [72, 88], [43, 44]], loss = 0.6465378999710083\n",
            "Epoch = 7, pairs = [[52, 56], [32, 48], [19, 20], [77, 79]], loss = 0.5362861752510071\n",
            "Epoch = 7, pairs = [[65, 66], [37, 38], [62, 66], [17, 18]], loss = 0.39325660467147827\n",
            "Epoch = 7, pairs = [[85, 87], [20, 24], [49, 51], [68, 76]], loss = 0.46574538946151733\n",
            "Epoch = 7, pairs = [[3, 4], [51, 53], [60, 68], [58, 62]], loss = 0.5073214769363403\n",
            "Epoch = 7, pairs = [[7, 8], [72, 74], [24, 32], [32, 33]], loss = 0.38870102167129517\n",
            "Epoch = 7, pairs = [[35, 36], [86, 87], [12, 13], [39, 40]], loss = 0.2737119197845459\n",
            "Epoch = 7, pairs = [[4, 8], [68, 72], [2, 3], [53, 54]], loss = 0.3244951069355011\n",
            "Epoch = 7, pairs = [[11, 13], [45, 47], [18, 20], [63, 65]], loss = 0.37189480662345886\n",
            "Epoch = 7, pairs = [[54, 58], [82, 86], [26, 27], [24, 40]], loss = 0.5685206651687622\n",
            "Epoch = 7, pairs = [[25, 27], [84, 86], [29, 31], [76, 84]], loss = 0.3799804151058197\n",
            "Epoch = 7, pairs = [[8, 16], [85, 86], [59, 61], [16, 24]], loss = 0.5439715385437012\n",
            "Epoch = 7, pairs = [[0, 32], [76, 78], [58, 59], [22, 23]], loss = 0.6330464482307434\n",
            "Epoch = 7, pairs = [[43, 45], [80, 88], [26, 30], [36, 37]], loss = 0.4791112542152405\n",
            "Epoch = 7, pairs = [[80, 82], [48, 50], [14, 18], [22, 24]], loss = 0.31038898229599\n",
            "Epoch = 7, pairs = [[8, 10], [63, 64], [3, 5], [15, 16]], loss = 0.3282821774482727\n",
            "Epoch = 7, pairs = [[32, 40], [38, 40], [24, 28], [8, 9]], loss = 0.4159926474094391\n",
            "Epoch = 7, pairs = [[55, 56], [32, 64], [53, 55], [79, 81]], loss = 0.7832232117652893\n",
            "Epoch = 7, pairs = [[47, 49], [42, 43], [14, 15], [89, 90]], loss = 0.26484254002571106\n",
            "Epoch = 7, pairs = [[40, 42], [20, 21], [30, 34], [28, 30]], loss = 0.39730051159858704\n",
            "Epoch = 7, pairs = [[40, 44], [33, 34], [36, 40], [25, 26]], loss = 0.3824595808982849\n",
            "Epoch = 7, pairs = [[40, 41], [88, 90], [74, 76], [30, 31]], loss = 0.3283679485321045\n",
            "Epoch = 7, pairs = [[82, 83], [87, 88], [86, 88], [80, 81]], loss = 0.2767884433269501\n",
            "Epoch = 7, pairs = [[50, 54], [46, 47], [48, 64], [62, 64]], loss = 0.7513989210128784\n",
            "Epoch = 7, pairs = [[12, 14], [13, 14], [26, 28], [83, 84]], loss = 0.2609032094478607\n",
            "Epoch = 7, pairs = [[64, 65], [69, 70], [32, 34], [57, 58]], loss = 0.35639166831970215\n",
            "Epoch = 7, pairs = [[48, 52], [27, 28], [66, 68], [61, 63]], loss = 0.3775160610675812\n",
            "Epoch = 7, pairs = [[44, 52], [40, 48], [5, 7], [86, 90]], loss = 0.6686245799064636\n",
            "Epoch = 7, pairs = [[60, 64], [70, 71], [14, 16], [44, 48]], loss = 0.47412633895874023\n",
            "Epoch = 7, pairs = [[68, 70], [51, 52], [24, 25], [81, 83]], loss = 0.3248414397239685\n",
            "Epoch 7 took 84.44s.\n",
            "( 0,  1): reprojection: 0.231010, disparity: 0.056658\n",
            "( 0,  2): reprojection: 0.341985, disparity: 0.063527\n",
            "( 0,  4): reprojection: 0.379864, disparity: 0.083847\n",
            "( 0,  8): reprojection: 0.472304, disparity: 0.113386\n",
            "( 0, 16): reprojection: 0.800884, disparity: 0.138848\n",
            "( 0, 32): reprojection: 1.052782, disparity: 0.230068\n",
            "( 1,  2): reprojection: 0.192888, disparity: 0.046865\n",
            "( 1,  3): reprojection: 0.246816, disparity: 0.057999\n",
            "( 2,  3): reprojection: 0.109202, disparity: 0.044527\n",
            "( 2,  4): reprojection: 0.177562, disparity: 0.056861\n",
            "( 2,  6): reprojection: 0.353552, disparity: 0.071087\n",
            "( 3,  4): reprojection: 0.104701, disparity: 0.044922\n",
            "( 3,  5): reprojection: 0.203910, disparity: 0.055109\n",
            "( 4,  5): reprojection: 0.156502, disparity: 0.047385\n",
            "( 4,  6): reprojection: 0.223947, disparity: 0.051303\n",
            "( 4,  8): reprojection: 0.286178, disparity: 0.061416\n",
            "( 4, 12): reprojection: 0.413759, disparity: 0.079639\n",
            "( 5,  6): reprojection: 0.112945, disparity: 0.045026\n",
            "( 5,  7): reprojection: 0.150647, disparity: 0.049772\n",
            "( 6,  7): reprojection: 0.092686, disparity: 0.043687\n",
            "( 6,  8): reprojection: 0.157601, disparity: 0.048357\n",
            "( 6, 10): reprojection: 0.384825, disparity: 0.061625\n",
            "( 7,  8): reprojection: 0.085041, disparity: 0.044168\n",
            "( 7,  9): reprojection: 0.190215, disparity: 0.052449\n",
            "( 8,  9): reprojection: 0.157933, disparity: 0.046571\n",
            "( 8, 10): reprojection: 0.360874, disparity: 0.053142\n",
            "( 8, 12): reprojection: 0.512202, disparity: 0.063153\n",
            "( 8, 16): reprojection: 0.651085, disparity: 0.085007\n",
            "( 8, 24): reprojection: 0.799613, disparity: 0.160200\n",
            "( 9, 10): reprojection: 0.253096, disparity: 0.049150\n",
            "( 9, 11): reprojection: 0.379257, disparity: 0.055981\n",
            "(10, 11): reprojection: 0.169598, disparity: 0.043448\n",
            "(10, 12): reprojection: 0.209893, disparity: 0.054273\n",
            "(10, 14): reprojection: 0.340763, disparity: 0.077845\n",
            "(11, 12): reprojection: 0.138645, disparity: 0.045037\n",
            "(11, 13): reprojection: 0.228969, disparity: 0.058352\n",
            "(12, 13): reprojection: 0.132768, disparity: 0.044323\n",
            "(12, 14): reprojection: 0.257931, disparity: 0.055858\n",
            "(12, 16): reprojection: 0.420712, disparity: 0.076259\n",
            "(12, 20): reprojection: 0.685400, disparity: 0.107306\n",
            "(13, 14): reprojection: 0.183124, disparity: 0.046884\n",
            "(13, 15): reprojection: 0.220125, disparity: 0.055337\n",
            "(14, 15): reprojection: 0.182855, disparity: 0.044888\n",
            "(14, 16): reprojection: 0.427368, disparity: 0.055449\n",
            "(14, 18): reprojection: 0.346833, disparity: 0.074331\n",
            "(15, 16): reprojection: 0.291325, disparity: 0.048134\n",
            "(15, 17): reprojection: 0.379887, disparity: 0.058684\n",
            "(16, 17): reprojection: 0.137302, disparity: 0.046302\n",
            "(16, 18): reprojection: 0.314629, disparity: 0.063840\n",
            "(16, 20): reprojection: 0.579690, disparity: 0.085717\n",
            "(16, 24): reprojection: 0.822984, disparity: 0.117756\n",
            "(16, 32): reprojection: 1.225824, disparity: 0.168716\n",
            "(16, 48): reprojection: 1.492709, disparity: 0.161707\n",
            "(17, 18): reprojection: 0.308480, disparity: 0.053059\n",
            "(17, 19): reprojection: 0.484012, disparity: 0.069455\n",
            "(18, 19): reprojection: 0.216272, disparity: 0.049538\n",
            "(18, 20): reprojection: 0.311970, disparity: 0.060564\n",
            "(18, 22): reprojection: 0.493100, disparity: 0.079683\n",
            "(19, 20): reprojection: 0.119185, disparity: 0.047053\n",
            "(19, 21): reprojection: 0.259273, disparity: 0.061696\n",
            "(20, 21): reprojection: 0.178204, disparity: 0.049694\n",
            "(20, 22): reprojection: 0.273459, disparity: 0.063325\n",
            "(20, 24): reprojection: 0.365024, disparity: 0.076574\n",
            "(20, 28): reprojection: 0.582933, disparity: 0.104506\n",
            "(21, 22): reprojection: 0.148001, disparity: 0.051346\n",
            "(21, 23): reprojection: 0.228227, disparity: 0.058718\n",
            "(22, 23): reprojection: 0.117289, disparity: 0.049862\n",
            "(22, 24): reprojection: 0.165209, disparity: 0.060591\n",
            "(22, 26): reprojection: 0.345266, disparity: 0.076079\n",
            "(23, 24): reprojection: 0.103239, disparity: 0.049195\n",
            "(23, 25): reprojection: 0.178571, disparity: 0.059324\n",
            "(24, 25): reprojection: 0.118024, disparity: 0.051125\n",
            "(24, 26): reprojection: 0.175161, disparity: 0.060641\n",
            "(24, 28): reprojection: 0.333170, disparity: 0.075539\n",
            "(24, 32): reprojection: 0.652950, disparity: 0.100909\n",
            "(24, 40): reprojection: 0.932607, disparity: 0.165505\n",
            "(25, 26): reprojection: 0.124322, disparity: 0.054367\n",
            "(25, 27): reprojection: 0.225930, disparity: 0.061329\n",
            "(26, 27): reprojection: 0.138388, disparity: 0.053434\n",
            "(26, 28): reprojection: 0.187968, disparity: 0.065766\n",
            "(26, 30): reprojection: 0.339106, disparity: 0.086917\n",
            "(27, 28): reprojection: 0.130171, disparity: 0.051728\n",
            "(27, 29): reprojection: 0.249092, disparity: 0.062589\n",
            "(28, 29): reprojection: 0.166799, disparity: 0.053739\n",
            "(28, 30): reprojection: 0.246796, disparity: 0.063631\n",
            "(28, 32): reprojection: 0.440376, disparity: 0.078885\n",
            "(28, 36): reprojection: 0.667343, disparity: 0.104810\n",
            "(29, 30): reprojection: 0.132567, disparity: 0.051350\n",
            "(29, 31): reprojection: 0.275369, disparity: 0.057316\n",
            "(30, 31): reprojection: 0.188510, disparity: 0.049532\n",
            "(30, 32): reprojection: 0.257154, disparity: 0.060988\n",
            "(30, 34): reprojection: 0.474614, disparity: 0.075440\n",
            "(31, 32): reprojection: 0.124798, disparity: 0.052341\n",
            "(31, 33): reprojection: 0.307865, disparity: 0.058849\n",
            "(32, 33): reprojection: 0.243492, disparity: 0.056763\n",
            "(32, 34): reprojection: 0.250402, disparity: 0.065382\n",
            "(32, 36): reprojection: 0.352087, disparity: 0.087342\n",
            "(32, 40): reprojection: 0.475437, disparity: 0.131928\n",
            "(32, 48): reprojection: 0.935446, disparity: 0.158430\n",
            "(32, 64): reprojection: 1.255281, disparity: 0.156165\n",
            "(33, 34): reprojection: 0.144745, disparity: 0.051285\n",
            "(33, 35): reprojection: 0.251312, disparity: 0.067314\n",
            "(34, 35): reprojection: 0.178403, disparity: 0.055656\n",
            "(34, 36): reprojection: 0.265777, disparity: 0.068991\n",
            "(34, 38): reprojection: 0.376317, disparity: 0.087426\n",
            "(35, 36): reprojection: 0.219633, disparity: 0.057300\n",
            "(35, 37): reprojection: 0.251045, disparity: 0.068257\n",
            "(36, 37): reprojection: 0.254256, disparity: 0.057188\n",
            "(36, 38): reprojection: 0.389802, disparity: 0.064831\n",
            "(36, 40): reprojection: 0.309354, disparity: 0.084716\n",
            "(36, 44): reprojection: 0.620675, disparity: 0.112280\n",
            "(37, 38): reprojection: 0.224693, disparity: 0.054318\n",
            "(37, 39): reprojection: 0.241002, disparity: 0.069247\n",
            "(38, 39): reprojection: 0.185676, disparity: 0.059247\n",
            "(38, 40): reprojection: 0.355455, disparity: 0.075710\n",
            "(38, 42): reprojection: 0.652788, disparity: 0.094420\n",
            "(39, 40): reprojection: 0.269895, disparity: 0.057131\n",
            "(39, 41): reprojection: 0.478367, disparity: 0.068991\n",
            "(40, 41): reprojection: 0.289477, disparity: 0.057952\n",
            "(40, 42): reprojection: 0.417128, disparity: 0.069676\n",
            "(40, 44): reprojection: 0.539111, disparity: 0.089970\n",
            "(40, 48): reprojection: 0.756237, disparity: 0.105725\n",
            "(40, 56): reprojection: 0.776902, disparity: 0.184293\n",
            "(41, 42): reprojection: 0.186254, disparity: 0.058130\n",
            "(41, 43): reprojection: 0.284319, disparity: 0.067154\n",
            "(42, 43): reprojection: 0.172645, disparity: 0.060425\n",
            "(42, 44): reprojection: 0.315542, disparity: 0.070346\n",
            "(42, 46): reprojection: 0.534929, disparity: 0.082084\n",
            "(43, 44): reprojection: 0.189776, disparity: 0.056901\n",
            "(43, 45): reprojection: 0.282945, disparity: 0.067604\n",
            "(44, 45): reprojection: 0.141274, disparity: 0.056458\n",
            "(44, 46): reprojection: 0.287286, disparity: 0.067214\n",
            "(44, 48): reprojection: 0.433659, disparity: 0.083279\n",
            "(44, 52): reprojection: 0.627633, disparity: 0.107033\n",
            "(45, 46): reprojection: 0.201500, disparity: 0.058986\n",
            "(45, 47): reprojection: 0.326250, disparity: 0.077432\n",
            "(46, 47): reprojection: 0.182425, disparity: 0.060759\n",
            "(46, 48): reprojection: 0.295871, disparity: 0.075990\n",
            "(46, 50): reprojection: 0.410334, disparity: 0.083221\n",
            "(47, 48): reprojection: 0.200773, disparity: 0.057564\n",
            "(47, 49): reprojection: 0.255022, disparity: 0.066040\n",
            "(48, 49): reprojection: 0.159556, disparity: 0.062783\n",
            "(48, 50): reprojection: 0.231193, disparity: 0.068875\n",
            "(48, 52): reprojection: 0.368629, disparity: 0.084722\n",
            "(48, 56): reprojection: 0.681443, disparity: 0.106372\n",
            "(48, 64): reprojection: 0.938288, disparity: 0.208883\n",
            "(48, 80): reprojection: 1.376741, disparity: 0.131276\n",
            "(49, 50): reprojection: 0.138210, disparity: 0.060666\n",
            "(49, 51): reprojection: 0.242463, disparity: 0.082789\n",
            "(50, 51): reprojection: 0.155754, disparity: 0.069286\n",
            "(50, 52): reprojection: 0.257136, disparity: 0.079083\n",
            "(50, 54): reprojection: 0.381589, disparity: 0.102126\n",
            "(51, 52): reprojection: 0.156972, disparity: 0.059679\n",
            "(51, 53): reprojection: 0.290029, disparity: 0.066720\n",
            "(52, 53): reprojection: 0.170951, disparity: 0.065063\n",
            "(52, 54): reprojection: 0.239386, disparity: 0.072442\n",
            "(52, 56): reprojection: 0.559316, disparity: 0.076257\n",
            "(52, 60): reprojection: 0.648765, disparity: 0.137477\n",
            "(53, 54): reprojection: 0.118266, disparity: 0.058714\n",
            "(53, 55): reprojection: 0.318667, disparity: 0.071449\n",
            "(54, 55): reprojection: 0.302772, disparity: 0.059700\n",
            "(54, 56): reprojection: 0.570265, disparity: 0.065761\n",
            "(54, 58): reprojection: 0.392242, disparity: 0.081898\n",
            "(55, 56): reprojection: 0.343554, disparity: 0.063981\n",
            "(55, 57): reprojection: 0.387194, disparity: 0.068804\n",
            "(56, 57): reprojection: 0.297219, disparity: 0.061974\n",
            "(56, 58): reprojection: 0.539138, disparity: 0.083315\n",
            "(56, 60): reprojection: 0.681258, disparity: 0.121972\n",
            "(56, 64): reprojection: 0.832409, disparity: 0.123221\n",
            "(56, 72): reprojection: 1.111578, disparity: 0.182532\n",
            "(57, 58): reprojection: 0.338373, disparity: 0.078263\n",
            "(57, 59): reprojection: 0.457865, disparity: 0.091645\n",
            "(58, 59): reprojection: 0.242134, disparity: 0.066851\n",
            "(58, 60): reprojection: 0.354129, disparity: 0.077905\n",
            "(58, 62): reprojection: 0.492950, disparity: 0.098962\n",
            "(59, 60): reprojection: 0.181259, disparity: 0.073216\n",
            "(59, 61): reprojection: 0.298388, disparity: 0.076544\n",
            "(60, 61): reprojection: 0.224261, disparity: 0.069495\n",
            "(60, 62): reprojection: 0.309845, disparity: 0.077136\n",
            "(60, 64): reprojection: 0.507485, disparity: 0.082036\n",
            "(60, 68): reprojection: 0.683295, disparity: 0.118414\n",
            "(61, 62): reprojection: 0.209390, disparity: 0.071151\n",
            "(61, 63): reprojection: 0.280199, disparity: 0.076571\n",
            "(62, 63): reprojection: 0.217708, disparity: 0.067477\n",
            "(62, 64): reprojection: 0.324479, disparity: 0.074634\n",
            "(62, 66): reprojection: 0.489660, disparity: 0.104673\n",
            "(63, 64): reprojection: 0.207309, disparity: 0.065014\n",
            "(63, 65): reprojection: 0.319914, disparity: 0.073540\n",
            "(64, 65): reprojection: 0.213561, disparity: 0.073735\n",
            "(64, 66): reprojection: 0.357783, disparity: 0.076949\n",
            "(64, 68): reprojection: 0.437459, disparity: 0.105280\n",
            "(64, 72): reprojection: 0.693408, disparity: 0.125451\n",
            "(64, 80): reprojection: 1.059969, disparity: 0.121270\n",
            "(65, 66): reprojection: 0.263680, disparity: 0.064610\n",
            "(65, 67): reprojection: 0.413830, disparity: 0.075005\n",
            "(66, 67): reprojection: 0.254512, disparity: 0.071105\n",
            "(66, 68): reprojection: 0.357725, disparity: 0.080511\n",
            "(66, 70): reprojection: 0.475994, disparity: 0.098177\n",
            "(67, 68): reprojection: 0.328623, disparity: 0.068789\n",
            "(67, 69): reprojection: 0.346662, disparity: 0.078430\n",
            "(68, 69): reprojection: 0.206881, disparity: 0.064804\n",
            "(68, 70): reprojection: 0.448992, disparity: 0.076736\n",
            "(68, 72): reprojection: 0.539892, disparity: 0.095633\n",
            "(68, 76): reprojection: 0.694234, disparity: 0.105119\n",
            "(69, 70): reprojection: 0.310079, disparity: 0.067233\n",
            "(69, 71): reprojection: 0.463234, disparity: 0.076572\n",
            "(70, 71): reprojection: 0.203701, disparity: 0.065989\n",
            "(70, 72): reprojection: 0.282735, disparity: 0.076533\n",
            "(70, 74): reprojection: 0.403824, disparity: 0.085456\n",
            "(71, 72): reprojection: 0.233956, disparity: 0.066369\n",
            "(71, 73): reprojection: 0.346307, disparity: 0.077552\n",
            "(72, 73): reprojection: 0.214019, disparity: 0.065687\n",
            "(72, 74): reprojection: 0.279245, disparity: 0.071270\n",
            "(72, 76): reprojection: 0.419485, disparity: 0.081111\n",
            "(72, 80): reprojection: 0.613065, disparity: 0.094273\n",
            "(72, 88): reprojection: 0.906489, disparity: 0.109451\n",
            "(73, 74): reprojection: 0.307453, disparity: 0.063844\n",
            "(73, 75): reprojection: 0.438318, disparity: 0.075689\n",
            "(74, 75): reprojection: 0.239900, disparity: 0.064584\n",
            "(74, 76): reprojection: 0.265249, disparity: 0.068821\n",
            "(74, 78): reprojection: 0.391137, disparity: 0.082991\n",
            "(75, 76): reprojection: 0.229767, disparity: 0.061586\n",
            "(75, 77): reprojection: 0.353431, disparity: 0.071745\n",
            "(76, 77): reprojection: 0.217826, disparity: 0.059453\n",
            "(76, 78): reprojection: 0.321472, disparity: 0.064972\n",
            "(76, 80): reprojection: 0.400030, disparity: 0.079454\n",
            "(76, 84): reprojection: 0.608956, disparity: 0.090404\n",
            "(77, 78): reprojection: 0.213486, disparity: 0.058797\n",
            "(77, 79): reprojection: 0.261633, disparity: 0.070012\n",
            "(78, 79): reprojection: 0.200887, disparity: 0.062624\n",
            "(78, 80): reprojection: 0.251493, disparity: 0.069096\n",
            "(78, 82): reprojection: 0.408849, disparity: 0.078898\n",
            "(79, 80): reprojection: 0.139444, disparity: 0.059240\n",
            "(79, 81): reprojection: 0.233710, disparity: 0.065154\n",
            "(80, 81): reprojection: 0.138877, disparity: 0.056124\n",
            "(80, 82): reprojection: 0.264228, disparity: 0.061332\n",
            "(80, 84): reprojection: 0.476219, disparity: 0.071544\n",
            "(80, 88): reprojection: 0.724467, disparity: 0.089738\n",
            "(81, 82): reprojection: 0.186332, disparity: 0.056806\n",
            "(81, 83): reprojection: 0.310562, disparity: 0.066097\n",
            "(82, 83): reprojection: 0.150093, disparity: 0.055829\n",
            "(82, 84): reprojection: 0.261234, disparity: 0.062839\n",
            "(82, 86): reprojection: 0.456178, disparity: 0.083492\n",
            "(83, 84): reprojection: 0.147930, disparity: 0.055255\n",
            "(83, 85): reprojection: 0.313283, disparity: 0.068247\n",
            "(84, 85): reprojection: 0.170703, disparity: 0.055703\n",
            "(84, 86): reprojection: 0.258963, disparity: 0.065590\n",
            "(84, 88): reprojection: 0.503313, disparity: 0.073411\n",
            "(85, 86): reprojection: 0.164518, disparity: 0.055995\n",
            "(85, 87): reprojection: 0.278578, disparity: 0.062566\n",
            "(86, 87): reprojection: 0.233441, disparity: 0.056825\n",
            "(86, 88): reprojection: 0.332071, disparity: 0.064709\n",
            "(86, 90): reprojection: 0.498856, disparity: 0.070292\n",
            "(87, 88): reprojection: 0.180165, disparity: 0.056711\n",
            "(87, 89): reprojection: 0.284705, disparity: 0.064695\n",
            "(88, 89): reprojection: 0.142579, disparity: 0.054194\n",
            "(88, 90): reprojection: 0.311594, disparity: 0.058810\n",
            "(89, 90): reprojection: 0.213985, disparity: 0.050724\n",
            "(89, 91): reprojection: 0.416781, disparity: 0.069682\n",
            "(90, 91): reprojection: 0.333447, disparity: 0.061925\n",
            "Mean:     reprojection: 0.333447, disparity: 0.061925\n",
            "Done Validation for epoch 8 (2080 iterations)\n",
            "Epoch = 8, pairs = [[32, 64], [19, 21], [24, 26], [0, 1]], loss = 0.5884254574775696\n",
            "Epoch = 8, pairs = [[60, 62], [62, 64], [38, 42], [37, 38]], loss = 0.44844263792037964\n",
            "Epoch = 8, pairs = [[5, 6], [86, 88], [71, 72], [87, 88]], loss = 0.279116153717041\n",
            "Epoch = 8, pairs = [[29, 30], [48, 64], [53, 54], [22, 26]], loss = 0.4664302170276642\n",
            "Epoch = 8, pairs = [[26, 27], [18, 20], [24, 32], [28, 36]], loss = 0.541594922542572\n",
            "Epoch = 8, pairs = [[48, 56], [30, 34], [51, 53], [70, 72]], loss = 0.4933329224586487\n",
            "Epoch = 8, pairs = [[63, 65], [3, 5], [0, 8], [76, 80]], loss = 0.41319870948791504\n",
            "Epoch = 8, pairs = [[1, 3], [43, 45], [8, 12], [39, 40]], loss = 0.4065370261669159\n",
            "Epoch = 8, pairs = [[54, 56], [8, 10], [20, 22], [59, 60]], loss = 0.4223693907260895\n",
            "Epoch = 8, pairs = [[23, 24], [80, 84], [28, 32], [53, 55]], loss = 0.3898712992668152\n",
            "Epoch = 8, pairs = [[26, 30], [79, 80], [20, 28], [61, 63]], loss = 0.470227986574173\n",
            "Epoch = 8, pairs = [[50, 51], [90, 91], [4, 5], [64, 66]], loss = 0.31762129068374634\n",
            "Epoch = 8, pairs = [[9, 10], [27, 28], [58, 62], [74, 78]], loss = 0.37603041529655457\n",
            "Epoch = 8, pairs = [[29, 31], [47, 49], [38, 39], [4, 8]], loss = 0.33850473165512085\n",
            "Epoch = 8, pairs = [[41, 42], [80, 88], [42, 43], [72, 74]], loss = 0.4257866144180298\n",
            "Epoch = 8, pairs = [[33, 34], [9, 11], [36, 37], [66, 67]], loss = 0.32696473598480225\n",
            "Epoch = 8, pairs = [[6, 8], [32, 33], [57, 59], [30, 32]], loss = 0.38109317421913147\n",
            "Epoch = 8, pairs = [[6, 10], [87, 89], [16, 48], [48, 49]], loss = 0.7719670534133911\n",
            "Epoch = 8, pairs = [[20, 24], [85, 87], [26, 28], [34, 35]], loss = 0.3136739730834961\n",
            "Epoch = 8, pairs = [[69, 71], [7, 9], [24, 25], [44, 48]], loss = 0.35062724351882935\n",
            "Epoch = 8, pairs = [[48, 80], [66, 70], [50, 54], [68, 70]], loss = 1.216955304145813\n",
            "Epoch = 8, pairs = [[63, 64], [21, 23], [73, 75], [35, 36]], loss = 0.32519352436065674\n",
            "Epoch = 8, pairs = [[68, 72], [4, 6], [67, 69], [15, 17]], loss = 0.41896793246269226\n",
            "Epoch = 8, pairs = [[8, 9], [16, 17], [46, 50], [78, 79]], loss = 0.30987513065338135\n",
            "Epoch = 8, pairs = [[75, 76], [75, 77], [42, 46], [35, 37]], loss = 0.4827839136123657\n",
            "Epoch = 8, pairs = [[84, 88], [77, 79], [81, 82], [40, 42]], loss = 0.4259006977081299\n",
            "Epoch = 8, pairs = [[8, 24], [83, 84], [88, 90], [22, 23]], loss = 0.533637523651123\n",
            "Epoch = 8, pairs = [[32, 34], [45, 47], [4, 12], [61, 62]], loss = 0.3996694087982178\n",
            "Epoch = 8, pairs = [[31, 33], [17, 19], [89, 91], [30, 31]], loss = 0.4228341579437256\n",
            "Epoch = 8, pairs = [[28, 29], [74, 76], [25, 26], [45, 46]], loss = 0.2663816809654236\n",
            "Epoch = 8, pairs = [[70, 71], [0, 32], [32, 40], [40, 48]], loss = 0.9093508720397949\n",
            "Epoch = 8, pairs = [[49, 50], [11, 12], [57, 58], [62, 63]], loss = 0.2931137681007385\n",
            "Epoch = 8, pairs = [[0, 16], [62, 66], [39, 41], [79, 81]], loss = 0.7385410070419312\n",
            "Epoch = 8, pairs = [[36, 38], [37, 39], [64, 72], [68, 69]], loss = 0.680535614490509\n",
            "Epoch = 8, pairs = [[69, 70], [0, 2], [82, 84], [64, 65]], loss = 0.37708231806755066\n",
            "Epoch = 8, pairs = [[56, 57], [54, 58], [10, 11], [44, 45]], loss = 0.3265811800956726\n",
            "Epoch = 8, pairs = [[56, 72], [14, 16], [5, 7], [34, 38]], loss = 0.6665477752685547\n",
            "Epoch = 8, pairs = [[64, 68], [51, 52], [14, 18], [40, 56]], loss = 0.6472309827804565\n",
            "Epoch = 8, pairs = [[22, 24], [42, 44], [56, 60], [60, 68]], loss = 0.546487033367157\n",
            "Epoch = 8, pairs = [[76, 84], [16, 24], [81, 83], [52, 53]], loss = 0.5809727907180786\n",
            "Epoch = 8, pairs = [[24, 28], [2, 4], [46, 48], [47, 48]], loss = 0.36663246154785156\n",
            "Epoch = 8, pairs = [[12, 16], [12, 13], [89, 90], [86, 90]], loss = 0.39986395835876465\n",
            "Epoch = 8, pairs = [[82, 83], [15, 16], [60, 64], [58, 59]], loss = 0.3872683048248291\n",
            "Epoch = 8, pairs = [[32, 48], [74, 75], [48, 50], [17, 18]], loss = 0.6489464044570923\n",
            "Epoch = 8, pairs = [[24, 40], [49, 51], [40, 41], [33, 35]], loss = 0.7117571830749512\n",
            "Epoch = 8, pairs = [[36, 44], [60, 61], [80, 82], [78, 80]], loss = 0.47328877449035645\n",
            "Epoch = 8, pairs = [[73, 74], [46, 47], [13, 14], [3, 4]], loss = 0.2604480981826782\n",
            "Epoch = 8, pairs = [[84, 85], [16, 32], [71, 73], [52, 54]], loss = 0.6469224095344543\n",
            "Epoch = 8, pairs = [[40, 44], [8, 16], [10, 14], [85, 86]], loss = 0.5537214279174805\n",
            "Epoch = 8, pairs = [[38, 40], [59, 61], [41, 43], [76, 78]], loss = 0.4243084788322449\n",
            "Epoch = 8, pairs = [[88, 89], [54, 55], [21, 22], [44, 46]], loss = 0.295062392950058\n",
            "Epoch = 8, pairs = [[16, 20], [20, 21], [2, 3], [55, 56]], loss = 0.37940919399261475\n",
            "Epoch = 8, pairs = [[64, 80], [84, 86], [16, 18], [12, 20]], loss = 0.753958523273468\n",
            "Epoch = 8, pairs = [[76, 77], [65, 67], [52, 60], [31, 32]], loss = 0.46574386954307556\n",
            "Epoch = 8, pairs = [[55, 57], [72, 76], [50, 52], [13, 15]], loss = 0.42446258664131165\n",
            "Epoch = 8, pairs = [[10, 12], [36, 40], [18, 22], [83, 85]], loss = 0.42229557037353516\n",
            "Epoch = 8, pairs = [[28, 30], [11, 13], [67, 68], [44, 52]], loss = 0.4221840500831604\n",
            "Epoch = 8, pairs = [[48, 52], [2, 6], [12, 14], [19, 20]], loss = 0.32543110847473145\n",
            "Epoch = 8, pairs = [[1, 2], [72, 88], [70, 74], [14, 15]], loss = 0.528627872467041\n",
            "Epoch = 8, pairs = [[82, 86], [27, 29], [34, 36], [43, 44]], loss = 0.3582293391227722\n",
            "Epoch = 8, pairs = [[72, 80], [65, 66], [66, 68], [52, 56]], loss = 0.5432549715042114\n",
            "Epoch = 8, pairs = [[72, 73], [23, 25], [56, 64], [56, 58]], loss = 0.5340226292610168\n",
            "Epoch = 8, pairs = [[58, 60], [68, 76], [25, 27], [77, 78]], loss = 0.44334715604782104\n",
            "Epoch = 8, pairs = [[18, 19], [7, 8], [6, 7], [32, 36]], loss = 0.24819082021713257\n",
            "Epoch = 8, pairs = [[0, 4], [78, 82], [80, 81], [86, 87]], loss = 0.35929977893829346\n",
            "Epoch 8 took 84.97s.\n",
            "( 0,  1): reprojection: 0.235261, disparity: 0.058195\n",
            "( 0,  2): reprojection: 0.348702, disparity: 0.064183\n",
            "( 0,  4): reprojection: 0.384389, disparity: 0.080715\n",
            "( 0,  8): reprojection: 0.435211, disparity: 0.100118\n",
            "( 0, 16): reprojection: 0.777882, disparity: 0.122219\n",
            "( 0, 32): reprojection: 1.142936, disparity: 0.173662\n",
            "( 1,  2): reprojection: 0.193074, disparity: 0.045617\n",
            "( 1,  3): reprojection: 0.244450, disparity: 0.052247\n",
            "( 2,  3): reprojection: 0.109570, disparity: 0.046523\n",
            "( 2,  4): reprojection: 0.168313, disparity: 0.056039\n",
            "( 2,  6): reprojection: 0.334354, disparity: 0.060809\n",
            "( 3,  4): reprojection: 0.107225, disparity: 0.047129\n",
            "( 3,  5): reprojection: 0.191054, disparity: 0.053601\n",
            "( 4,  5): reprojection: 0.152881, disparity: 0.048298\n",
            "( 4,  6): reprojection: 0.212357, disparity: 0.051284\n",
            "( 4,  8): reprojection: 0.272777, disparity: 0.060262\n",
            "( 4, 12): reprojection: 0.385789, disparity: 0.072464\n",
            "( 5,  6): reprojection: 0.117379, disparity: 0.044854\n",
            "( 5,  7): reprojection: 0.151916, disparity: 0.049654\n",
            "( 6,  7): reprojection: 0.094386, disparity: 0.044793\n",
            "( 6,  8): reprojection: 0.154822, disparity: 0.051147\n",
            "( 6, 10): reprojection: 0.387927, disparity: 0.061374\n",
            "( 7,  8): reprojection: 0.088419, disparity: 0.046472\n",
            "( 7,  9): reprojection: 0.192769, disparity: 0.052105\n",
            "( 8,  9): reprojection: 0.166802, disparity: 0.046953\n",
            "( 8, 10): reprojection: 0.365569, disparity: 0.053339\n",
            "( 8, 12): reprojection: 0.499517, disparity: 0.061047\n",
            "( 8, 16): reprojection: 0.680351, disparity: 0.080656\n",
            "( 8, 24): reprojection: 0.875385, disparity: 0.118563\n",
            "( 9, 10): reprojection: 0.254088, disparity: 0.048528\n",
            "( 9, 11): reprojection: 0.382239, disparity: 0.054517\n",
            "(10, 11): reprojection: 0.170608, disparity: 0.044729\n",
            "(10, 12): reprojection: 0.209505, disparity: 0.051166\n",
            "(10, 14): reprojection: 0.357218, disparity: 0.072296\n",
            "(11, 12): reprojection: 0.134806, disparity: 0.045069\n",
            "(11, 13): reprojection: 0.222222, disparity: 0.057956\n",
            "(12, 13): reprojection: 0.145099, disparity: 0.046369\n",
            "(12, 14): reprojection: 0.280610, disparity: 0.056569\n",
            "(12, 16): reprojection: 0.450092, disparity: 0.082826\n",
            "(12, 20): reprojection: 0.717949, disparity: 0.092975\n",
            "(13, 14): reprojection: 0.185009, disparity: 0.048090\n",
            "(13, 15): reprojection: 0.242286, disparity: 0.057369\n",
            "(14, 15): reprojection: 0.190860, disparity: 0.047935\n",
            "(14, 16): reprojection: 0.433594, disparity: 0.058916\n",
            "(14, 18): reprojection: 0.363946, disparity: 0.070953\n",
            "(15, 16): reprojection: 0.299905, disparity: 0.049400\n",
            "(15, 17): reprojection: 0.384911, disparity: 0.059732\n",
            "(16, 17): reprojection: 0.141586, disparity: 0.047472\n",
            "(16, 18): reprojection: 0.326021, disparity: 0.060138\n",
            "(16, 20): reprojection: 0.540341, disparity: 0.074649\n",
            "(16, 24): reprojection: 0.825776, disparity: 0.096934\n",
            "(16, 32): reprojection: 1.322412, disparity: 0.138035\n",
            "(16, 48): reprojection: 1.448168, disparity: 0.164444\n",
            "(17, 18): reprojection: 0.322372, disparity: 0.051982\n",
            "(17, 19): reprojection: 0.486903, disparity: 0.063858\n",
            "(18, 19): reprojection: 0.209550, disparity: 0.049286\n",
            "(18, 20): reprojection: 0.281472, disparity: 0.058740\n",
            "(18, 22): reprojection: 0.502989, disparity: 0.073295\n",
            "(19, 20): reprojection: 0.108818, disparity: 0.048312\n",
            "(19, 21): reprojection: 0.250920, disparity: 0.059396\n",
            "(20, 21): reprojection: 0.198918, disparity: 0.050643\n",
            "(20, 22): reprojection: 0.304283, disparity: 0.063083\n",
            "(20, 24): reprojection: 0.386166, disparity: 0.072604\n",
            "(20, 28): reprojection: 0.631132, disparity: 0.108545\n",
            "(21, 22): reprojection: 0.165097, disparity: 0.051196\n",
            "(21, 23): reprojection: 0.253970, disparity: 0.060529\n",
            "(22, 23): reprojection: 0.129219, disparity: 0.050380\n",
            "(22, 24): reprojection: 0.161126, disparity: 0.059180\n",
            "(22, 26): reprojection: 0.335865, disparity: 0.076093\n",
            "(23, 24): reprojection: 0.098550, disparity: 0.049818\n",
            "(23, 25): reprojection: 0.183517, disparity: 0.059589\n",
            "(24, 25): reprojection: 0.141667, disparity: 0.052418\n",
            "(24, 26): reprojection: 0.189995, disparity: 0.062951\n",
            "(24, 28): reprojection: 0.318044, disparity: 0.082797\n",
            "(24, 32): reprojection: 0.606095, disparity: 0.110725\n",
            "(24, 40): reprojection: 0.906339, disparity: 0.181064\n",
            "(25, 26): reprojection: 0.120952, disparity: 0.055269\n",
            "(25, 27): reprojection: 0.222703, disparity: 0.065831\n",
            "(26, 27): reprojection: 0.149859, disparity: 0.056086\n",
            "(26, 28): reprojection: 0.188046, disparity: 0.066148\n",
            "(26, 30): reprojection: 0.339999, disparity: 0.089185\n",
            "(27, 28): reprojection: 0.141857, disparity: 0.053731\n",
            "(27, 29): reprojection: 0.259089, disparity: 0.065485\n",
            "(28, 29): reprojection: 0.185584, disparity: 0.056200\n",
            "(28, 30): reprojection: 0.247347, disparity: 0.068056\n",
            "(28, 32): reprojection: 0.453724, disparity: 0.083043\n",
            "(28, 36): reprojection: 0.667906, disparity: 0.113710\n",
            "(29, 30): reprojection: 0.138170, disparity: 0.053153\n",
            "(29, 31): reprojection: 0.273113, disparity: 0.060679\n",
            "(30, 31): reprojection: 0.188480, disparity: 0.052274\n",
            "(30, 32): reprojection: 0.265521, disparity: 0.064377\n",
            "(30, 34): reprojection: 0.495997, disparity: 0.085045\n",
            "(31, 32): reprojection: 0.136146, disparity: 0.054983\n",
            "(31, 33): reprojection: 0.323748, disparity: 0.064964\n",
            "(32, 33): reprojection: 0.255503, disparity: 0.054780\n",
            "(32, 34): reprojection: 0.264908, disparity: 0.064375\n",
            "(32, 36): reprojection: 0.366163, disparity: 0.083295\n",
            "(32, 40): reprojection: 0.481581, disparity: 0.128645\n",
            "(32, 48): reprojection: 1.014683, disparity: 0.173609\n",
            "(32, 64): reprojection: 1.205573, disparity: 0.224914\n",
            "(33, 34): reprojection: 0.156879, disparity: 0.053492\n",
            "(33, 35): reprojection: 0.262190, disparity: 0.064032\n",
            "(34, 35): reprojection: 0.189358, disparity: 0.055202\n",
            "(34, 36): reprojection: 0.286029, disparity: 0.065319\n",
            "(34, 38): reprojection: 0.357513, disparity: 0.092630\n",
            "(35, 36): reprojection: 0.236155, disparity: 0.057772\n",
            "(35, 37): reprojection: 0.258546, disparity: 0.072679\n",
            "(36, 37): reprojection: 0.253212, disparity: 0.061726\n",
            "(36, 38): reprojection: 0.365451, disparity: 0.074622\n",
            "(36, 40): reprojection: 0.318564, disparity: 0.090934\n",
            "(36, 44): reprojection: 0.674999, disparity: 0.125183\n",
            "(37, 38): reprojection: 0.223655, disparity: 0.058779\n",
            "(37, 39): reprojection: 0.241769, disparity: 0.068996\n",
            "(38, 39): reprojection: 0.201351, disparity: 0.058518\n",
            "(38, 40): reprojection: 0.383962, disparity: 0.074109\n",
            "(38, 42): reprojection: 0.709960, disparity: 0.101019\n",
            "(39, 40): reprojection: 0.286516, disparity: 0.062456\n",
            "(39, 41): reprojection: 0.508064, disparity: 0.076859\n",
            "(40, 41): reprojection: 0.302468, disparity: 0.061755\n",
            "(40, 42): reprojection: 0.434263, disparity: 0.079835\n",
            "(40, 44): reprojection: 0.573756, disparity: 0.101324\n",
            "(40, 48): reprojection: 0.773939, disparity: 0.135031\n",
            "(40, 56): reprojection: 0.749236, disparity: 0.229439\n",
            "(41, 42): reprojection: 0.196703, disparity: 0.061215\n",
            "(41, 43): reprojection: 0.301244, disparity: 0.071252\n",
            "(42, 43): reprojection: 0.178050, disparity: 0.060634\n",
            "(42, 44): reprojection: 0.325275, disparity: 0.074556\n",
            "(42, 46): reprojection: 0.521939, disparity: 0.092745\n",
            "(43, 44): reprojection: 0.203446, disparity: 0.060899\n",
            "(43, 45): reprojection: 0.299492, disparity: 0.071671\n",
            "(44, 45): reprojection: 0.148968, disparity: 0.062887\n",
            "(44, 46): reprojection: 0.280548, disparity: 0.082876\n",
            "(44, 48): reprojection: 0.407469, disparity: 0.097316\n",
            "(44, 52): reprojection: 0.566288, disparity: 0.114848\n",
            "(45, 46): reprojection: 0.201404, disparity: 0.065701\n",
            "(45, 47): reprojection: 0.334811, disparity: 0.082648\n",
            "(46, 47): reprojection: 0.197122, disparity: 0.062455\n",
            "(46, 48): reprojection: 0.291308, disparity: 0.078461\n",
            "(46, 50): reprojection: 0.394071, disparity: 0.094505\n",
            "(47, 48): reprojection: 0.197799, disparity: 0.059621\n",
            "(47, 49): reprojection: 0.254324, disparity: 0.076185\n",
            "(48, 49): reprojection: 0.172606, disparity: 0.070356\n",
            "(48, 50): reprojection: 0.236850, disparity: 0.077823\n",
            "(48, 52): reprojection: 0.327873, disparity: 0.089640\n",
            "(48, 56): reprojection: 0.651673, disparity: 0.120176\n",
            "(48, 64): reprojection: 0.899430, disparity: 0.227460\n",
            "(48, 80): reprojection: 1.207403, disparity: 0.124392\n",
            "(49, 50): reprojection: 0.146667, disparity: 0.062364\n",
            "(49, 51): reprojection: 0.229500, disparity: 0.074629\n",
            "(50, 51): reprojection: 0.142229, disparity: 0.063388\n",
            "(50, 52): reprojection: 0.235373, disparity: 0.072416\n",
            "(50, 54): reprojection: 0.366993, disparity: 0.106843\n",
            "(51, 52): reprojection: 0.162005, disparity: 0.061642\n",
            "(51, 53): reprojection: 0.277098, disparity: 0.075646\n",
            "(52, 53): reprojection: 0.166757, disparity: 0.069758\n",
            "(52, 54): reprojection: 0.236616, disparity: 0.087975\n",
            "(52, 56): reprojection: 0.524045, disparity: 0.095465\n",
            "(52, 60): reprojection: 0.548991, disparity: 0.147479\n",
            "(53, 54): reprojection: 0.135559, disparity: 0.068453\n",
            "(53, 55): reprojection: 0.335034, disparity: 0.082961\n",
            "(54, 55): reprojection: 0.302312, disparity: 0.062062\n",
            "(54, 56): reprojection: 0.565859, disparity: 0.068462\n",
            "(54, 58): reprojection: 0.342569, disparity: 0.081783\n",
            "(55, 56): reprojection: 0.347663, disparity: 0.066384\n",
            "(55, 57): reprojection: 0.373291, disparity: 0.074437\n",
            "(56, 57): reprojection: 0.301968, disparity: 0.065758\n",
            "(56, 58): reprojection: 0.532895, disparity: 0.081151\n",
            "(56, 60): reprojection: 0.646318, disparity: 0.109843\n",
            "(56, 64): reprojection: 0.812318, disparity: 0.133638\n",
            "(56, 72): reprojection: 1.024672, disparity: 0.133427\n",
            "(57, 58): reprojection: 0.345730, disparity: 0.072613\n",
            "(57, 59): reprojection: 0.460888, disparity: 0.081357\n",
            "(58, 59): reprojection: 0.233540, disparity: 0.069221\n",
            "(58, 60): reprojection: 0.354516, disparity: 0.084303\n",
            "(58, 62): reprojection: 0.426726, disparity: 0.102899\n",
            "(59, 60): reprojection: 0.181856, disparity: 0.074310\n",
            "(59, 61): reprojection: 0.275361, disparity: 0.083404\n",
            "(60, 61): reprojection: 0.210988, disparity: 0.075709\n",
            "(60, 62): reprojection: 0.273856, disparity: 0.084570\n",
            "(60, 64): reprojection: 0.453193, disparity: 0.093490\n",
            "(60, 68): reprojection: 0.655934, disparity: 0.116002\n",
            "(61, 62): reprojection: 0.196337, disparity: 0.073962\n",
            "(61, 63): reprojection: 0.287430, disparity: 0.080366\n",
            "(62, 63): reprojection: 0.225813, disparity: 0.071131\n",
            "(62, 64): reprojection: 0.346071, disparity: 0.076379\n",
            "(62, 66): reprojection: 0.581846, disparity: 0.108205\n",
            "(63, 64): reprojection: 0.222739, disparity: 0.068413\n",
            "(63, 65): reprojection: 0.352417, disparity: 0.074568\n",
            "(64, 65): reprojection: 0.239043, disparity: 0.074981\n",
            "(64, 66): reprojection: 0.389256, disparity: 0.078026\n",
            "(64, 68): reprojection: 0.496054, disparity: 0.098993\n",
            "(64, 72): reprojection: 0.790886, disparity: 0.111523\n",
            "(64, 80): reprojection: 1.145357, disparity: 0.116357\n",
            "(65, 66): reprojection: 0.273503, disparity: 0.067060\n",
            "(65, 67): reprojection: 0.426678, disparity: 0.077170\n",
            "(66, 67): reprojection: 0.262566, disparity: 0.070910\n",
            "(66, 68): reprojection: 0.366806, disparity: 0.074189\n",
            "(66, 70): reprojection: 0.463504, disparity: 0.089575\n",
            "(67, 68): reprojection: 0.334813, disparity: 0.066474\n",
            "(67, 69): reprojection: 0.334170, disparity: 0.070937\n",
            "(68, 69): reprojection: 0.203932, disparity: 0.065326\n",
            "(68, 70): reprojection: 0.442086, disparity: 0.072090\n",
            "(68, 72): reprojection: 0.515066, disparity: 0.085063\n",
            "(68, 76): reprojection: 0.635099, disparity: 0.091022\n",
            "(69, 70): reprojection: 0.313535, disparity: 0.068338\n",
            "(69, 71): reprojection: 0.448403, disparity: 0.074635\n",
            "(70, 71): reprojection: 0.219238, disparity: 0.067194\n",
            "(70, 72): reprojection: 0.275512, disparity: 0.072444\n",
            "(70, 74): reprojection: 0.386952, disparity: 0.076000\n",
            "(71, 72): reprojection: 0.233192, disparity: 0.066149\n",
            "(71, 73): reprojection: 0.334825, disparity: 0.072889\n",
            "(72, 73): reprojection: 0.211135, disparity: 0.065184\n",
            "(72, 74): reprojection: 0.286576, disparity: 0.069850\n",
            "(72, 76): reprojection: 0.396487, disparity: 0.077917\n",
            "(72, 80): reprojection: 0.602313, disparity: 0.095194\n",
            "(72, 88): reprojection: 0.801199, disparity: 0.105302\n",
            "(73, 74): reprojection: 0.315831, disparity: 0.064196\n",
            "(73, 75): reprojection: 0.438536, disparity: 0.072568\n",
            "(74, 75): reprojection: 0.238383, disparity: 0.065370\n",
            "(74, 76): reprojection: 0.253827, disparity: 0.069623\n",
            "(74, 78): reprojection: 0.378315, disparity: 0.078285\n",
            "(75, 76): reprojection: 0.232141, disparity: 0.062793\n",
            "(75, 77): reprojection: 0.353765, disparity: 0.071047\n",
            "(76, 77): reprojection: 0.223837, disparity: 0.063237\n",
            "(76, 78): reprojection: 0.332480, disparity: 0.065845\n",
            "(76, 80): reprojection: 0.399922, disparity: 0.076408\n",
            "(76, 84): reprojection: 0.543522, disparity: 0.091203\n",
            "(77, 78): reprojection: 0.226293, disparity: 0.061317\n",
            "(77, 79): reprojection: 0.252636, disparity: 0.068913\n",
            "(78, 79): reprojection: 0.191873, disparity: 0.064201\n",
            "(78, 80): reprojection: 0.238240, disparity: 0.069001\n",
            "(78, 82): reprojection: 0.363999, disparity: 0.078234\n",
            "(79, 80): reprojection: 0.143775, disparity: 0.061703\n",
            "(79, 81): reprojection: 0.232544, disparity: 0.065873\n",
            "(80, 81): reprojection: 0.141607, disparity: 0.059038\n",
            "(80, 82): reprojection: 0.257238, disparity: 0.062178\n",
            "(80, 84): reprojection: 0.420000, disparity: 0.072960\n",
            "(80, 88): reprojection: 0.689829, disparity: 0.093764\n",
            "(81, 82): reprojection: 0.176827, disparity: 0.059412\n",
            "(81, 83): reprojection: 0.282414, disparity: 0.067276\n",
            "(82, 83): reprojection: 0.143216, disparity: 0.058485\n",
            "(82, 84): reprojection: 0.237684, disparity: 0.064680\n",
            "(82, 86): reprojection: 0.425880, disparity: 0.081649\n",
            "(83, 84): reprojection: 0.137078, disparity: 0.059315\n",
            "(83, 85): reprojection: 0.296664, disparity: 0.071836\n",
            "(84, 85): reprojection: 0.171446, disparity: 0.058173\n",
            "(84, 86): reprojection: 0.256710, disparity: 0.068290\n",
            "(84, 88): reprojection: 0.506802, disparity: 0.078088\n",
            "(85, 86): reprojection: 0.168026, disparity: 0.057127\n",
            "(85, 87): reprojection: 0.278593, disparity: 0.064370\n",
            "(86, 87): reprojection: 0.234267, disparity: 0.060141\n",
            "(86, 88): reprojection: 0.337964, disparity: 0.070850\n",
            "(86, 90): reprojection: 0.494978, disparity: 0.074879\n",
            "(87, 88): reprojection: 0.184871, disparity: 0.058859\n",
            "(87, 89): reprojection: 0.284300, disparity: 0.067591\n",
            "(88, 89): reprojection: 0.140429, disparity: 0.055728\n",
            "(88, 90): reprojection: 0.306643, disparity: 0.063521\n",
            "(89, 90): reprojection: 0.213559, disparity: 0.052957\n",
            "(89, 91): reprojection: 0.422895, disparity: 0.075523\n",
            "(90, 91): reprojection: 0.338900, disparity: 0.067283\n",
            "Mean:     reprojection: 0.338900, disparity: 0.067283\n",
            "Done Validation for epoch 9 (2340 iterations)\n",
            "Epoch = 9, pairs = [[18, 20], [49, 51], [39, 41], [72, 74]], loss = 0.3957327902317047\n",
            "Epoch = 9, pairs = [[45, 47], [52, 53], [7, 9], [43, 45]], loss = 0.31973880529403687\n",
            "Epoch = 9, pairs = [[48, 49], [31, 33], [36, 38], [23, 24]], loss = 0.3103196620941162\n",
            "Epoch = 9, pairs = [[27, 29], [55, 57], [65, 67], [19, 21]], loss = 0.4060492515563965\n",
            "Epoch = 9, pairs = [[24, 26], [40, 48], [14, 16], [4, 5]], loss = 0.4993453919887543\n",
            "Epoch = 9, pairs = [[65, 66], [48, 64], [24, 32], [32, 64]], loss = 1.0212457180023193\n",
            "Epoch = 9, pairs = [[38, 42], [51, 52], [67, 68], [0, 4]], loss = 0.44650980830192566\n",
            "Epoch = 9, pairs = [[56, 58], [79, 81], [4, 6], [44, 52]], loss = 0.5115315914154053\n",
            "Epoch = 9, pairs = [[46, 50], [67, 69], [1, 2], [64, 66]], loss = 0.4524553418159485\n",
            "Epoch = 9, pairs = [[70, 74], [22, 24], [62, 66], [86, 87]], loss = 0.4758246839046478\n",
            "Epoch = 9, pairs = [[14, 15], [36, 44], [34, 38], [20, 24]], loss = 0.6378748416900635\n",
            "Epoch = 9, pairs = [[86, 90], [87, 89], [40, 44], [4, 12]], loss = 0.5380910634994507\n",
            "Epoch = 9, pairs = [[68, 69], [0, 32], [87, 88], [46, 47]], loss = 0.7603219747543335\n",
            "Epoch = 9, pairs = [[60, 68], [2, 4], [28, 36], [16, 17]], loss = 0.5166730880737305\n",
            "Epoch = 9, pairs = [[1, 3], [50, 54], [30, 32], [82, 83]], loss = 0.3609301745891571\n",
            "Epoch = 9, pairs = [[76, 77], [86, 88], [0, 8], [9, 10]], loss = 0.3981301188468933\n",
            "Epoch = 9, pairs = [[62, 64], [72, 73], [58, 60], [78, 79]], loss = 0.41093236207962036\n",
            "Epoch = 9, pairs = [[22, 26], [15, 16], [51, 53], [36, 40]], loss = 0.45981350541114807\n",
            "Epoch = 9, pairs = [[19, 20], [32, 36], [52, 56], [83, 84]], loss = 0.38353824615478516\n",
            "Epoch = 9, pairs = [[30, 31], [73, 74], [64, 80], [50, 51]], loss = 0.6048768162727356\n",
            "Epoch = 9, pairs = [[30, 34], [68, 70], [15, 17], [10, 12]], loss = 0.46119239926338196\n",
            "Epoch = 9, pairs = [[72, 88], [68, 76], [41, 42], [14, 18]], loss = 0.6633882522583008\n",
            "Epoch = 9, pairs = [[60, 61], [16, 24], [60, 62], [33, 34]], loss = 0.48302459716796875\n",
            "Epoch = 9, pairs = [[60, 64], [32, 33], [29, 30], [3, 5]], loss = 0.33995670080184937\n",
            "Epoch = 9, pairs = [[74, 76], [35, 37], [76, 84], [37, 39]], loss = 0.43873363733291626\n",
            "Epoch = 9, pairs = [[26, 27], [5, 6], [11, 13], [75, 77]], loss = 0.2683112621307373\n",
            "Epoch = 9, pairs = [[47, 49], [11, 12], [56, 57], [40, 42]], loss = 0.3422463536262512\n",
            "Epoch = 9, pairs = [[8, 12], [84, 85], [0, 16], [9, 11]], loss = 0.5384447574615479\n",
            "Epoch = 9, pairs = [[69, 70], [53, 54], [4, 8], [28, 30]], loss = 0.31158432364463806\n",
            "Epoch = 9, pairs = [[41, 43], [57, 59], [12, 14], [13, 14]], loss = 0.37117820978164673\n",
            "Epoch = 9, pairs = [[54, 55], [0, 2], [53, 55], [32, 40]], loss = 0.46530482172966003\n",
            "Epoch = 9, pairs = [[59, 61], [47, 48], [56, 72], [76, 80]], loss = 0.6758282780647278\n",
            "Epoch = 9, pairs = [[56, 60], [71, 73], [34, 36], [6, 7]], loss = 0.42536213994026184\n",
            "Epoch = 9, pairs = [[70, 72], [12, 16], [39, 40], [46, 48]], loss = 0.3804973363876343\n",
            "Epoch = 9, pairs = [[45, 46], [66, 68], [21, 23], [21, 22]], loss = 0.29479914903640747\n",
            "Epoch = 9, pairs = [[80, 82], [80, 88], [6, 10], [64, 72]], loss = 0.7160559892654419\n",
            "Epoch = 9, pairs = [[34, 35], [48, 56], [89, 90], [78, 80]], loss = 0.38976728916168213\n",
            "Epoch = 9, pairs = [[58, 62], [72, 76], [26, 30], [18, 22]], loss = 0.5102957487106323\n",
            "Epoch = 9, pairs = [[69, 71], [28, 29], [54, 56], [78, 82]], loss = 0.4579888880252838\n",
            "Epoch = 9, pairs = [[6, 8], [66, 67], [8, 16], [13, 15]], loss = 0.43014758825302124\n",
            "Epoch = 9, pairs = [[40, 56], [16, 20], [71, 72], [8, 24]], loss = 0.8183172345161438\n",
            "Epoch = 9, pairs = [[66, 70], [20, 22], [5, 7], [18, 19]], loss = 0.34647494554519653\n",
            "Epoch = 9, pairs = [[0, 1], [89, 91], [8, 10], [2, 6]], loss = 0.4091559052467346\n",
            "Epoch = 9, pairs = [[64, 65], [16, 18], [25, 27], [20, 28]], loss = 0.37807953357696533\n",
            "Epoch = 9, pairs = [[85, 86], [74, 75], [77, 79], [12, 13]], loss = 0.27764174342155457\n",
            "Epoch = 9, pairs = [[49, 50], [77, 78], [63, 65], [80, 84]], loss = 0.373370498418808\n",
            "Epoch = 9, pairs = [[64, 68], [29, 31], [24, 25], [48, 50]], loss = 0.3351218104362488\n",
            "Epoch = 9, pairs = [[10, 11], [85, 87], [31, 32], [22, 23]], loss = 0.22546795010566711\n",
            "Epoch = 9, pairs = [[17, 18], [26, 28], [38, 40], [68, 72]], loss = 0.40768522024154663\n",
            "Epoch = 9, pairs = [[84, 86], [2, 3], [32, 34], [62, 63]], loss = 0.2598245143890381\n",
            "Epoch = 9, pairs = [[48, 52], [76, 78], [36, 37], [54, 58]], loss = 0.43645572662353516\n",
            "Epoch = 9, pairs = [[33, 35], [32, 48], [88, 89], [61, 62]], loss = 0.5074249505996704\n",
            "Epoch = 9, pairs = [[38, 39], [44, 45], [58, 59], [79, 80]], loss = 0.2433394193649292\n",
            "Epoch = 9, pairs = [[10, 14], [25, 26], [20, 21], [24, 28]], loss = 0.29042261838912964\n",
            "Epoch = 9, pairs = [[56, 64], [52, 54], [44, 48], [3, 4]], loss = 0.48586589097976685\n",
            "Epoch = 9, pairs = [[75, 76], [59, 60], [73, 75], [80, 81]], loss = 0.3140270709991455\n",
            "Epoch = 9, pairs = [[24, 40], [43, 44], [37, 38], [57, 58]], loss = 0.5136411190032959\n",
            "Epoch = 9, pairs = [[44, 46], [82, 86], [81, 83], [48, 80]], loss = 0.7382545471191406\n",
            "Epoch = 9, pairs = [[40, 41], [61, 63], [90, 91], [12, 20]], loss = 0.4590323567390442\n",
            "Epoch = 9, pairs = [[70, 71], [42, 43], [16, 48], [63, 64]], loss = 0.814409613609314\n",
            "Epoch = 9, pairs = [[83, 85], [88, 90], [7, 8], [84, 88]], loss = 0.36868569254875183\n",
            "Epoch = 9, pairs = [[42, 44], [28, 32], [52, 60], [8, 9]], loss = 0.447683185338974\n",
            "Epoch = 9, pairs = [[16, 32], [23, 25], [74, 78], [17, 19]], loss = 0.597619891166687\n",
            "Epoch = 9, pairs = [[50, 52], [72, 80], [55, 56], [27, 28]], loss = 0.38553759455680847\n",
            "Epoch = 9, pairs = [[35, 36], [42, 46], [82, 84], [81, 82]], loss = 0.36407554149627686\n",
            "Epoch 9 took 85.17s.\n",
            "( 0,  1): reprojection: 0.230820, disparity: 0.053962\n",
            "( 0,  2): reprojection: 0.341306, disparity: 0.051747\n",
            "( 0,  4): reprojection: 0.378568, disparity: 0.057599\n",
            "( 0,  8): reprojection: 0.471729, disparity: 0.075387\n",
            "( 0, 16): reprojection: 0.768314, disparity: 0.077103\n",
            "( 0, 32): reprojection: 1.061805, disparity: 0.111291\n",
            "( 1,  2): reprojection: 0.193547, disparity: 0.045123\n",
            "( 1,  3): reprojection: 0.247556, disparity: 0.049990\n",
            "( 2,  3): reprojection: 0.109653, disparity: 0.044050\n",
            "( 2,  4): reprojection: 0.179279, disparity: 0.049724\n",
            "( 2,  6): reprojection: 0.352701, disparity: 0.061532\n",
            "( 3,  4): reprojection: 0.106057, disparity: 0.044848\n",
            "( 3,  5): reprojection: 0.204385, disparity: 0.051283\n",
            "( 4,  5): reprojection: 0.156183, disparity: 0.047362\n",
            "( 4,  6): reprojection: 0.225909, disparity: 0.048281\n",
            "( 4,  8): reprojection: 0.285370, disparity: 0.056925\n",
            "( 4, 12): reprojection: 0.423841, disparity: 0.067560\n",
            "( 5,  6): reprojection: 0.116715, disparity: 0.044805\n",
            "( 5,  7): reprojection: 0.154064, disparity: 0.049013\n",
            "( 6,  7): reprojection: 0.092332, disparity: 0.044322\n",
            "( 6,  8): reprojection: 0.157356, disparity: 0.048241\n",
            "( 6, 10): reprojection: 0.386555, disparity: 0.057562\n",
            "( 7,  8): reprojection: 0.088139, disparity: 0.044183\n",
            "( 7,  9): reprojection: 0.195546, disparity: 0.050044\n",
            "( 8,  9): reprojection: 0.161943, disparity: 0.046432\n",
            "( 8, 10): reprojection: 0.366968, disparity: 0.052706\n",
            "( 8, 12): reprojection: 0.521565, disparity: 0.060211\n",
            "( 8, 16): reprojection: 0.663981, disparity: 0.074384\n",
            "( 8, 24): reprojection: 0.776346, disparity: 0.102737\n",
            "( 9, 10): reprojection: 0.256579, disparity: 0.047150\n",
            "( 9, 11): reprojection: 0.384719, disparity: 0.053147\n",
            "(10, 11): reprojection: 0.173213, disparity: 0.043453\n",
            "(10, 12): reprojection: 0.211186, disparity: 0.050465\n",
            "(10, 14): reprojection: 0.341732, disparity: 0.063161\n",
            "(11, 12): reprojection: 0.141824, disparity: 0.043927\n",
            "(11, 13): reprojection: 0.233003, disparity: 0.054132\n",
            "(12, 13): reprojection: 0.135135, disparity: 0.045402\n",
            "(12, 14): reprojection: 0.263566, disparity: 0.052276\n",
            "(12, 16): reprojection: 0.429965, disparity: 0.071317\n",
            "(12, 20): reprojection: 0.678756, disparity: 0.080426\n",
            "(13, 14): reprojection: 0.187679, disparity: 0.047848\n",
            "(13, 15): reprojection: 0.223998, disparity: 0.054553\n",
            "(14, 15): reprojection: 0.187641, disparity: 0.046773\n",
            "(14, 16): reprojection: 0.436932, disparity: 0.057314\n",
            "(14, 18): reprojection: 0.351432, disparity: 0.066697\n",
            "(15, 16): reprojection: 0.299734, disparity: 0.049002\n",
            "(15, 17): reprojection: 0.383001, disparity: 0.058722\n",
            "(16, 17): reprojection: 0.138249, disparity: 0.047025\n",
            "(16, 18): reprojection: 0.315747, disparity: 0.057795\n",
            "(16, 20): reprojection: 0.576357, disparity: 0.070077\n",
            "(16, 24): reprojection: 0.797470, disparity: 0.087310\n",
            "(16, 32): reprojection: 1.104031, disparity: 0.119903\n",
            "(16, 48): reprojection: 1.421757, disparity: 0.350950\n",
            "(17, 18): reprojection: 0.315752, disparity: 0.051342\n",
            "(17, 19): reprojection: 0.485778, disparity: 0.063367\n",
            "(18, 19): reprojection: 0.221406, disparity: 0.049543\n",
            "(18, 20): reprojection: 0.321770, disparity: 0.058584\n",
            "(18, 22): reprojection: 0.480377, disparity: 0.070086\n",
            "(19, 20): reprojection: 0.122700, disparity: 0.048032\n",
            "(19, 21): reprojection: 0.263159, disparity: 0.058734\n",
            "(20, 21): reprojection: 0.180541, disparity: 0.049497\n",
            "(20, 22): reprojection: 0.270177, disparity: 0.059067\n",
            "(20, 24): reprojection: 0.338408, disparity: 0.065485\n",
            "(20, 28): reprojection: 0.505035, disparity: 0.087400\n",
            "(21, 22): reprojection: 0.145181, disparity: 0.050730\n",
            "(21, 23): reprojection: 0.213790, disparity: 0.057281\n",
            "(22, 23): reprojection: 0.117050, disparity: 0.050045\n",
            "(22, 24): reprojection: 0.157160, disparity: 0.059010\n",
            "(22, 26): reprojection: 0.338645, disparity: 0.073890\n",
            "(23, 24): reprojection: 0.101789, disparity: 0.051317\n",
            "(23, 25): reprojection: 0.179972, disparity: 0.060885\n",
            "(24, 25): reprojection: 0.121331, disparity: 0.052361\n",
            "(24, 26): reprojection: 0.175292, disparity: 0.060898\n",
            "(24, 28): reprojection: 0.317254, disparity: 0.076782\n",
            "(24, 32): reprojection: 0.595376, disparity: 0.092740\n",
            "(24, 40): reprojection: 0.856137, disparity: 0.175328\n",
            "(25, 26): reprojection: 0.127691, disparity: 0.056096\n",
            "(25, 27): reprojection: 0.221630, disparity: 0.060520\n",
            "(26, 27): reprojection: 0.143001, disparity: 0.052812\n",
            "(26, 28): reprojection: 0.183661, disparity: 0.062571\n",
            "(26, 30): reprojection: 0.324745, disparity: 0.072548\n",
            "(27, 28): reprojection: 0.131432, disparity: 0.052664\n",
            "(27, 29): reprojection: 0.252723, disparity: 0.061976\n",
            "(28, 29): reprojection: 0.174312, disparity: 0.056037\n",
            "(28, 30): reprojection: 0.248395, disparity: 0.062772\n",
            "(28, 32): reprojection: 0.411399, disparity: 0.069161\n",
            "(28, 36): reprojection: 0.609973, disparity: 0.099303\n",
            "(29, 30): reprojection: 0.133848, disparity: 0.052193\n",
            "(29, 31): reprojection: 0.260778, disparity: 0.056833\n",
            "(30, 31): reprojection: 0.187761, disparity: 0.052689\n",
            "(30, 32): reprojection: 0.250977, disparity: 0.061461\n",
            "(30, 34): reprojection: 0.442862, disparity: 0.078219\n",
            "(31, 32): reprojection: 0.127405, disparity: 0.052184\n",
            "(31, 33): reprojection: 0.294434, disparity: 0.064401\n",
            "(32, 33): reprojection: 0.237919, disparity: 0.055260\n",
            "(32, 34): reprojection: 0.233902, disparity: 0.060976\n",
            "(32, 36): reprojection: 0.319414, disparity: 0.079981\n",
            "(32, 40): reprojection: 0.417391, disparity: 0.125806\n",
            "(32, 48): reprojection: 0.814329, disparity: 0.214866\n",
            "(32, 64): reprojection: 1.308140, disparity: 0.405900\n",
            "(33, 34): reprojection: 0.152355, disparity: 0.052068\n",
            "(33, 35): reprojection: 0.246460, disparity: 0.059376\n",
            "(34, 35): reprojection: 0.177315, disparity: 0.051589\n",
            "(34, 36): reprojection: 0.257944, disparity: 0.060686\n",
            "(34, 38): reprojection: 0.339395, disparity: 0.087016\n",
            "(35, 36): reprojection: 0.224605, disparity: 0.055759\n",
            "(35, 37): reprojection: 0.259380, disparity: 0.071449\n",
            "(36, 37): reprojection: 0.252250, disparity: 0.064573\n",
            "(36, 38): reprojection: 0.386640, disparity: 0.069886\n",
            "(36, 40): reprojection: 0.302339, disparity: 0.090478\n",
            "(36, 44): reprojection: 0.603290, disparity: 0.143665\n",
            "(37, 38): reprojection: 0.221353, disparity: 0.057295\n",
            "(37, 39): reprojection: 0.231593, disparity: 0.062709\n",
            "(38, 39): reprojection: 0.187108, disparity: 0.056133\n",
            "(38, 40): reprojection: 0.365928, disparity: 0.069710\n",
            "(38, 42): reprojection: 0.655476, disparity: 0.100879\n",
            "(39, 40): reprojection: 0.279137, disparity: 0.058938\n",
            "(39, 41): reprojection: 0.496605, disparity: 0.076277\n",
            "(40, 41): reprojection: 0.296075, disparity: 0.063263\n",
            "(40, 42): reprojection: 0.429583, disparity: 0.083184\n",
            "(40, 44): reprojection: 0.547326, disparity: 0.114308\n",
            "(40, 48): reprojection: 0.748946, disparity: 0.128975\n",
            "(40, 56): reprojection: 0.705034, disparity: 0.232624\n",
            "(41, 42): reprojection: 0.192748, disparity: 0.062055\n",
            "(41, 43): reprojection: 0.293398, disparity: 0.076170\n",
            "(42, 43): reprojection: 0.177620, disparity: 0.062356\n",
            "(42, 44): reprojection: 0.322008, disparity: 0.076649\n",
            "(42, 46): reprojection: 0.528966, disparity: 0.083060\n",
            "(43, 44): reprojection: 0.195742, disparity: 0.058651\n",
            "(43, 45): reprojection: 0.286238, disparity: 0.066939\n",
            "(44, 45): reprojection: 0.144363, disparity: 0.057653\n",
            "(44, 46): reprojection: 0.284106, disparity: 0.068475\n",
            "(44, 48): reprojection: 0.415536, disparity: 0.085555\n",
            "(44, 52): reprojection: 0.601660, disparity: 0.116096\n",
            "(45, 46): reprojection: 0.200401, disparity: 0.060131\n",
            "(45, 47): reprojection: 0.316255, disparity: 0.074254\n",
            "(46, 47): reprojection: 0.185814, disparity: 0.062316\n",
            "(46, 48): reprojection: 0.292066, disparity: 0.071425\n",
            "(46, 50): reprojection: 0.394501, disparity: 0.094766\n",
            "(47, 48): reprojection: 0.200061, disparity: 0.058792\n",
            "(47, 49): reprojection: 0.250788, disparity: 0.068625\n",
            "(48, 49): reprojection: 0.160687, disparity: 0.068430\n",
            "(48, 50): reprojection: 0.226608, disparity: 0.084272\n",
            "(48, 52): reprojection: 0.354522, disparity: 0.092169\n",
            "(48, 56): reprojection: 0.653734, disparity: 0.108524\n",
            "(48, 64): reprojection: 0.860575, disparity: 0.194630\n",
            "(48, 80): reprojection: 1.058836, disparity: 0.184636\n",
            "(49, 50): reprojection: 0.137449, disparity: 0.064907\n",
            "(49, 51): reprojection: 0.230496, disparity: 0.078872\n",
            "(50, 51): reprojection: 0.149901, disparity: 0.060908\n",
            "(50, 52): reprojection: 0.247940, disparity: 0.069076\n",
            "(50, 54): reprojection: 0.383923, disparity: 0.083613\n",
            "(51, 52): reprojection: 0.169554, disparity: 0.060688\n",
            "(51, 53): reprojection: 0.299743, disparity: 0.068061\n",
            "(52, 53): reprojection: 0.174267, disparity: 0.060834\n",
            "(52, 54): reprojection: 0.247719, disparity: 0.070394\n",
            "(52, 56): reprojection: 0.532550, disparity: 0.074177\n",
            "(52, 60): reprojection: 0.562165, disparity: 0.098827\n",
            "(53, 54): reprojection: 0.130217, disparity: 0.061739\n",
            "(53, 55): reprojection: 0.320652, disparity: 0.074290\n",
            "(54, 55): reprojection: 0.305317, disparity: 0.063341\n",
            "(54, 56): reprojection: 0.556711, disparity: 0.065503\n",
            "(54, 58): reprojection: 0.357385, disparity: 0.079744\n",
            "(55, 56): reprojection: 0.342510, disparity: 0.065030\n",
            "(55, 57): reprojection: 0.371155, disparity: 0.067364\n",
            "(56, 57): reprojection: 0.297798, disparity: 0.065642\n",
            "(56, 58): reprojection: 0.536677, disparity: 0.079697\n",
            "(56, 60): reprojection: 0.651730, disparity: 0.086968\n",
            "(56, 64): reprojection: 0.796893, disparity: 0.092822\n",
            "(56, 72): reprojection: 0.869082, disparity: 0.143141\n",
            "(57, 58): reprojection: 0.346023, disparity: 0.068209\n",
            "(57, 59): reprojection: 0.464906, disparity: 0.072682\n",
            "(58, 59): reprojection: 0.242136, disparity: 0.067037\n",
            "(58, 60): reprojection: 0.351155, disparity: 0.074235\n",
            "(58, 62): reprojection: 0.452722, disparity: 0.086464\n",
            "(59, 60): reprojection: 0.182878, disparity: 0.071161\n",
            "(59, 61): reprojection: 0.295795, disparity: 0.075062\n",
            "(60, 61): reprojection: 0.218685, disparity: 0.069622\n",
            "(60, 62): reprojection: 0.298837, disparity: 0.075266\n",
            "(60, 64): reprojection: 0.487630, disparity: 0.078825\n",
            "(60, 68): reprojection: 0.590719, disparity: 0.106876\n",
            "(61, 62): reprojection: 0.211035, disparity: 0.072147\n",
            "(61, 63): reprojection: 0.292372, disparity: 0.073036\n",
            "(62, 63): reprojection: 0.221187, disparity: 0.066758\n",
            "(62, 64): reprojection: 0.327769, disparity: 0.070900\n",
            "(62, 66): reprojection: 0.482282, disparity: 0.097846\n",
            "(63, 64): reprojection: 0.206383, disparity: 0.065734\n",
            "(63, 65): reprojection: 0.326434, disparity: 0.071160\n",
            "(64, 65): reprojection: 0.208237, disparity: 0.070062\n",
            "(64, 66): reprojection: 0.352785, disparity: 0.070611\n",
            "(64, 68): reprojection: 0.440973, disparity: 0.087524\n",
            "(64, 72): reprojection: 0.687971, disparity: 0.097574\n",
            "(64, 80): reprojection: 1.005068, disparity: 0.112636\n",
            "(65, 66): reprojection: 0.273296, disparity: 0.065598\n",
            "(65, 67): reprojection: 0.414563, disparity: 0.070168\n",
            "(66, 67): reprojection: 0.254339, disparity: 0.069221\n",
            "(66, 68): reprojection: 0.355767, disparity: 0.067898\n",
            "(66, 70): reprojection: 0.433715, disparity: 0.080641\n",
            "(67, 68): reprojection: 0.334253, disparity: 0.063218\n",
            "(67, 69): reprojection: 0.338840, disparity: 0.069547\n",
            "(68, 69): reprojection: 0.200500, disparity: 0.064391\n",
            "(68, 70): reprojection: 0.440981, disparity: 0.072414\n",
            "(68, 72): reprojection: 0.490991, disparity: 0.081368\n",
            "(68, 76): reprojection: 0.622247, disparity: 0.090737\n",
            "(69, 70): reprojection: 0.306979, disparity: 0.066171\n",
            "(69, 71): reprojection: 0.445440, disparity: 0.069506\n",
            "(70, 71): reprojection: 0.208594, disparity: 0.063872\n",
            "(70, 72): reprojection: 0.264034, disparity: 0.068344\n",
            "(70, 74): reprojection: 0.362447, disparity: 0.078259\n",
            "(71, 72): reprojection: 0.229243, disparity: 0.065852\n",
            "(71, 73): reprojection: 0.344586, disparity: 0.073491\n",
            "(72, 73): reprojection: 0.218887, disparity: 0.065516\n",
            "(72, 74): reprojection: 0.280185, disparity: 0.071768\n",
            "(72, 76): reprojection: 0.411908, disparity: 0.075856\n",
            "(72, 80): reprojection: 0.530252, disparity: 0.092325\n",
            "(72, 88): reprojection: 0.828953, disparity: 0.105276\n",
            "(73, 74): reprojection: 0.310777, disparity: 0.063776\n",
            "(73, 75): reprojection: 0.427646, disparity: 0.071060\n",
            "(74, 75): reprojection: 0.232559, disparity: 0.062769\n",
            "(74, 76): reprojection: 0.245067, disparity: 0.066545\n",
            "(74, 78): reprojection: 0.352396, disparity: 0.074830\n",
            "(75, 76): reprojection: 0.225327, disparity: 0.064282\n",
            "(75, 77): reprojection: 0.340255, disparity: 0.073384\n",
            "(76, 77): reprojection: 0.214908, disparity: 0.060817\n",
            "(76, 78): reprojection: 0.307619, disparity: 0.063771\n",
            "(76, 80): reprojection: 0.377898, disparity: 0.072251\n",
            "(76, 84): reprojection: 0.569994, disparity: 0.082557\n",
            "(77, 78): reprojection: 0.214725, disparity: 0.060135\n",
            "(77, 79): reprojection: 0.252116, disparity: 0.064854\n",
            "(78, 79): reprojection: 0.195493, disparity: 0.061987\n",
            "(78, 80): reprojection: 0.251136, disparity: 0.067111\n",
            "(78, 82): reprojection: 0.401167, disparity: 0.075924\n",
            "(79, 80): reprojection: 0.143696, disparity: 0.061321\n",
            "(79, 81): reprojection: 0.234169, disparity: 0.068069\n",
            "(80, 81): reprojection: 0.139121, disparity: 0.059490\n",
            "(80, 82): reprojection: 0.258256, disparity: 0.063009\n",
            "(80, 84): reprojection: 0.455232, disparity: 0.069129\n",
            "(80, 88): reprojection: 0.716477, disparity: 0.087440\n",
            "(81, 82): reprojection: 0.184768, disparity: 0.058621\n",
            "(81, 83): reprojection: 0.299341, disparity: 0.064519\n",
            "(82, 83): reprojection: 0.150602, disparity: 0.057665\n",
            "(82, 84): reprojection: 0.246191, disparity: 0.062986\n",
            "(82, 86): reprojection: 0.461609, disparity: 0.075953\n",
            "(83, 84): reprojection: 0.141213, disparity: 0.056905\n",
            "(83, 85): reprojection: 0.303157, disparity: 0.067223\n",
            "(84, 85): reprojection: 0.172273, disparity: 0.057115\n",
            "(84, 86): reprojection: 0.264910, disparity: 0.063071\n",
            "(84, 88): reprojection: 0.510013, disparity: 0.068157\n",
            "(85, 86): reprojection: 0.168081, disparity: 0.055354\n",
            "(85, 87): reprojection: 0.283891, disparity: 0.060656\n",
            "(86, 87): reprojection: 0.238777, disparity: 0.057490\n",
            "(86, 88): reprojection: 0.337603, disparity: 0.065411\n",
            "(86, 90): reprojection: 0.496545, disparity: 0.069717\n",
            "(87, 88): reprojection: 0.181619, disparity: 0.056999\n",
            "(87, 89): reprojection: 0.281982, disparity: 0.063958\n",
            "(88, 89): reprojection: 0.140021, disparity: 0.053981\n",
            "(88, 90): reprojection: 0.309462, disparity: 0.058534\n",
            "(89, 90): reprojection: 0.214595, disparity: 0.051277\n",
            "(89, 91): reprojection: 0.420511, disparity: 0.062792\n",
            "(90, 91): reprojection: 0.338807, disparity: 0.058506\n",
            "Mean:     reprojection: 0.338807, disparity: 0.058506\n",
            "Done Validation for epoch 10 (2600 iterations)\n",
            "Epoch = 10, pairs = [[48, 80], [8, 9], [0, 2], [13, 14]], loss = 0.5497732162475586\n",
            "Epoch = 10, pairs = [[28, 29], [74, 76], [15, 16], [30, 34]], loss = 0.348832368850708\n",
            "Epoch = 10, pairs = [[7, 8], [29, 30], [5, 6], [80, 82]], loss = 0.2001129686832428\n",
            "Epoch = 10, pairs = [[85, 86], [48, 49], [68, 72], [39, 40]], loss = 0.34702068567276\n",
            "Epoch = 10, pairs = [[31, 32], [14, 18], [54, 56], [72, 76]], loss = 0.4196408987045288\n",
            "Epoch = 10, pairs = [[73, 75], [54, 58], [78, 80], [21, 22]], loss = 0.359130322933197\n",
            "Epoch = 10, pairs = [[21, 23], [8, 10], [25, 26], [4, 8]], loss = 0.31026357412338257\n",
            "Epoch = 10, pairs = [[45, 47], [55, 56], [36, 37], [7, 9]], loss = 0.346153199672699\n",
            "Epoch = 10, pairs = [[50, 51], [68, 76], [70, 71], [46, 47]], loss = 0.37289467453956604\n",
            "Epoch = 10, pairs = [[69, 71], [84, 86], [24, 40], [39, 41]], loss = 0.6200505495071411\n",
            "Epoch = 10, pairs = [[43, 45], [87, 89], [24, 32], [60, 61]], loss = 0.4077950119972229\n",
            "Epoch = 10, pairs = [[90, 91], [0, 4], [4, 5], [56, 72]], loss = 0.5896860361099243\n",
            "Epoch = 10, pairs = [[45, 46], [32, 48], [64, 72], [70, 72]], loss = 0.6345713138580322\n",
            "Epoch = 10, pairs = [[2, 4], [64, 68], [58, 59], [52, 56]], loss = 0.41873303055763245\n",
            "Epoch = 10, pairs = [[19, 21], [44, 45], [78, 82], [20, 22]], loss = 0.32117217779159546\n",
            "Epoch = 10, pairs = [[78, 79], [82, 86], [56, 64], [20, 28]], loss = 0.5785099267959595\n",
            "Epoch = 10, pairs = [[86, 88], [12, 13], [84, 85], [5, 7]], loss = 0.25087571144104004\n",
            "Epoch = 10, pairs = [[83, 85], [80, 88], [16, 24], [68, 69]], loss = 0.5787684917449951\n",
            "Epoch = 10, pairs = [[8, 16], [76, 80], [2, 3], [42, 46]], loss = 0.5107374787330627\n",
            "Epoch = 10, pairs = [[57, 58], [79, 80], [52, 53], [26, 27]], loss = 0.2619298994541168\n",
            "Epoch = 10, pairs = [[0, 16], [16, 17], [89, 90], [60, 68]], loss = 0.5122112035751343\n",
            "Epoch = 10, pairs = [[4, 12], [75, 76], [52, 54], [37, 39]], loss = 0.34226810932159424\n",
            "Epoch = 10, pairs = [[28, 36], [35, 36], [50, 52], [16, 48]], loss = 0.7173218727111816\n",
            "Epoch = 10, pairs = [[16, 18], [53, 54], [46, 50], [56, 60]], loss = 0.45355650782585144\n",
            "Epoch = 10, pairs = [[82, 84], [10, 11], [57, 59], [89, 91]], loss = 0.4224902391433716\n",
            "Epoch = 10, pairs = [[68, 70], [18, 19], [61, 62], [66, 70]], loss = 0.46660304069519043\n",
            "Epoch = 10, pairs = [[6, 7], [34, 36], [37, 38], [42, 44]], loss = 0.28911668062210083\n",
            "Epoch = 10, pairs = [[22, 24], [31, 33], [24, 25], [58, 60]], loss = 0.30620473623275757\n",
            "Epoch = 10, pairs = [[62, 63], [54, 55], [74, 75], [76, 78]], loss = 0.34604761004447937\n",
            "Epoch = 10, pairs = [[32, 40], [63, 65], [71, 72], [67, 69]], loss = 0.41067424416542053\n",
            "Epoch = 10, pairs = [[35, 37], [63, 64], [44, 48], [48, 52]], loss = 0.39007818698883057\n",
            "Epoch = 10, pairs = [[72, 74], [43, 44], [15, 17], [86, 90]], loss = 0.39945393800735474\n",
            "Epoch = 10, pairs = [[18, 22], [65, 66], [46, 48], [3, 4]], loss = 0.35741519927978516\n",
            "Epoch = 10, pairs = [[10, 14], [18, 20], [33, 34], [56, 57]], loss = 0.3324490189552307\n",
            "Epoch = 10, pairs = [[0, 1], [17, 19], [40, 42], [85, 87]], loss = 0.4137687683105469\n",
            "Epoch = 10, pairs = [[12, 14], [3, 5], [49, 50], [47, 49]], loss = 0.28878211975097656\n",
            "Epoch = 10, pairs = [[12, 16], [82, 83], [9, 10], [67, 68]], loss = 0.3355419635772705\n",
            "Epoch = 10, pairs = [[48, 64], [1, 3], [40, 44], [55, 57]], loss = 0.6632071137428284\n",
            "Epoch = 10, pairs = [[23, 24], [32, 34], [1, 2], [32, 64]], loss = 0.49765443801879883\n",
            "Epoch = 10, pairs = [[36, 40], [20, 24], [77, 78], [72, 73]], loss = 0.3422596752643585\n",
            "Epoch = 10, pairs = [[80, 84], [60, 62], [64, 80], [33, 35]], loss = 0.573017418384552\n",
            "Epoch = 10, pairs = [[48, 56], [8, 12], [30, 31], [25, 27]], loss = 0.44899439811706543\n",
            "Epoch = 10, pairs = [[14, 15], [24, 26], [59, 60], [48, 50]], loss = 0.25088372826576233\n",
            "Epoch = 10, pairs = [[61, 63], [28, 32], [40, 48], [27, 28]], loss = 0.5109285712242126\n",
            "Epoch = 10, pairs = [[56, 58], [4, 6], [34, 38], [22, 26]], loss = 0.40990403294563293\n",
            "Epoch = 10, pairs = [[16, 32], [60, 64], [88, 90], [52, 60]], loss = 0.7123037576675415\n",
            "Epoch = 10, pairs = [[42, 43], [84, 88], [79, 81], [70, 74]], loss = 0.39751607179641724\n",
            "Epoch = 10, pairs = [[38, 42], [14, 16], [80, 81], [12, 20]], loss = 0.5317057371139526\n",
            "Epoch = 10, pairs = [[66, 68], [51, 53], [64, 65], [81, 82]], loss = 0.324856698513031\n",
            "Epoch = 10, pairs = [[76, 77], [9, 11], [88, 89], [83, 84]], loss = 0.27907729148864746\n",
            "Epoch = 10, pairs = [[23, 25], [17, 18], [34, 35], [53, 55]], loss = 0.3156523108482361\n",
            "Epoch = 10, pairs = [[76, 84], [6, 8], [44, 46], [72, 80]], loss = 0.5097546577453613\n",
            "Epoch = 10, pairs = [[64, 66], [16, 20], [87, 88], [74, 78]], loss = 0.43278270959854126\n",
            "Epoch = 10, pairs = [[10, 12], [81, 83], [36, 44], [26, 28]], loss = 0.3920784592628479\n",
            "Epoch = 10, pairs = [[86, 87], [62, 66], [2, 6], [0, 32]], loss = 0.6287095546722412\n",
            "Epoch = 10, pairs = [[6, 10], [58, 62], [26, 30], [32, 33]], loss = 0.4117324650287628\n",
            "Epoch = 10, pairs = [[50, 54], [29, 31], [0, 8], [32, 36]], loss = 0.44204771518707275\n",
            "Epoch = 10, pairs = [[19, 20], [66, 67], [51, 52], [11, 13]], loss = 0.2542699873447418\n",
            "Epoch = 10, pairs = [[69, 70], [73, 74], [41, 42], [75, 77]], loss = 0.36874282360076904\n",
            "Epoch = 10, pairs = [[13, 15], [20, 21], [8, 24], [71, 73]], loss = 0.578612208366394\n",
            "Epoch = 10, pairs = [[30, 32], [24, 28], [22, 23], [27, 29]], loss = 0.30795595049858093\n",
            "Epoch = 10, pairs = [[62, 64], [28, 30], [38, 40], [41, 43]], loss = 0.4058750867843628\n",
            "Epoch = 10, pairs = [[11, 12], [77, 79], [40, 56], [40, 41]], loss = 0.43158605694770813\n",
            "Epoch = 10, pairs = [[49, 51], [65, 67], [44, 52], [36, 38]], loss = 0.47314077615737915\n",
            "Epoch = 10, pairs = [[72, 88], [59, 61], [38, 39], [47, 48]], loss = 0.4433286488056183\n",
            "Epoch 10 took 84.28s.\n",
            "( 0,  1): reprojection: 0.232437, disparity: 0.048386\n",
            "( 0,  2): reprojection: 0.347484, disparity: 0.048237\n",
            "( 0,  4): reprojection: 0.388265, disparity: 0.056721\n",
            "( 0,  8): reprojection: 0.507636, disparity: 0.076362\n",
            "( 0, 16): reprojection: 0.777379, disparity: 0.081376\n",
            "( 0, 32): reprojection: 1.041767, disparity: 0.086427\n",
            "( 1,  2): reprojection: 0.196183, disparity: 0.042702\n",
            "( 1,  3): reprojection: 0.252332, disparity: 0.046073\n",
            "( 2,  3): reprojection: 0.109958, disparity: 0.042342\n",
            "( 2,  4): reprojection: 0.181575, disparity: 0.046945\n",
            "( 2,  6): reprojection: 0.349892, disparity: 0.059321\n",
            "( 3,  4): reprojection: 0.108514, disparity: 0.044091\n",
            "( 3,  5): reprojection: 0.204197, disparity: 0.048498\n",
            "( 4,  5): reprojection: 0.155534, disparity: 0.045613\n",
            "( 4,  6): reprojection: 0.221159, disparity: 0.047187\n",
            "( 4,  8): reprojection: 0.289721, disparity: 0.054698\n",
            "( 4, 12): reprojection: 0.369000, disparity: 0.068286\n",
            "( 5,  6): reprojection: 0.115274, disparity: 0.042869\n",
            "( 5,  7): reprojection: 0.161460, disparity: 0.045953\n",
            "( 6,  7): reprojection: 0.098399, disparity: 0.041765\n",
            "( 6,  8): reprojection: 0.164418, disparity: 0.045689\n",
            "( 6, 10): reprojection: 0.375586, disparity: 0.055136\n",
            "( 7,  8): reprojection: 0.089276, disparity: 0.043277\n",
            "( 7,  9): reprojection: 0.189155, disparity: 0.048670\n",
            "( 8,  9): reprojection: 0.155956, disparity: 0.045199\n",
            "( 8, 10): reprojection: 0.350525, disparity: 0.048518\n",
            "( 8, 12): reprojection: 0.469143, disparity: 0.058444\n",
            "( 8, 16): reprojection: 0.614183, disparity: 0.073063\n",
            "( 8, 24): reprojection: 0.727669, disparity: 0.085663\n",
            "( 9, 10): reprojection: 0.242316, disparity: 0.043582\n",
            "( 9, 11): reprojection: 0.361020, disparity: 0.049921\n",
            "(10, 11): reprojection: 0.157338, disparity: 0.041235\n",
            "(10, 12): reprojection: 0.195235, disparity: 0.048068\n",
            "(10, 14): reprojection: 0.326913, disparity: 0.059072\n",
            "(11, 12): reprojection: 0.136995, disparity: 0.041509\n",
            "(11, 13): reprojection: 0.223917, disparity: 0.050851\n",
            "(12, 13): reprojection: 0.132195, disparity: 0.042679\n",
            "(12, 14): reprojection: 0.256080, disparity: 0.048661\n",
            "(12, 16): reprojection: 0.441138, disparity: 0.067274\n",
            "(12, 20): reprojection: 0.671083, disparity: 0.075019\n",
            "(13, 14): reprojection: 0.180601, disparity: 0.043657\n",
            "(13, 15): reprojection: 0.227766, disparity: 0.050123\n",
            "(14, 15): reprojection: 0.188916, disparity: 0.044356\n",
            "(14, 16): reprojection: 0.429515, disparity: 0.054582\n",
            "(14, 18): reprojection: 0.356904, disparity: 0.064290\n",
            "(15, 16): reprojection: 0.296831, disparity: 0.046348\n",
            "(15, 17): reprojection: 0.379186, disparity: 0.057244\n",
            "(16, 17): reprojection: 0.137471, disparity: 0.045937\n",
            "(16, 18): reprojection: 0.307706, disparity: 0.053080\n",
            "(16, 20): reprojection: 0.552274, disparity: 0.062577\n",
            "(16, 24): reprojection: 0.781004, disparity: 0.073771\n",
            "(16, 32): reprojection: 1.251739, disparity: 0.090759\n",
            "(16, 48): reprojection: 1.281840, disparity: 0.156621\n",
            "(17, 18): reprojection: 0.304789, disparity: 0.047386\n",
            "(17, 19): reprojection: 0.467379, disparity: 0.056494\n",
            "(18, 19): reprojection: 0.210837, disparity: 0.045708\n",
            "(18, 20): reprojection: 0.302507, disparity: 0.053504\n",
            "(18, 22): reprojection: 0.473900, disparity: 0.059509\n",
            "(19, 20): reprojection: 0.124214, disparity: 0.044743\n",
            "(19, 21): reprojection: 0.252399, disparity: 0.053474\n",
            "(20, 21): reprojection: 0.176498, disparity: 0.046505\n",
            "(20, 22): reprojection: 0.264730, disparity: 0.053733\n",
            "(20, 24): reprojection: 0.354480, disparity: 0.059000\n",
            "(20, 28): reprojection: 0.619484, disparity: 0.074881\n",
            "(21, 22): reprojection: 0.146056, disparity: 0.047282\n",
            "(21, 23): reprojection: 0.220045, disparity: 0.053458\n",
            "(22, 23): reprojection: 0.115800, disparity: 0.046747\n",
            "(22, 24): reprojection: 0.175482, disparity: 0.055080\n",
            "(22, 26): reprojection: 0.333662, disparity: 0.066382\n",
            "(23, 24): reprojection: 0.106438, disparity: 0.047433\n",
            "(23, 25): reprojection: 0.175349, disparity: 0.055227\n",
            "(24, 25): reprojection: 0.119852, disparity: 0.049086\n",
            "(24, 26): reprojection: 0.173798, disparity: 0.057431\n",
            "(24, 28): reprojection: 0.353347, disparity: 0.066690\n",
            "(24, 32): reprojection: 0.727865, disparity: 0.075405\n",
            "(24, 40): reprojection: 1.000818, disparity: 0.113378\n",
            "(25, 26): reprojection: 0.128852, disparity: 0.052599\n",
            "(25, 27): reprojection: 0.223891, disparity: 0.057046\n",
            "(26, 27): reprojection: 0.140934, disparity: 0.049312\n",
            "(26, 28): reprojection: 0.188999, disparity: 0.055235\n",
            "(26, 30): reprojection: 0.346024, disparity: 0.061265\n",
            "(27, 28): reprojection: 0.130420, disparity: 0.050383\n",
            "(27, 29): reprojection: 0.243159, disparity: 0.056180\n",
            "(28, 29): reprojection: 0.169286, disparity: 0.051821\n",
            "(28, 30): reprojection: 0.239465, disparity: 0.058056\n",
            "(28, 32): reprojection: 0.441027, disparity: 0.063574\n",
            "(28, 36): reprojection: 0.712546, disparity: 0.082154\n",
            "(29, 30): reprojection: 0.132095, disparity: 0.048998\n",
            "(29, 31): reprojection: 0.281772, disparity: 0.054092\n",
            "(30, 31): reprojection: 0.201220, disparity: 0.048824\n",
            "(30, 32): reprojection: 0.272786, disparity: 0.055583\n",
            "(30, 34): reprojection: 0.471816, disparity: 0.067370\n",
            "(31, 32): reprojection: 0.130232, disparity: 0.048269\n",
            "(31, 33): reprojection: 0.310101, disparity: 0.056209\n",
            "(32, 33): reprojection: 0.242660, disparity: 0.051210\n",
            "(32, 34): reprojection: 0.250516, disparity: 0.055810\n",
            "(32, 36): reprojection: 0.371415, disparity: 0.067858\n",
            "(32, 40): reprojection: 0.517170, disparity: 0.086238\n",
            "(32, 48): reprojection: 0.884036, disparity: 0.133931\n",
            "(32, 64): reprojection: 1.094562, disparity: 0.185488\n",
            "(33, 34): reprojection: 0.144219, disparity: 0.050113\n",
            "(33, 35): reprojection: 0.255689, disparity: 0.055227\n",
            "(34, 35): reprojection: 0.181167, disparity: 0.049349\n",
            "(34, 36): reprojection: 0.266121, disparity: 0.057090\n",
            "(34, 38): reprojection: 0.355681, disparity: 0.071091\n",
            "(35, 36): reprojection: 0.222441, disparity: 0.052595\n",
            "(35, 37): reprojection: 0.276824, disparity: 0.062675\n",
            "(36, 37): reprojection: 0.257999, disparity: 0.055143\n",
            "(36, 38): reprojection: 0.394390, disparity: 0.061200\n",
            "(36, 40): reprojection: 0.306880, disparity: 0.068970\n",
            "(36, 44): reprojection: 0.595654, disparity: 0.085732\n",
            "(37, 38): reprojection: 0.221136, disparity: 0.053568\n",
            "(37, 39): reprojection: 0.242634, disparity: 0.058409\n",
            "(38, 39): reprojection: 0.181739, disparity: 0.051054\n",
            "(38, 40): reprojection: 0.351416, disparity: 0.061323\n",
            "(38, 42): reprojection: 0.619367, disparity: 0.076209\n",
            "(39, 40): reprojection: 0.269965, disparity: 0.054967\n",
            "(39, 41): reprojection: 0.474086, disparity: 0.062730\n",
            "(40, 41): reprojection: 0.277570, disparity: 0.056262\n",
            "(40, 42): reprojection: 0.406298, disparity: 0.062906\n",
            "(40, 44): reprojection: 0.520016, disparity: 0.077924\n",
            "(40, 48): reprojection: 0.745163, disparity: 0.106994\n",
            "(40, 56): reprojection: 0.691741, disparity: 0.136614\n",
            "(41, 42): reprojection: 0.188627, disparity: 0.056907\n",
            "(41, 43): reprojection: 0.283419, disparity: 0.063388\n",
            "(42, 43): reprojection: 0.171657, disparity: 0.056598\n",
            "(42, 44): reprojection: 0.315377, disparity: 0.065895\n",
            "(42, 46): reprojection: 0.522920, disparity: 0.075749\n",
            "(43, 44): reprojection: 0.194118, disparity: 0.056732\n",
            "(43, 45): reprojection: 0.290852, disparity: 0.064771\n",
            "(44, 45): reprojection: 0.143455, disparity: 0.055755\n",
            "(44, 46): reprojection: 0.284469, disparity: 0.065253\n",
            "(44, 48): reprojection: 0.443232, disparity: 0.086556\n",
            "(44, 52): reprojection: 0.646051, disparity: 0.095884\n",
            "(45, 46): reprojection: 0.201131, disparity: 0.058565\n",
            "(45, 47): reprojection: 0.319448, disparity: 0.072619\n",
            "(46, 47): reprojection: 0.182867, disparity: 0.060390\n",
            "(46, 48): reprojection: 0.294478, disparity: 0.076579\n",
            "(46, 50): reprojection: 0.401923, disparity: 0.078362\n",
            "(47, 48): reprojection: 0.202467, disparity: 0.059737\n",
            "(47, 49): reprojection: 0.256014, disparity: 0.066173\n",
            "(48, 49): reprojection: 0.160542, disparity: 0.061844\n",
            "(48, 50): reprojection: 0.241273, disparity: 0.069104\n",
            "(48, 52): reprojection: 0.370235, disparity: 0.091908\n",
            "(48, 56): reprojection: 0.705670, disparity: 0.107402\n",
            "(48, 64): reprojection: 0.811093, disparity: 0.121610\n",
            "(48, 80): reprojection: 1.065814, disparity: 0.120119\n",
            "(49, 50): reprojection: 0.138214, disparity: 0.059954\n",
            "(49, 51): reprojection: 0.240236, disparity: 0.067496\n",
            "(50, 51): reprojection: 0.158073, disparity: 0.059925\n",
            "(50, 52): reprojection: 0.248680, disparity: 0.067906\n",
            "(50, 54): reprojection: 0.366785, disparity: 0.077754\n",
            "(51, 52): reprojection: 0.163009, disparity: 0.059929\n",
            "(51, 53): reprojection: 0.287546, disparity: 0.064603\n",
            "(52, 53): reprojection: 0.169013, disparity: 0.060583\n",
            "(52, 54): reprojection: 0.237293, disparity: 0.066477\n",
            "(52, 56): reprojection: 0.531819, disparity: 0.069053\n",
            "(52, 60): reprojection: 0.588088, disparity: 0.086300\n",
            "(53, 54): reprojection: 0.124199, disparity: 0.058104\n",
            "(53, 55): reprojection: 0.313731, disparity: 0.066575\n",
            "(54, 55): reprojection: 0.296654, disparity: 0.056883\n",
            "(54, 56): reprojection: 0.557441, disparity: 0.066286\n",
            "(54, 58): reprojection: 0.395354, disparity: 0.070869\n",
            "(55, 56): reprojection: 0.341508, disparity: 0.061995\n",
            "(55, 57): reprojection: 0.382765, disparity: 0.065538\n",
            "(56, 57): reprojection: 0.293159, disparity: 0.059209\n",
            "(56, 58): reprojection: 0.531370, disparity: 0.070879\n",
            "(56, 60): reprojection: 0.663945, disparity: 0.080271\n",
            "(56, 64): reprojection: 0.791742, disparity: 0.091260\n",
            "(56, 72): reprojection: 0.861197, disparity: 0.127627\n",
            "(57, 58): reprojection: 0.343250, disparity: 0.065701\n",
            "(57, 59): reprojection: 0.462892, disparity: 0.070057\n",
            "(58, 59): reprojection: 0.236525, disparity: 0.065052\n",
            "(58, 60): reprojection: 0.350463, disparity: 0.072131\n",
            "(58, 62): reprojection: 0.467735, disparity: 0.086779\n",
            "(59, 60): reprojection: 0.183841, disparity: 0.068202\n",
            "(59, 61): reprojection: 0.303934, disparity: 0.072721\n",
            "(60, 61): reprojection: 0.220791, disparity: 0.067833\n",
            "(60, 62): reprojection: 0.305805, disparity: 0.075569\n",
            "(60, 64): reprojection: 0.489228, disparity: 0.079024\n",
            "(60, 68): reprojection: 0.561620, disparity: 0.116234\n",
            "(61, 62): reprojection: 0.205057, disparity: 0.069946\n",
            "(61, 63): reprojection: 0.288228, disparity: 0.073453\n",
            "(62, 63): reprojection: 0.213344, disparity: 0.063326\n",
            "(62, 64): reprojection: 0.308411, disparity: 0.069850\n",
            "(62, 66): reprojection: 0.433943, disparity: 0.091947\n",
            "(63, 64): reprojection: 0.201722, disparity: 0.063612\n",
            "(63, 65): reprojection: 0.310745, disparity: 0.068008\n",
            "(64, 65): reprojection: 0.200240, disparity: 0.066897\n",
            "(64, 66): reprojection: 0.333769, disparity: 0.067814\n",
            "(64, 68): reprojection: 0.409475, disparity: 0.088328\n",
            "(64, 72): reprojection: 0.636546, disparity: 0.107542\n",
            "(64, 80): reprojection: 0.899261, disparity: 0.119878\n",
            "(65, 66): reprojection: 0.258915, disparity: 0.061509\n",
            "(65, 67): reprojection: 0.391524, disparity: 0.067525\n",
            "(66, 67): reprojection: 0.245354, disparity: 0.066634\n",
            "(66, 68): reprojection: 0.344502, disparity: 0.068669\n",
            "(66, 70): reprojection: 0.440184, disparity: 0.087498\n",
            "(67, 68): reprojection: 0.327631, disparity: 0.061835\n",
            "(67, 69): reprojection: 0.339953, disparity: 0.069681\n",
            "(68, 69): reprojection: 0.202855, disparity: 0.063608\n",
            "(68, 70): reprojection: 0.441385, disparity: 0.068931\n",
            "(68, 72): reprojection: 0.498328, disparity: 0.082169\n",
            "(68, 76): reprojection: 0.616178, disparity: 0.094198\n",
            "(69, 70): reprojection: 0.307714, disparity: 0.063143\n",
            "(69, 71): reprojection: 0.439881, disparity: 0.066422\n",
            "(70, 71): reprojection: 0.205600, disparity: 0.062063\n",
            "(70, 72): reprojection: 0.264820, disparity: 0.065991\n",
            "(70, 74): reprojection: 0.356534, disparity: 0.078222\n",
            "(71, 72): reprojection: 0.226254, disparity: 0.062326\n",
            "(71, 73): reprojection: 0.337042, disparity: 0.068340\n",
            "(72, 73): reprojection: 0.214004, disparity: 0.061901\n",
            "(72, 74): reprojection: 0.275608, disparity: 0.068021\n",
            "(72, 76): reprojection: 0.382611, disparity: 0.071037\n",
            "(72, 80): reprojection: 0.512779, disparity: 0.084713\n",
            "(72, 88): reprojection: 0.804481, disparity: 0.093168\n",
            "(73, 74): reprojection: 0.308594, disparity: 0.060622\n",
            "(73, 75): reprojection: 0.426264, disparity: 0.067349\n",
            "(74, 75): reprojection: 0.233481, disparity: 0.060708\n",
            "(74, 76): reprojection: 0.240965, disparity: 0.062363\n",
            "(74, 78): reprojection: 0.353110, disparity: 0.069208\n",
            "(75, 76): reprojection: 0.221947, disparity: 0.059550\n",
            "(75, 77): reprojection: 0.335996, disparity: 0.065388\n",
            "(76, 77): reprojection: 0.215453, disparity: 0.058806\n",
            "(76, 78): reprojection: 0.312295, disparity: 0.060389\n",
            "(76, 80): reprojection: 0.386324, disparity: 0.068995\n",
            "(76, 84): reprojection: 0.609815, disparity: 0.080066\n",
            "(77, 78): reprojection: 0.212559, disparity: 0.057609\n",
            "(77, 79): reprojection: 0.249798, disparity: 0.062098\n",
            "(78, 79): reprojection: 0.197426, disparity: 0.060118\n",
            "(78, 80): reprojection: 0.245057, disparity: 0.064251\n",
            "(78, 82): reprojection: 0.395899, disparity: 0.070564\n",
            "(79, 80): reprojection: 0.139422, disparity: 0.059407\n",
            "(79, 81): reprojection: 0.227692, disparity: 0.061987\n",
            "(80, 81): reprojection: 0.139831, disparity: 0.056269\n",
            "(80, 82): reprojection: 0.261284, disparity: 0.059682\n",
            "(80, 84): reprojection: 0.463039, disparity: 0.063055\n",
            "(80, 88): reprojection: 0.707123, disparity: 0.083538\n",
            "(81, 82): reprojection: 0.185100, disparity: 0.055327\n",
            "(81, 83): reprojection: 0.299916, disparity: 0.061033\n",
            "(82, 83): reprojection: 0.148424, disparity: 0.054641\n",
            "(82, 84): reprojection: 0.251205, disparity: 0.059580\n",
            "(82, 86): reprojection: 0.444920, disparity: 0.074228\n",
            "(83, 84): reprojection: 0.144215, disparity: 0.054279\n",
            "(83, 85): reprojection: 0.305303, disparity: 0.063503\n",
            "(84, 85): reprojection: 0.172169, disparity: 0.053493\n",
            "(84, 86): reprojection: 0.260753, disparity: 0.060966\n",
            "(84, 88): reprojection: 0.498788, disparity: 0.068319\n",
            "(85, 86): reprojection: 0.167062, disparity: 0.053320\n",
            "(85, 87): reprojection: 0.279968, disparity: 0.059131\n",
            "(86, 87): reprojection: 0.233797, disparity: 0.056359\n",
            "(86, 88): reprojection: 0.332215, disparity: 0.063854\n",
            "(86, 90): reprojection: 0.490847, disparity: 0.066734\n",
            "(87, 88): reprojection: 0.178648, disparity: 0.055332\n",
            "(87, 89): reprojection: 0.278231, disparity: 0.061330\n",
            "(88, 89): reprojection: 0.140628, disparity: 0.052372\n",
            "(88, 90): reprojection: 0.308120, disparity: 0.056325\n",
            "(89, 90): reprojection: 0.213629, disparity: 0.049204\n",
            "(89, 91): reprojection: 0.413599, disparity: 0.059246\n",
            "(90, 91): reprojection: 0.333180, disparity: 0.055487\n",
            "Mean:     reprojection: 0.333180, disparity: 0.055487\n",
            "Done Validation for epoch 11 (2860 iterations)\n",
            "Epoch = 11, pairs = [[84, 86], [1, 3], [59, 60], [22, 26]], loss = 0.3196685016155243\n",
            "Epoch = 11, pairs = [[31, 32], [78, 80], [61, 62], [68, 70]], loss = 0.32042473554611206\n",
            "Epoch = 11, pairs = [[64, 65], [21, 23], [24, 25], [29, 30]], loss = 0.22418071329593658\n",
            "Epoch = 11, pairs = [[0, 32], [66, 70], [42, 44], [66, 67]], loss = 0.6940540075302124\n",
            "Epoch = 11, pairs = [[26, 30], [50, 54], [39, 40], [58, 59]], loss = 0.3735436797142029\n",
            "Epoch = 11, pairs = [[28, 29], [26, 27], [85, 87], [60, 62]], loss = 0.2735954821109772\n",
            "Epoch = 11, pairs = [[55, 56], [44, 46], [48, 52], [63, 64]], loss = 0.3703456223011017\n",
            "Epoch = 11, pairs = [[80, 84], [43, 44], [17, 18], [1, 2]], loss = 0.3316342830657959\n",
            "Epoch = 11, pairs = [[34, 38], [24, 28], [33, 35], [65, 67]], loss = 0.41183677315711975\n",
            "Epoch = 11, pairs = [[40, 44], [59, 61], [4, 6], [50, 52]], loss = 0.39463382959365845\n",
            "Epoch = 11, pairs = [[37, 38], [0, 16], [43, 45], [56, 60]], loss = 0.5557044744491577\n",
            "Epoch = 11, pairs = [[3, 4], [70, 71], [0, 4], [6, 8]], loss = 0.25760337710380554\n",
            "Epoch = 11, pairs = [[82, 83], [28, 30], [10, 11], [73, 74]], loss = 0.2928880453109741\n",
            "Epoch = 11, pairs = [[30, 32], [17, 19], [4, 5], [12, 14]], loss = 0.35379552841186523\n",
            "Epoch = 11, pairs = [[5, 6], [87, 88], [16, 18], [35, 36]], loss = 0.28272923827171326\n",
            "Epoch = 11, pairs = [[54, 58], [23, 25], [5, 7], [51, 52]], loss = 0.2805628776550293\n",
            "Epoch = 11, pairs = [[72, 74], [6, 10], [49, 51], [36, 44]], loss = 0.49992835521698\n",
            "Epoch = 11, pairs = [[90, 91], [4, 12], [19, 20], [28, 32]], loss = 0.3935846984386444\n",
            "Epoch = 11, pairs = [[60, 61], [37, 39], [46, 48], [11, 13]], loss = 0.3023933470249176\n",
            "Epoch = 11, pairs = [[22, 23], [79, 81], [70, 72], [74, 76]], loss = 0.27531200647354126\n",
            "Epoch = 11, pairs = [[13, 15], [41, 43], [80, 82], [8, 24]], loss = 0.47866567969322205\n",
            "Epoch = 11, pairs = [[47, 48], [55, 57], [24, 26], [52, 60]], loss = 0.44264617562294006\n",
            "Epoch = 11, pairs = [[56, 57], [61, 63], [86, 90], [88, 89]], loss = 0.36907562613487244\n",
            "Epoch = 11, pairs = [[41, 42], [14, 15], [29, 31], [2, 4]], loss = 0.25230714678764343\n",
            "Epoch = 11, pairs = [[68, 72], [20, 21], [12, 20], [44, 45]], loss = 0.43490469455718994\n",
            "Epoch = 11, pairs = [[74, 78], [24, 32], [50, 51], [18, 20]], loss = 0.4077030122280121\n",
            "Epoch = 11, pairs = [[26, 28], [72, 88], [27, 28], [77, 79]], loss = 0.4605370759963989\n",
            "Epoch = 11, pairs = [[7, 8], [64, 80], [48, 50], [16, 20]], loss = 0.5476772785186768\n",
            "Epoch = 11, pairs = [[0, 2], [12, 16], [9, 11], [35, 37]], loss = 0.41656526923179626\n",
            "Epoch = 11, pairs = [[11, 12], [68, 76], [16, 32], [31, 33]], loss = 0.6060439944267273\n",
            "Epoch = 11, pairs = [[82, 84], [25, 27], [18, 22], [54, 56]], loss = 0.42814335227012634\n",
            "Epoch = 11, pairs = [[38, 39], [8, 9], [81, 83], [30, 34]], loss = 0.31676846742630005\n",
            "Epoch = 11, pairs = [[32, 34], [62, 66], [72, 73], [13, 14]], loss = 0.3525344431400299\n",
            "Epoch = 11, pairs = [[42, 43], [53, 54], [3, 5], [64, 66]], loss = 0.28553760051727295\n",
            "Epoch = 11, pairs = [[67, 69], [20, 22], [81, 82], [45, 47]], loss = 0.3570719361305237\n",
            "Epoch = 11, pairs = [[45, 46], [16, 24], [75, 76], [57, 59]], loss = 0.5114567279815674\n",
            "Epoch = 11, pairs = [[32, 40], [83, 84], [56, 64], [66, 68]], loss = 0.5631279349327087\n",
            "Epoch = 11, pairs = [[44, 52], [4, 8], [36, 37], [36, 38]], loss = 0.4307239353656769\n",
            "Epoch = 11, pairs = [[19, 21], [38, 42], [52, 54], [27, 29]], loss = 0.42726433277130127\n",
            "Epoch = 11, pairs = [[9, 10], [69, 71], [12, 13], [58, 62]], loss = 0.3736342489719391\n",
            "Epoch = 11, pairs = [[48, 49], [48, 80], [70, 74], [60, 64]], loss = 0.8222644925117493\n",
            "Epoch = 11, pairs = [[32, 36], [24, 40], [20, 28], [73, 75]], loss = 0.7044485807418823\n",
            "Epoch = 11, pairs = [[62, 64], [18, 19], [23, 24], [48, 64]], loss = 0.7486876249313354\n",
            "Epoch = 11, pairs = [[89, 91], [30, 31], [16, 17], [87, 89]], loss = 0.34015730023384094\n",
            "Epoch = 11, pairs = [[14, 16], [56, 58], [51, 53], [2, 3]], loss = 0.4767165184020996\n",
            "Epoch = 11, pairs = [[7, 9], [52, 56], [36, 40], [10, 12]], loss = 0.5048003196716309\n",
            "Epoch = 11, pairs = [[69, 70], [21, 22], [6, 7], [58, 60]], loss = 0.34165582060813904\n",
            "Epoch = 11, pairs = [[82, 86], [49, 50], [68, 69], [8, 16]], loss = 0.5594056844711304\n",
            "Epoch = 11, pairs = [[33, 34], [62, 63], [80, 81], [78, 82]], loss = 0.33069390058517456\n",
            "Epoch = 11, pairs = [[56, 72], [76, 84], [71, 72], [86, 88]], loss = 0.7975201606750488\n",
            "Epoch = 11, pairs = [[79, 80], [10, 14], [15, 16], [0, 8]], loss = 0.40393054485321045\n",
            "Epoch = 11, pairs = [[2, 6], [14, 18], [8, 12], [52, 53]], loss = 0.4637816846370697\n",
            "Epoch = 11, pairs = [[0, 1], [57, 58], [89, 90], [72, 76]], loss = 0.4483718276023865\n",
            "Epoch = 11, pairs = [[48, 56], [46, 47], [46, 50], [40, 41]], loss = 0.7752084136009216\n",
            "Epoch = 11, pairs = [[53, 55], [34, 35], [76, 78], [76, 80]], loss = 0.5456531047821045\n",
            "Epoch = 11, pairs = [[15, 17], [84, 88], [32, 33], [39, 41]], loss = 0.5280843377113342\n",
            "Epoch = 11, pairs = [[40, 48], [63, 65], [40, 56], [76, 77]], loss = 1.055129051208496\n",
            "Epoch = 11, pairs = [[86, 87], [42, 46], [64, 72], [20, 24]], loss = 0.6682544946670532\n",
            "Epoch = 11, pairs = [[64, 68], [22, 24], [32, 48], [74, 75]], loss = 0.6105765104293823\n",
            "Epoch = 11, pairs = [[80, 88], [38, 40], [88, 90], [84, 85]], loss = 0.5610111355781555\n",
            "Epoch = 11, pairs = [[47, 49], [72, 80], [32, 64], [16, 48]], loss = 1.636289119720459\n",
            "Epoch = 11, pairs = [[25, 26], [34, 36], [8, 10], [65, 66]], loss = 0.33610299229621887\n",
            "Epoch = 11, pairs = [[67, 68], [40, 42], [54, 55], [28, 36]], loss = 0.547476589679718\n",
            "Epoch = 11, pairs = [[44, 48], [75, 77], [71, 73], [85, 86]], loss = 0.44613611698150635\n",
            "Epoch = 11, pairs = [[83, 85], [78, 79], [60, 68], [77, 78]], loss = 0.5492234826087952\n",
            "Epoch 11 took 84.95s.\n",
            "( 0,  1): reprojection: 0.250397, disparity: 0.068247\n",
            "( 0,  2): reprojection: 0.381032, disparity: 0.069241\n",
            "( 0,  4): reprojection: 0.457436, disparity: 0.080615\n",
            "( 0,  8): reprojection: 0.590044, disparity: 0.092860\n",
            "( 0, 16): reprojection: 1.414086, disparity: 0.107271\n",
            "( 0, 32): reprojection: 3.405924, disparity: 0.194608\n",
            "( 1,  2): reprojection: 0.200715, disparity: 0.049730\n",
            "( 1,  3): reprojection: 0.260697, disparity: 0.055406\n",
            "( 2,  3): reprojection: 0.117880, disparity: 0.049879\n",
            "( 2,  4): reprojection: 0.180127, disparity: 0.058075\n",
            "( 2,  6): reprojection: 0.353333, disparity: 0.070385\n",
            "( 3,  4): reprojection: 0.121982, disparity: 0.052493\n",
            "( 3,  5): reprojection: 0.201181, disparity: 0.062243\n",
            "( 4,  5): reprojection: 0.153161, disparity: 0.051529\n",
            "( 4,  6): reprojection: 0.226133, disparity: 0.058153\n",
            "( 4,  8): reprojection: 0.360252, disparity: 0.070611\n",
            "( 4, 12): reprojection: 0.635505, disparity: 0.092868\n",
            "( 5,  6): reprojection: 0.129823, disparity: 0.049039\n",
            "( 5,  7): reprojection: 0.198454, disparity: 0.057512\n",
            "( 6,  7): reprojection: 0.121418, disparity: 0.048348\n",
            "( 6,  8): reprojection: 0.215531, disparity: 0.055466\n",
            "( 6, 10): reprojection: 0.534106, disparity: 0.074851\n",
            "( 7,  8): reprojection: 0.115711, disparity: 0.049074\n",
            "( 7,  9): reprojection: 0.253011, disparity: 0.058117\n",
            "( 8,  9): reprojection: 0.200593, disparity: 0.051017\n",
            "( 8, 10): reprojection: 0.419686, disparity: 0.060990\n",
            "( 8, 12): reprojection: 0.651789, disparity: 0.076076\n",
            "( 8, 16): reprojection: 1.073316, disparity: 0.097627\n",
            "( 8, 24): reprojection: 1.871380, disparity: 0.140070\n",
            "( 9, 10): reprojection: 0.267869, disparity: 0.052099\n",
            "( 9, 11): reprojection: 0.423597, disparity: 0.063854\n",
            "(10, 11): reprojection: 0.187869, disparity: 0.047782\n",
            "(10, 12): reprojection: 0.275304, disparity: 0.057019\n",
            "(10, 14): reprojection: 0.542864, disparity: 0.075802\n",
            "(11, 12): reprojection: 0.160577, disparity: 0.046556\n",
            "(11, 13): reprojection: 0.286404, disparity: 0.057770\n",
            "(12, 13): reprojection: 0.188793, disparity: 0.047701\n",
            "(12, 14): reprojection: 0.381820, disparity: 0.060430\n",
            "(12, 16): reprojection: 0.701950, disparity: 0.083390\n",
            "(12, 20): reprojection: 1.244677, disparity: 0.108623\n",
            "(13, 14): reprojection: 0.231004, disparity: 0.052494\n",
            "(13, 15): reprojection: 0.376552, disparity: 0.060040\n",
            "(14, 15): reprojection: 0.246498, disparity: 0.050150\n",
            "(14, 16): reprojection: 0.516855, disparity: 0.061439\n",
            "(14, 18): reprojection: 0.636451, disparity: 0.079886\n",
            "(15, 16): reprojection: 0.351003, disparity: 0.054692\n",
            "(15, 17): reprojection: 0.475494, disparity: 0.064148\n",
            "(16, 17): reprojection: 0.189723, disparity: 0.050482\n",
            "(16, 18): reprojection: 0.437989, disparity: 0.073152\n",
            "(16, 20): reprojection: 0.774689, disparity: 0.091616\n",
            "(16, 24): reprojection: 1.382486, disparity: 0.117614\n",
            "(16, 32): reprojection: 2.720371, disparity: 0.190833\n",
            "(16, 48): reprojection: 4.878230, disparity: 0.859739\n",
            "(17, 18): reprojection: 0.357172, disparity: 0.058686\n",
            "(17, 19): reprojection: 0.587067, disparity: 0.077204\n",
            "(18, 19): reprojection: 0.263985, disparity: 0.054477\n",
            "(18, 20): reprojection: 0.367898, disparity: 0.066235\n",
            "(18, 22): reprojection: 0.780484, disparity: 0.085239\n",
            "(19, 20): reprojection: 0.169217, disparity: 0.053254\n",
            "(19, 21): reprojection: 0.361080, disparity: 0.065170\n",
            "(20, 21): reprojection: 0.266005, disparity: 0.054667\n",
            "(20, 22): reprojection: 0.448300, disparity: 0.069282\n",
            "(20, 24): reprojection: 0.712896, disparity: 0.081935\n",
            "(20, 28): reprojection: 1.467961, disparity: 0.125880\n",
            "(21, 22): reprojection: 0.238096, disparity: 0.054817\n",
            "(21, 23): reprojection: 0.426743, disparity: 0.065721\n",
            "(22, 23): reprojection: 0.207536, disparity: 0.054686\n",
            "(22, 24): reprojection: 0.340403, disparity: 0.065541\n",
            "(22, 26): reprojection: 0.594657, disparity: 0.083919\n",
            "(23, 24): reprojection: 0.159737, disparity: 0.055005\n",
            "(23, 25): reprojection: 0.308775, disparity: 0.066446\n",
            "(24, 25): reprojection: 0.220275, disparity: 0.056639\n",
            "(24, 26): reprojection: 0.327846, disparity: 0.069940\n",
            "(24, 28): reprojection: 0.596862, disparity: 0.091250\n",
            "(24, 32): reprojection: 1.185773, disparity: 0.121984\n",
            "(24, 40): reprojection: 1.938253, disparity: 0.280236\n",
            "(25, 26): reprojection: 0.182743, disparity: 0.061138\n",
            "(25, 27): reprojection: 0.359571, disparity: 0.073108\n",
            "(26, 27): reprojection: 0.220977, disparity: 0.060913\n",
            "(26, 28): reprojection: 0.325849, disparity: 0.074749\n",
            "(26, 30): reprojection: 0.618793, disparity: 0.094430\n",
            "(27, 28): reprojection: 0.199307, disparity: 0.059491\n",
            "(27, 29): reprojection: 0.375108, disparity: 0.072380\n",
            "(28, 29): reprojection: 0.234456, disparity: 0.059818\n",
            "(28, 30): reprojection: 0.356510, disparity: 0.071253\n",
            "(28, 32): reprojection: 0.742166, disparity: 0.090984\n",
            "(28, 36): reprojection: 1.328651, disparity: 0.151942\n",
            "(29, 30): reprojection: 0.193988, disparity: 0.057095\n",
            "(29, 31): reprojection: 0.389597, disparity: 0.069660\n",
            "(30, 31): reprojection: 0.230197, disparity: 0.058076\n",
            "(30, 32): reprojection: 0.393245, disparity: 0.070265\n",
            "(30, 34): reprojection: 0.759264, disparity: 0.108505\n",
            "(31, 32): reprojection: 0.174983, disparity: 0.056396\n",
            "(31, 33): reprojection: 0.441056, disparity: 0.074322\n",
            "(32, 33): reprojection: 0.308290, disparity: 0.064711\n",
            "(32, 34): reprojection: 0.395301, disparity: 0.081034\n",
            "(32, 36): reprojection: 0.658038, disparity: 0.115338\n",
            "(32, 40): reprojection: 1.015473, disparity: 0.226038\n",
            "(32, 48): reprojection: 2.078916, disparity: 0.371997\n",
            "(32, 64): reprojection: 3.768615, disparity: 0.671952\n",
            "(33, 34): reprojection: 0.178858, disparity: 0.058045\n",
            "(33, 35): reprojection: 0.366568, disparity: 0.074736\n",
            "(34, 35): reprojection: 0.235113, disparity: 0.059280\n",
            "(34, 36): reprojection: 0.405618, disparity: 0.079054\n",
            "(34, 38): reprojection: 0.615202, disparity: 0.116815\n",
            "(35, 36): reprojection: 0.282883, disparity: 0.063248\n",
            "(35, 37): reprojection: 0.386883, disparity: 0.089615\n",
            "(36, 37): reprojection: 0.304812, disparity: 0.070328\n",
            "(36, 38): reprojection: 0.430454, disparity: 0.081601\n",
            "(36, 40): reprojection: 0.536540, disparity: 0.139029\n",
            "(36, 44): reprojection: 1.160590, disparity: 0.224905\n",
            "(37, 38): reprojection: 0.247557, disparity: 0.060562\n",
            "(37, 39): reprojection: 0.329866, disparity: 0.081736\n",
            "(38, 39): reprojection: 0.244798, disparity: 0.065960\n",
            "(38, 40): reprojection: 0.442967, disparity: 0.108499\n",
            "(38, 42): reprojection: 0.811492, disparity: 0.148740\n",
            "(39, 40): reprojection: 0.315856, disparity: 0.082817\n",
            "(39, 41): reprojection: 0.564346, disparity: 0.103099\n",
            "(40, 41): reprojection: 0.321526, disparity: 0.067243\n",
            "(40, 42): reprojection: 0.463896, disparity: 0.082338\n",
            "(40, 44): reprojection: 0.687014, disparity: 0.118211\n",
            "(40, 48): reprojection: 1.110328, disparity: 0.217638\n",
            "(40, 56): reprojection: 1.608730, disparity: 0.246259\n",
            "(41, 42): reprojection: 0.218305, disparity: 0.065685\n",
            "(41, 43): reprojection: 0.382195, disparity: 0.088397\n",
            "(42, 43): reprojection: 0.201404, disparity: 0.068010\n",
            "(42, 44): reprojection: 0.388534, disparity: 0.086911\n",
            "(42, 46): reprojection: 0.650489, disparity: 0.141178\n",
            "(43, 44): reprojection: 0.252451, disparity: 0.074093\n",
            "(43, 45): reprojection: 0.396000, disparity: 0.106967\n",
            "(44, 45): reprojection: 0.184088, disparity: 0.065058\n",
            "(44, 46): reprojection: 0.331730, disparity: 0.094920\n",
            "(44, 48): reprojection: 0.556533, disparity: 0.153815\n",
            "(44, 52): reprojection: 0.938682, disparity: 0.206594\n",
            "(45, 46): reprojection: 0.226767, disparity: 0.074920\n",
            "(45, 47): reprojection: 0.420807, disparity: 0.138815\n",
            "(46, 47): reprojection: 0.234951, disparity: 0.068607\n",
            "(46, 48): reprojection: 0.349625, disparity: 0.089184\n",
            "(46, 50): reprojection: 0.536779, disparity: 0.100692\n",
            "(47, 48): reprojection: 0.224683, disparity: 0.065621\n",
            "(47, 49): reprojection: 0.326478, disparity: 0.076034\n",
            "(48, 49): reprojection: 0.209387, disparity: 0.063954\n",
            "(48, 50): reprojection: 0.350711, disparity: 0.073713\n",
            "(48, 52): reprojection: 0.498687, disparity: 0.101213\n",
            "(48, 56): reprojection: 0.988542, disparity: 0.138066\n",
            "(48, 64): reprojection: 1.642180, disparity: 0.173966\n",
            "(48, 80): reprojection: 4.082412, disparity: 0.191311\n",
            "(49, 50): reprojection: 0.194160, disparity: 0.064054\n",
            "(49, 51): reprojection: 0.325225, disparity: 0.074575\n",
            "(50, 51): reprojection: 0.174504, disparity: 0.064572\n",
            "(50, 52): reprojection: 0.307653, disparity: 0.072352\n",
            "(50, 54): reprojection: 0.489236, disparity: 0.089610\n",
            "(51, 52): reprojection: 0.182470, disparity: 0.062813\n",
            "(51, 53): reprojection: 0.334746, disparity: 0.077539\n",
            "(52, 53): reprojection: 0.185702, disparity: 0.064240\n",
            "(52, 54): reprojection: 0.309200, disparity: 0.079359\n",
            "(52, 56): reprojection: 0.617418, disparity: 0.080109\n",
            "(52, 60): reprojection: 0.963991, disparity: 0.120999\n",
            "(53, 54): reprojection: 0.171920, disparity: 0.062481\n",
            "(53, 55): reprojection: 0.422389, disparity: 0.088712\n",
            "(54, 55): reprojection: 0.327246, disparity: 0.065523\n",
            "(54, 56): reprojection: 0.607725, disparity: 0.070456\n",
            "(54, 58): reprojection: 0.517339, disparity: 0.090045\n",
            "(55, 56): reprojection: 0.376927, disparity: 0.069149\n",
            "(55, 57): reprojection: 0.460206, disparity: 0.073184\n",
            "(56, 57): reprojection: 0.336972, disparity: 0.073393\n",
            "(56, 58): reprojection: 0.598949, disparity: 0.096664\n",
            "(56, 60): reprojection: 0.783366, disparity: 0.103930\n",
            "(56, 64): reprojection: 1.288735, disparity: 0.127604\n",
            "(56, 72): reprojection: 2.189710, disparity: 0.129232\n",
            "(57, 58): reprojection: 0.369263, disparity: 0.071839\n",
            "(57, 59): reprojection: 0.511656, disparity: 0.081681\n",
            "(58, 59): reprojection: 0.263927, disparity: 0.071176\n",
            "(58, 60): reprojection: 0.453108, disparity: 0.095880\n",
            "(58, 62): reprojection: 0.551105, disparity: 0.096824\n",
            "(59, 60): reprojection: 0.223018, disparity: 0.072312\n",
            "(59, 61): reprojection: 0.337776, disparity: 0.084636\n",
            "(60, 61): reprojection: 0.228440, disparity: 0.070882\n",
            "(60, 62): reprojection: 0.333154, disparity: 0.083397\n",
            "(60, 64): reprojection: 0.601863, disparity: 0.096324\n",
            "(60, 68): reprojection: 1.045162, disparity: 0.112980\n",
            "(61, 62): reprojection: 0.211893, disparity: 0.074940\n",
            "(61, 63): reprojection: 0.353058, disparity: 0.077497\n",
            "(62, 63): reprojection: 0.263397, disparity: 0.067433\n",
            "(62, 64): reprojection: 0.451016, disparity: 0.076801\n",
            "(62, 66): reprojection: 0.864539, disparity: 0.116114\n",
            "(63, 64): reprojection: 0.257717, disparity: 0.071775\n",
            "(63, 65): reprojection: 0.446502, disparity: 0.075783\n",
            "(64, 65): reprojection: 0.283996, disparity: 0.075340\n",
            "(64, 66): reprojection: 0.497760, disparity: 0.080659\n",
            "(64, 68): reprojection: 0.731353, disparity: 0.111327\n",
            "(64, 72): reprojection: 1.354659, disparity: 0.127347\n",
            "(64, 80): reprojection: 2.435843, disparity: 0.153663\n",
            "(65, 66): reprojection: 0.317263, disparity: 0.068866\n",
            "(65, 67): reprojection: 0.517385, disparity: 0.081520\n",
            "(66, 67): reprojection: 0.304669, disparity: 0.075217\n",
            "(66, 68): reprojection: 0.456775, disparity: 0.078966\n",
            "(66, 70): reprojection: 0.675787, disparity: 0.097079\n",
            "(67, 68): reprojection: 0.370196, disparity: 0.069586\n",
            "(67, 69): reprojection: 0.379815, disparity: 0.074017\n",
            "(68, 69): reprojection: 0.241039, disparity: 0.066958\n",
            "(68, 70): reprojection: 0.509297, disparity: 0.077788\n",
            "(68, 72): reprojection: 0.747609, disparity: 0.090008\n",
            "(68, 76): reprojection: 1.161670, disparity: 0.107356\n",
            "(69, 70): reprojection: 0.328692, disparity: 0.067209\n",
            "(69, 71): reprojection: 0.499686, disparity: 0.071702\n",
            "(70, 71): reprojection: 0.259326, disparity: 0.066066\n",
            "(70, 72): reprojection: 0.381749, disparity: 0.072671\n",
            "(70, 74): reprojection: 0.658824, disparity: 0.082042\n",
            "(71, 72): reprojection: 0.257503, disparity: 0.066824\n",
            "(71, 73): reprojection: 0.401866, disparity: 0.074701\n",
            "(72, 73): reprojection: 0.255816, disparity: 0.067689\n",
            "(72, 74): reprojection: 0.393974, disparity: 0.073904\n",
            "(72, 76): reprojection: 0.515698, disparity: 0.083890\n",
            "(72, 80): reprojection: 1.145535, disparity: 0.113520\n",
            "(72, 88): reprojection: 1.440065, disparity: 0.153872\n",
            "(73, 74): reprojection: 0.354219, disparity: 0.066503\n",
            "(73, 75): reprojection: 0.499590, disparity: 0.074305\n",
            "(74, 75): reprojection: 0.265738, disparity: 0.065659\n",
            "(74, 76): reprojection: 0.332498, disparity: 0.072405\n",
            "(74, 78): reprojection: 0.557950, disparity: 0.086710\n",
            "(75, 76): reprojection: 0.253008, disparity: 0.065147\n",
            "(75, 77): reprojection: 0.408895, disparity: 0.073123\n",
            "(76, 77): reprojection: 0.262008, disparity: 0.062745\n",
            "(76, 78): reprojection: 0.437197, disparity: 0.067541\n",
            "(76, 80): reprojection: 0.621526, disparity: 0.083666\n",
            "(76, 84): reprojection: 0.890945, disparity: 0.112215\n",
            "(77, 78): reprojection: 0.273491, disparity: 0.061968\n",
            "(77, 79): reprojection: 0.320709, disparity: 0.071184\n",
            "(78, 79): reprojection: 0.214564, disparity: 0.065255\n",
            "(78, 80): reprojection: 0.290630, disparity: 0.076117\n",
            "(78, 82): reprojection: 0.454516, disparity: 0.095860\n",
            "(79, 80): reprojection: 0.169404, disparity: 0.062382\n",
            "(79, 81): reprojection: 0.306312, disparity: 0.070325\n",
            "(80, 81): reprojection: 0.181747, disparity: 0.061157\n",
            "(80, 82): reprojection: 0.321701, disparity: 0.078171\n",
            "(80, 84): reprojection: 0.480119, disparity: 0.085488\n",
            "(80, 88): reprojection: 0.860629, disparity: 0.114557\n",
            "(81, 82): reprojection: 0.193547, disparity: 0.061840\n",
            "(81, 83): reprojection: 0.286021, disparity: 0.072267\n",
            "(82, 83): reprojection: 0.150782, disparity: 0.061388\n",
            "(82, 84): reprojection: 0.275114, disparity: 0.069434\n",
            "(82, 86): reprojection: 0.467424, disparity: 0.086547\n",
            "(83, 84): reprojection: 0.146941, disparity: 0.060302\n",
            "(83, 85): reprojection: 0.317150, disparity: 0.072930\n",
            "(84, 85): reprojection: 0.189476, disparity: 0.059674\n",
            "(84, 86): reprojection: 0.283802, disparity: 0.068054\n",
            "(84, 88): reprojection: 0.544410, disparity: 0.082722\n",
            "(85, 86): reprojection: 0.177039, disparity: 0.057439\n",
            "(85, 87): reprojection: 0.302209, disparity: 0.067970\n",
            "(86, 87): reprojection: 0.241139, disparity: 0.062692\n",
            "(86, 88): reprojection: 0.346716, disparity: 0.073333\n",
            "(86, 90): reprojection: 0.520913, disparity: 0.082530\n",
            "(87, 88): reprojection: 0.185045, disparity: 0.061085\n",
            "(87, 89): reprojection: 0.296463, disparity: 0.070199\n",
            "(88, 89): reprojection: 0.151408, disparity: 0.056107\n",
            "(88, 90): reprojection: 0.313933, disparity: 0.061423\n",
            "(89, 90): reprojection: 0.214662, disparity: 0.054780\n",
            "(89, 91): reprojection: 0.435455, disparity: 0.068285\n",
            "(90, 91): reprojection: 0.334380, disparity: 0.059956\n",
            "Mean:     reprojection: 0.334380, disparity: 0.059956\n",
            "Done Validation for epoch 12 (3120 iterations)\n",
            "Epoch = 12, pairs = [[1, 2], [13, 15], [3, 4], [3, 5]], loss = 0.2639148533344269\n",
            "Epoch = 12, pairs = [[85, 87], [78, 79], [28, 32], [64, 72]], loss = 0.6852914094924927\n",
            "Epoch = 12, pairs = [[50, 52], [67, 69], [78, 82], [76, 80]], loss = 0.549119234085083\n",
            "Epoch = 12, pairs = [[4, 12], [25, 26], [8, 9], [64, 68]], loss = 0.49372950196266174\n",
            "Epoch = 12, pairs = [[44, 52], [65, 67], [20, 21], [46, 50]], loss = 0.6759436726570129\n",
            "Epoch = 12, pairs = [[40, 48], [29, 31], [49, 50], [53, 54]], loss = 0.5139493942260742\n",
            "Epoch = 12, pairs = [[29, 30], [82, 86], [5, 6], [33, 34]], loss = 0.27147966623306274\n",
            "Epoch = 12, pairs = [[32, 40], [52, 60], [73, 74], [22, 24]], loss = 0.6329010725021362\n",
            "Epoch = 12, pairs = [[62, 63], [0, 8], [56, 57], [18, 22]], loss = 0.5104992389678955\n",
            "Epoch = 12, pairs = [[36, 37], [76, 84], [6, 8], [72, 74]], loss = 0.5013260841369629\n",
            "Epoch = 12, pairs = [[10, 14], [24, 25], [20, 24], [84, 88]], loss = 0.4922850728034973\n",
            "Epoch = 12, pairs = [[50, 54], [80, 82], [67, 68], [27, 29]], loss = 0.4646698832511902\n",
            "Epoch = 12, pairs = [[20, 22], [14, 18], [59, 61], [34, 36]], loss = 0.37959110736846924\n",
            "Epoch = 12, pairs = [[88, 90], [90, 91], [37, 39], [35, 36]], loss = 0.349092572927475\n",
            "Epoch = 12, pairs = [[16, 24], [58, 59], [77, 78], [85, 86]], loss = 0.4610520899295807\n",
            "Epoch = 12, pairs = [[9, 10], [37, 38], [51, 52], [48, 56]], loss = 0.44647884368896484\n",
            "Epoch = 12, pairs = [[48, 64], [57, 58], [71, 73], [60, 61]], loss = 0.5946249961853027\n",
            "Epoch = 12, pairs = [[60, 62], [8, 16], [61, 62], [58, 60]], loss = 0.47219160199165344\n",
            "Epoch = 12, pairs = [[72, 76], [75, 76], [30, 31], [64, 80]], loss = 0.5902839303016663\n",
            "Epoch = 12, pairs = [[60, 64], [47, 49], [42, 43], [12, 13]], loss = 0.35668647289276123\n",
            "Epoch = 12, pairs = [[84, 86], [68, 70], [16, 48], [4, 6]], loss = 0.6400542259216309\n",
            "Epoch = 12, pairs = [[73, 75], [52, 56], [48, 49], [44, 45]], loss = 0.3829185366630554\n",
            "Epoch = 12, pairs = [[28, 36], [36, 38], [16, 32], [12, 20]], loss = 0.8458013534545898\n",
            "Epoch = 12, pairs = [[46, 48], [56, 64], [33, 35], [84, 85]], loss = 0.5234857797622681\n",
            "Epoch = 12, pairs = [[78, 80], [62, 66], [80, 84], [86, 87]], loss = 0.5438992977142334\n",
            "Epoch = 12, pairs = [[11, 13], [72, 88], [0, 4], [28, 29]], loss = 0.5348716378211975\n",
            "Epoch = 12, pairs = [[80, 81], [2, 3], [56, 60], [32, 34]], loss = 0.37526798248291016\n",
            "Epoch = 12, pairs = [[75, 77], [62, 64], [70, 71], [38, 39]], loss = 0.35887977480888367\n",
            "Epoch = 12, pairs = [[63, 65], [24, 26], [1, 3], [16, 18]], loss = 0.3677305579185486\n",
            "Epoch = 12, pairs = [[42, 44], [0, 2], [44, 46], [39, 41]], loss = 0.47845783829689026\n",
            "Epoch = 12, pairs = [[61, 63], [6, 10], [32, 33], [59, 60]], loss = 0.37566643953323364\n",
            "Epoch = 12, pairs = [[46, 47], [22, 26], [12, 16], [53, 55]], loss = 0.4853684902191162\n",
            "Epoch = 12, pairs = [[87, 89], [22, 23], [45, 46], [45, 47]], loss = 0.3163883686065674\n",
            "Epoch = 12, pairs = [[25, 27], [48, 80], [15, 17], [5, 7]], loss = 0.909633219242096\n",
            "Epoch = 12, pairs = [[26, 27], [23, 24], [63, 64], [48, 50]], loss = 0.24054884910583496\n",
            "Epoch = 12, pairs = [[0, 32], [23, 25], [28, 30], [83, 84]], loss = 0.641114354133606\n",
            "Epoch = 12, pairs = [[81, 83], [56, 58], [60, 68], [24, 28]], loss = 0.634425163269043\n",
            "Epoch = 12, pairs = [[7, 9], [72, 73], [55, 57], [70, 74]], loss = 0.4096509516239166\n",
            "Epoch = 12, pairs = [[9, 11], [31, 32], [74, 76], [88, 89]], loss = 0.3018931746482849\n",
            "Epoch = 12, pairs = [[79, 81], [82, 83], [80, 88], [8, 12]], loss = 0.5130794644355774\n",
            "Epoch = 12, pairs = [[51, 53], [76, 77], [43, 45], [0, 16]], loss = 0.5345944166183472\n",
            "Epoch = 12, pairs = [[8, 24], [71, 72], [31, 33], [58, 62]], loss = 0.6125749349594116\n",
            "Epoch = 12, pairs = [[38, 42], [76, 78], [0, 1], [19, 21]], loss = 0.4790811538696289\n",
            "Epoch = 12, pairs = [[68, 76], [52, 53], [16, 17], [21, 23]], loss = 0.42570990324020386\n",
            "Epoch = 12, pairs = [[18, 19], [26, 28], [64, 66], [6, 7]], loss = 0.2931166887283325\n",
            "Epoch = 12, pairs = [[86, 88], [56, 72], [32, 36], [21, 22]], loss = 0.5802134275436401\n",
            "Epoch = 12, pairs = [[57, 59], [74, 78], [8, 10], [2, 4]], loss = 0.41528329253196716\n",
            "Epoch = 12, pairs = [[34, 38], [2, 6], [66, 67], [24, 32]], loss = 0.44658929109573364\n",
            "Epoch = 12, pairs = [[38, 40], [77, 79], [41, 43], [81, 82]], loss = 0.3585966229438782\n",
            "Epoch = 12, pairs = [[43, 44], [42, 46], [86, 90], [50, 51]], loss = 0.43412479758262634\n",
            "Epoch = 12, pairs = [[54, 55], [18, 20], [20, 28], [69, 70]], loss = 0.4759538471698761\n",
            "Epoch = 12, pairs = [[14, 15], [26, 30], [35, 37], [40, 41]], loss = 0.32912635803222656\n",
            "Epoch = 12, pairs = [[69, 71], [15, 16], [39, 40], [48, 52]], loss = 0.41063982248306274\n",
            "Epoch = 12, pairs = [[27, 28], [36, 40], [68, 72], [66, 70]], loss = 0.4631101191043854\n",
            "Epoch = 12, pairs = [[82, 84], [30, 32], [49, 51], [40, 56]], loss = 0.5415139198303223\n",
            "Epoch = 12, pairs = [[13, 14], [64, 65], [89, 91], [17, 19]], loss = 0.39591264724731445\n",
            "Epoch = 12, pairs = [[83, 85], [54, 56], [7, 8], [41, 42]], loss = 0.34369564056396484\n",
            "Epoch = 12, pairs = [[44, 48], [72, 80], [47, 48], [68, 69]], loss = 0.4334191679954529\n",
            "Epoch = 12, pairs = [[32, 48], [12, 14], [4, 8], [79, 80]], loss = 0.5193468332290649\n",
            "Epoch = 12, pairs = [[32, 64], [40, 42], [36, 44], [4, 5]], loss = 0.8082184791564941\n",
            "Epoch = 12, pairs = [[24, 40], [19, 20], [10, 11], [87, 88]], loss = 0.3932625353336334\n",
            "Epoch = 12, pairs = [[52, 54], [65, 66], [34, 35], [11, 12]], loss = 0.2716861963272095\n",
            "Epoch = 12, pairs = [[66, 68], [10, 12], [14, 16], [54, 58]], loss = 0.4088521897792816\n",
            "Epoch = 12, pairs = [[30, 34], [74, 75], [17, 18], [70, 72]], loss = 0.3938329517841339\n",
            "Epoch = 12, pairs = [[16, 20], [40, 44], [89, 90], [55, 56]], loss = 0.48719629645347595\n",
            "Epoch 12 took 84.14s.\n",
            "( 0,  1): reprojection: 0.234181, disparity: 0.055638\n",
            "( 0,  2): reprojection: 0.352165, disparity: 0.056527\n",
            "( 0,  4): reprojection: 0.401029, disparity: 0.066699\n",
            "( 0,  8): reprojection: 0.536118, disparity: 0.081043\n",
            "( 0, 16): reprojection: 0.901897, disparity: 0.111885\n",
            "( 0, 32): reprojection: 1.474695, disparity: 0.146596\n",
            "( 1,  2): reprojection: 0.197533, disparity: 0.046097\n",
            "( 1,  3): reprojection: 0.255717, disparity: 0.049239\n",
            "( 2,  3): reprojection: 0.111777, disparity: 0.044523\n",
            "( 2,  4): reprojection: 0.185421, disparity: 0.050646\n",
            "( 2,  6): reprojection: 0.358105, disparity: 0.058265\n",
            "( 3,  4): reprojection: 0.112582, disparity: 0.046240\n",
            "( 3,  5): reprojection: 0.206682, disparity: 0.051191\n",
            "( 4,  5): reprojection: 0.155941, disparity: 0.047876\n",
            "( 4,  6): reprojection: 0.222942, disparity: 0.048325\n",
            "( 4,  8): reprojection: 0.303351, disparity: 0.059444\n",
            "( 4, 12): reprojection: 0.380824, disparity: 0.075581\n",
            "( 5,  6): reprojection: 0.118174, disparity: 0.046399\n",
            "( 5,  7): reprojection: 0.172803, disparity: 0.048858\n",
            "( 6,  7): reprojection: 0.102757, disparity: 0.046140\n",
            "( 6,  8): reprojection: 0.175290, disparity: 0.052714\n",
            "( 6, 10): reprojection: 0.400705, disparity: 0.061567\n",
            "( 7,  8): reprojection: 0.096339, disparity: 0.045490\n",
            "( 7,  9): reprojection: 0.203223, disparity: 0.050278\n",
            "( 8,  9): reprojection: 0.161951, disparity: 0.048164\n",
            "( 8, 10): reprojection: 0.360186, disparity: 0.054212\n",
            "( 8, 12): reprojection: 0.472972, disparity: 0.063645\n",
            "( 8, 16): reprojection: 0.641684, disparity: 0.090056\n",
            "( 8, 24): reprojection: 0.839728, disparity: 0.108751\n",
            "( 9, 10): reprojection: 0.243985, disparity: 0.048211\n",
            "( 9, 11): reprojection: 0.363070, disparity: 0.055475\n",
            "(10, 11): reprojection: 0.155475, disparity: 0.043755\n",
            "(10, 12): reprojection: 0.193742, disparity: 0.050983\n",
            "(10, 14): reprojection: 0.323993, disparity: 0.065160\n",
            "(11, 12): reprojection: 0.134293, disparity: 0.043471\n",
            "(11, 13): reprojection: 0.218669, disparity: 0.055790\n",
            "(12, 13): reprojection: 0.131818, disparity: 0.044754\n",
            "(12, 14): reprojection: 0.255695, disparity: 0.051169\n",
            "(12, 16): reprojection: 0.475793, disparity: 0.074730\n",
            "(12, 20): reprojection: 0.740849, disparity: 0.078009\n",
            "(13, 14): reprojection: 0.180152, disparity: 0.045223\n",
            "(13, 15): reprojection: 0.248484, disparity: 0.053752\n",
            "(14, 15): reprojection: 0.202783, disparity: 0.046403\n",
            "(14, 16): reprojection: 0.447184, disparity: 0.057569\n",
            "(14, 18): reprojection: 0.393451, disparity: 0.067341\n",
            "(15, 16): reprojection: 0.304505, disparity: 0.048632\n",
            "(15, 17): reprojection: 0.392004, disparity: 0.058778\n",
            "(16, 17): reprojection: 0.146972, disparity: 0.046904\n",
            "(16, 18): reprojection: 0.310095, disparity: 0.056352\n",
            "(16, 20): reprojection: 0.560964, disparity: 0.064240\n",
            "(16, 24): reprojection: 0.821435, disparity: 0.077791\n",
            "(16, 32): reprojection: 1.497149, disparity: 0.112690\n",
            "(16, 48): reprojection: 1.345548, disparity: 0.252690\n",
            "(17, 18): reprojection: 0.296986, disparity: 0.048569\n",
            "(17, 19): reprojection: 0.465717, disparity: 0.059034\n",
            "(18, 19): reprojection: 0.215771, disparity: 0.047878\n",
            "(18, 20): reprojection: 0.315396, disparity: 0.054997\n",
            "(18, 22): reprojection: 0.493310, disparity: 0.061783\n",
            "(19, 20): reprojection: 0.135095, disparity: 0.046747\n",
            "(19, 21): reprojection: 0.265806, disparity: 0.055528\n",
            "(20, 21): reprojection: 0.180677, disparity: 0.049669\n",
            "(20, 22): reprojection: 0.274968, disparity: 0.057959\n",
            "(20, 24): reprojection: 0.400822, disparity: 0.068911\n",
            "(20, 28): reprojection: 0.740252, disparity: 0.092726\n",
            "(21, 22): reprojection: 0.155771, disparity: 0.049134\n",
            "(21, 23): reprojection: 0.241350, disparity: 0.057432\n",
            "(22, 23): reprojection: 0.130631, disparity: 0.050059\n",
            "(22, 24): reprojection: 0.211916, disparity: 0.058855\n",
            "(22, 26): reprojection: 0.388916, disparity: 0.069707\n",
            "(23, 24): reprojection: 0.123746, disparity: 0.050039\n",
            "(23, 25): reprojection: 0.197721, disparity: 0.056184\n",
            "(24, 25): reprojection: 0.133400, disparity: 0.051742\n",
            "(24, 26): reprojection: 0.202059, disparity: 0.059658\n",
            "(24, 28): reprojection: 0.420154, disparity: 0.067589\n",
            "(24, 32): reprojection: 0.857649, disparity: 0.090436\n",
            "(24, 40): reprojection: 1.208282, disparity: 0.158420\n",
            "(25, 26): reprojection: 0.144183, disparity: 0.054563\n",
            "(25, 27): reprojection: 0.261074, disparity: 0.061811\n",
            "(26, 27): reprojection: 0.157404, disparity: 0.055882\n",
            "(26, 28): reprojection: 0.225795, disparity: 0.058981\n",
            "(26, 30): reprojection: 0.391164, disparity: 0.074108\n",
            "(27, 28): reprojection: 0.133492, disparity: 0.053873\n",
            "(27, 29): reprojection: 0.252349, disparity: 0.059180\n",
            "(28, 29): reprojection: 0.170507, disparity: 0.055210\n",
            "(28, 30): reprojection: 0.251481, disparity: 0.062012\n",
            "(28, 32): reprojection: 0.499140, disparity: 0.075012\n",
            "(28, 36): reprojection: 0.750544, disparity: 0.090654\n",
            "(29, 30): reprojection: 0.138218, disparity: 0.050943\n",
            "(29, 31): reprojection: 0.304126, disparity: 0.055883\n",
            "(30, 31): reprojection: 0.215528, disparity: 0.052164\n",
            "(30, 32): reprojection: 0.300792, disparity: 0.060935\n",
            "(30, 34): reprojection: 0.516107, disparity: 0.076385\n",
            "(31, 32): reprojection: 0.134629, disparity: 0.058437\n",
            "(31, 33): reprojection: 0.338828, disparity: 0.059202\n",
            "(32, 33): reprojection: 0.249898, disparity: 0.057965\n",
            "(32, 34): reprojection: 0.281503, disparity: 0.069376\n",
            "(32, 36): reprojection: 0.419596, disparity: 0.079676\n",
            "(32, 40): reprojection: 0.457928, disparity: 0.212377\n",
            "(32, 48): reprojection: 1.057770, disparity: 0.312588\n",
            "(32, 64): reprojection: 1.414235, disparity: 0.352526\n",
            "(33, 34): reprojection: 0.142340, disparity: 0.057894\n",
            "(33, 35): reprojection: 0.266856, disparity: 0.066567\n",
            "(34, 35): reprojection: 0.183822, disparity: 0.052318\n",
            "(34, 36): reprojection: 0.275677, disparity: 0.063086\n",
            "(34, 38): reprojection: 0.420554, disparity: 0.094964\n",
            "(35, 36): reprojection: 0.226682, disparity: 0.055274\n",
            "(35, 37): reprojection: 0.291593, disparity: 0.092174\n",
            "(36, 37): reprojection: 0.276834, disparity: 0.069837\n",
            "(36, 38): reprojection: 0.416521, disparity: 0.076511\n",
            "(36, 40): reprojection: 0.340964, disparity: 0.128387\n",
            "(36, 44): reprojection: 0.679032, disparity: 0.209775\n",
            "(37, 38): reprojection: 0.224720, disparity: 0.055471\n",
            "(37, 39): reprojection: 0.249739, disparity: 0.069356\n",
            "(38, 39): reprojection: 0.181491, disparity: 0.057750\n",
            "(38, 40): reprojection: 0.349730, disparity: 0.092291\n",
            "(38, 42): reprojection: 0.617273, disparity: 0.137144\n",
            "(39, 40): reprojection: 0.270032, disparity: 0.071904\n",
            "(39, 41): reprojection: 0.466460, disparity: 0.094792\n",
            "(40, 41): reprojection: 0.274393, disparity: 0.063928\n",
            "(40, 42): reprojection: 0.413814, disparity: 0.081636\n",
            "(40, 44): reprojection: 0.527272, disparity: 0.118647\n",
            "(40, 48): reprojection: 0.789514, disparity: 0.195280\n",
            "(40, 56): reprojection: 0.919141, disparity: 0.298113\n",
            "(41, 42): reprojection: 0.200993, disparity: 0.060919\n",
            "(41, 43): reprojection: 0.303004, disparity: 0.079258\n",
            "(42, 43): reprojection: 0.173068, disparity: 0.063916\n",
            "(42, 44): reprojection: 0.318091, disparity: 0.084875\n",
            "(42, 46): reprojection: 0.533056, disparity: 0.131930\n",
            "(43, 44): reprojection: 0.197052, disparity: 0.068563\n",
            "(43, 45): reprojection: 0.307057, disparity: 0.086871\n",
            "(44, 45): reprojection: 0.150313, disparity: 0.059929\n",
            "(44, 46): reprojection: 0.292133, disparity: 0.091958\n",
            "(44, 48): reprojection: 0.472238, disparity: 0.131935\n",
            "(44, 52): reprojection: 0.728216, disparity: 0.184583\n",
            "(45, 46): reprojection: 0.205330, disparity: 0.073514\n",
            "(45, 47): reprojection: 0.351984, disparity: 0.144206\n",
            "(46, 47): reprojection: 0.190390, disparity: 0.066271\n",
            "(46, 48): reprojection: 0.308231, disparity: 0.083378\n",
            "(46, 50): reprojection: 0.432371, disparity: 0.105884\n",
            "(47, 48): reprojection: 0.211368, disparity: 0.062086\n",
            "(47, 49): reprojection: 0.277212, disparity: 0.077312\n",
            "(48, 49): reprojection: 0.169431, disparity: 0.069596\n",
            "(48, 50): reprojection: 0.257169, disparity: 0.079283\n",
            "(48, 52): reprojection: 0.402564, disparity: 0.107511\n",
            "(48, 56): reprojection: 0.785522, disparity: 0.163519\n",
            "(48, 64): reprojection: 1.055322, disparity: 0.149939\n",
            "(48, 80): reprojection: 1.320349, disparity: 0.118934\n",
            "(49, 50): reprojection: 0.143307, disparity: 0.063968\n",
            "(49, 51): reprojection: 0.251551, disparity: 0.075104\n",
            "(50, 51): reprojection: 0.166607, disparity: 0.063139\n",
            "(50, 52): reprojection: 0.271103, disparity: 0.073839\n",
            "(50, 54): reprojection: 0.394903, disparity: 0.104383\n",
            "(51, 52): reprojection: 0.164764, disparity: 0.063080\n",
            "(51, 53): reprojection: 0.300438, disparity: 0.075029\n",
            "(52, 53): reprojection: 0.174385, disparity: 0.061773\n",
            "(52, 54): reprojection: 0.248298, disparity: 0.081055\n",
            "(52, 56): reprojection: 0.575030, disparity: 0.107938\n",
            "(52, 60): reprojection: 0.700604, disparity: 0.124457\n",
            "(53, 54): reprojection: 0.124028, disparity: 0.069838\n",
            "(53, 55): reprojection: 0.350403, disparity: 0.112436\n",
            "(54, 55): reprojection: 0.302929, disparity: 0.060452\n",
            "(54, 56): reprojection: 0.572419, disparity: 0.076455\n",
            "(54, 58): reprojection: 0.419107, disparity: 0.090265\n",
            "(55, 56): reprojection: 0.342730, disparity: 0.067286\n",
            "(55, 57): reprojection: 0.393986, disparity: 0.069524\n",
            "(56, 57): reprojection: 0.297785, disparity: 0.063491\n",
            "(56, 58): reprojection: 0.534450, disparity: 0.077375\n",
            "(56, 60): reprojection: 0.716581, disparity: 0.092964\n",
            "(56, 64): reprojection: 0.882201, disparity: 0.093517\n",
            "(56, 72): reprojection: 1.054520, disparity: 0.151790\n",
            "(57, 58): reprojection: 0.343326, disparity: 0.068027\n",
            "(57, 59): reprojection: 0.470016, disparity: 0.075767\n",
            "(58, 59): reprojection: 0.244540, disparity: 0.072132\n",
            "(58, 60): reprojection: 0.394570, disparity: 0.095911\n",
            "(58, 62): reprojection: 0.514895, disparity: 0.099196\n",
            "(59, 60): reprojection: 0.190948, disparity: 0.072733\n",
            "(59, 61): reprojection: 0.318030, disparity: 0.080757\n",
            "(60, 61): reprojection: 0.231889, disparity: 0.075682\n",
            "(60, 62): reprojection: 0.327166, disparity: 0.084052\n",
            "(60, 64): reprojection: 0.504302, disparity: 0.087380\n",
            "(60, 68): reprojection: 0.630080, disparity: 0.117402\n",
            "(61, 62): reprojection: 0.208519, disparity: 0.074498\n",
            "(61, 63): reprojection: 0.286122, disparity: 0.077038\n",
            "(62, 63): reprojection: 0.213178, disparity: 0.068167\n",
            "(62, 64): reprojection: 0.302512, disparity: 0.073263\n",
            "(62, 66): reprojection: 0.540216, disparity: 0.121021\n",
            "(63, 64): reprojection: 0.232400, disparity: 0.072832\n",
            "(63, 65): reprojection: 0.304367, disparity: 0.071608\n",
            "(64, 65): reprojection: 0.205129, disparity: 0.076249\n",
            "(64, 66): reprojection: 0.346816, disparity: 0.074200\n",
            "(64, 68): reprojection: 0.431830, disparity: 0.092198\n",
            "(64, 72): reprojection: 0.655152, disparity: 0.118887\n",
            "(64, 80): reprojection: 0.903360, disparity: 0.115362\n",
            "(65, 66): reprojection: 0.264927, disparity: 0.063612\n",
            "(65, 67): reprojection: 0.391861, disparity: 0.069262\n",
            "(66, 67): reprojection: 0.238715, disparity: 0.067765\n",
            "(66, 68): reprojection: 0.333493, disparity: 0.070394\n",
            "(66, 70): reprojection: 0.449179, disparity: 0.092755\n",
            "(67, 68): reprojection: 0.319485, disparity: 0.065262\n",
            "(67, 69): reprojection: 0.350221, disparity: 0.070357\n",
            "(68, 69): reprojection: 0.212704, disparity: 0.063598\n",
            "(68, 70): reprojection: 0.456967, disparity: 0.074454\n",
            "(68, 72): reprojection: 0.529675, disparity: 0.097198\n",
            "(68, 76): reprojection: 0.688446, disparity: 0.104876\n",
            "(69, 70): reprojection: 0.312151, disparity: 0.068745\n",
            "(69, 71): reprojection: 0.447948, disparity: 0.071555\n",
            "(70, 71): reprojection: 0.196416, disparity: 0.065600\n",
            "(70, 72): reprojection: 0.278274, disparity: 0.069047\n",
            "(70, 74): reprojection: 0.394134, disparity: 0.078527\n",
            "(71, 72): reprojection: 0.230885, disparity: 0.066669\n",
            "(71, 73): reprojection: 0.355850, disparity: 0.072951\n",
            "(72, 73): reprojection: 0.219818, disparity: 0.064049\n",
            "(72, 74): reprojection: 0.292769, disparity: 0.068744\n",
            "(72, 76): reprojection: 0.437800, disparity: 0.073040\n",
            "(72, 80): reprojection: 0.600599, disparity: 0.100297\n",
            "(72, 88): reprojection: 0.930177, disparity: 0.117951\n",
            "(73, 74): reprojection: 0.312902, disparity: 0.063168\n",
            "(73, 75): reprojection: 0.439391, disparity: 0.069810\n",
            "(74, 75): reprojection: 0.241526, disparity: 0.062884\n",
            "(74, 76): reprojection: 0.255090, disparity: 0.066675\n",
            "(74, 78): reprojection: 0.390932, disparity: 0.075531\n",
            "(75, 76): reprojection: 0.217948, disparity: 0.062554\n",
            "(75, 77): reprojection: 0.341557, disparity: 0.068536\n",
            "(76, 77): reprojection: 0.221834, disparity: 0.060468\n",
            "(76, 78): reprojection: 0.327110, disparity: 0.065457\n",
            "(76, 80): reprojection: 0.430603, disparity: 0.078472\n",
            "(76, 84): reprojection: 0.708279, disparity: 0.089307\n",
            "(77, 78): reprojection: 0.216051, disparity: 0.061230\n",
            "(77, 79): reprojection: 0.259879, disparity: 0.067493\n",
            "(78, 79): reprojection: 0.203390, disparity: 0.062826\n",
            "(78, 80): reprojection: 0.255916, disparity: 0.069690\n",
            "(78, 82): reprojection: 0.418385, disparity: 0.078721\n",
            "(79, 80): reprojection: 0.142521, disparity: 0.062617\n",
            "(79, 81): reprojection: 0.235948, disparity: 0.065920\n",
            "(80, 81): reprojection: 0.145695, disparity: 0.060152\n",
            "(80, 82): reprojection: 0.280972, disparity: 0.062216\n",
            "(80, 84): reprojection: 0.511134, disparity: 0.071642\n",
            "(80, 88): reprojection: 0.754248, disparity: 0.093537\n",
            "(81, 82): reprojection: 0.195711, disparity: 0.058680\n",
            "(81, 83): reprojection: 0.325015, disparity: 0.064852\n",
            "(82, 83): reprojection: 0.156460, disparity: 0.056853\n",
            "(82, 84): reprojection: 0.278865, disparity: 0.062475\n",
            "(82, 86): reprojection: 0.461390, disparity: 0.073915\n",
            "(83, 84): reprojection: 0.155518, disparity: 0.057897\n",
            "(83, 85): reprojection: 0.314195, disparity: 0.066461\n",
            "(84, 85): reprojection: 0.173286, disparity: 0.055349\n",
            "(84, 86): reprojection: 0.257583, disparity: 0.064613\n",
            "(84, 88): reprojection: 0.484237, disparity: 0.073141\n",
            "(85, 86): reprojection: 0.164371, disparity: 0.056568\n",
            "(85, 87): reprojection: 0.278882, disparity: 0.060714\n",
            "(86, 87): reprojection: 0.234182, disparity: 0.058510\n",
            "(86, 88): reprojection: 0.329000, disparity: 0.068296\n",
            "(86, 90): reprojection: 0.498205, disparity: 0.071819\n",
            "(87, 88): reprojection: 0.175268, disparity: 0.059224\n",
            "(87, 89): reprojection: 0.281762, disparity: 0.066949\n",
            "(88, 89): reprojection: 0.143885, disparity: 0.054762\n",
            "(88, 90): reprojection: 0.312104, disparity: 0.059336\n",
            "(89, 90): reprojection: 0.213301, disparity: 0.051424\n",
            "(89, 91): reprojection: 0.410256, disparity: 0.064539\n",
            "(90, 91): reprojection: 0.326596, disparity: 0.059105\n",
            "Mean:     reprojection: 0.326596, disparity: 0.059105\n",
            "Done Validation for epoch 13 (3380 iterations)\n",
            "Epoch = 13, pairs = [[11, 13], [14, 16], [7, 8], [68, 70]], loss = 0.36709117889404297\n",
            "Epoch = 13, pairs = [[77, 79], [73, 74], [33, 34], [40, 42]], loss = 0.3457418382167816\n",
            "Epoch = 13, pairs = [[50, 51], [28, 36], [69, 70], [32, 40]], loss = 0.5957647562026978\n",
            "Epoch = 13, pairs = [[81, 83], [17, 19], [19, 20], [48, 50]], loss = 0.3491741716861725\n",
            "Epoch = 13, pairs = [[83, 85], [50, 52], [52, 56], [5, 7]], loss = 0.40079110860824585\n",
            "Epoch = 13, pairs = [[11, 12], [30, 31], [86, 88], [56, 60]], loss = 0.4069555699825287\n",
            "Epoch = 13, pairs = [[64, 65], [9, 11], [0, 16], [10, 14]], loss = 0.4962298274040222\n",
            "Epoch = 13, pairs = [[43, 45], [72, 76], [71, 73], [22, 24]], loss = 0.36198127269744873\n",
            "Epoch = 13, pairs = [[16, 20], [72, 88], [43, 44], [40, 41]], loss = 0.5324236154556274\n",
            "Epoch = 13, pairs = [[76, 80], [12, 20], [83, 84], [74, 78]], loss = 0.4496004581451416\n",
            "Epoch = 13, pairs = [[72, 74], [20, 24], [65, 67], [79, 80]], loss = 0.3709440231323242\n",
            "Epoch = 13, pairs = [[32, 36], [51, 52], [58, 62], [56, 72]], loss = 0.581260085105896\n",
            "Epoch = 13, pairs = [[57, 59], [31, 33], [59, 60], [21, 22]], loss = 0.343760222196579\n",
            "Epoch = 13, pairs = [[52, 60], [15, 17], [59, 61], [66, 70]], loss = 0.49761033058166504\n",
            "Epoch = 13, pairs = [[30, 32], [32, 64], [69, 71], [90, 91]], loss = 0.6568496227264404\n",
            "Epoch = 13, pairs = [[1, 3], [76, 78], [24, 32], [78, 79]], loss = 0.4017108678817749\n",
            "Epoch = 13, pairs = [[4, 8], [25, 26], [48, 49], [15, 16]], loss = 0.27497291564941406\n",
            "Epoch = 13, pairs = [[48, 52], [8, 10], [49, 50], [60, 62]], loss = 0.35546112060546875\n",
            "Epoch = 13, pairs = [[70, 72], [86, 87], [26, 28], [8, 9]], loss = 0.26378846168518066\n",
            "Epoch = 13, pairs = [[47, 49], [46, 48], [8, 12], [76, 84]], loss = 0.4754951298236847\n",
            "Epoch = 13, pairs = [[17, 18], [36, 40], [80, 82], [54, 56]], loss = 0.43172597885131836\n",
            "Epoch = 13, pairs = [[60, 64], [65, 66], [12, 14], [20, 28]], loss = 0.46121010184288025\n",
            "Epoch = 13, pairs = [[64, 80], [24, 25], [10, 11], [66, 68]], loss = 0.5330168008804321\n",
            "Epoch = 13, pairs = [[67, 68], [22, 23], [61, 62], [81, 82]], loss = 0.27615588903427124\n",
            "Epoch = 13, pairs = [[64, 66], [71, 72], [82, 86], [35, 37]], loss = 0.39625996351242065\n",
            "Epoch = 13, pairs = [[16, 17], [40, 56], [56, 57], [16, 32]], loss = 0.6968661546707153\n",
            "Epoch = 13, pairs = [[62, 63], [40, 44], [46, 50], [57, 58]], loss = 0.5328370332717896\n",
            "Epoch = 13, pairs = [[89, 91], [12, 16], [30, 34], [74, 76]], loss = 0.4337270259857178\n",
            "Epoch = 13, pairs = [[27, 29], [75, 76], [42, 43], [32, 34]], loss = 0.28188762068748474\n",
            "Epoch = 13, pairs = [[42, 46], [8, 16], [64, 72], [38, 42]], loss = 0.7397329211235046\n",
            "Epoch = 13, pairs = [[27, 28], [66, 67], [84, 85], [33, 35]], loss = 0.2629135251045227\n",
            "Epoch = 13, pairs = [[80, 88], [20, 22], [19, 21], [88, 90]], loss = 0.46313685178756714\n",
            "Epoch = 13, pairs = [[54, 58], [87, 88], [48, 56], [68, 69]], loss = 0.4466051161289215\n",
            "Epoch = 13, pairs = [[16, 18], [28, 32], [38, 39], [54, 55]], loss = 0.37563401460647583\n",
            "Epoch = 13, pairs = [[6, 8], [16, 48], [23, 25], [63, 64]], loss = 0.5727932453155518\n",
            "Epoch = 13, pairs = [[0, 2], [73, 75], [32, 48], [70, 71]], loss = 0.5388246178627014\n",
            "Epoch = 13, pairs = [[41, 43], [34, 35], [77, 78], [20, 21]], loss = 0.2823421359062195\n",
            "Epoch = 13, pairs = [[68, 72], [44, 46], [36, 38], [16, 24]], loss = 0.5704660415649414\n",
            "Epoch = 13, pairs = [[72, 73], [35, 36], [68, 76], [41, 42]], loss = 0.4019550383090973\n",
            "Epoch = 13, pairs = [[4, 12], [26, 27], [60, 61], [0, 4]], loss = 0.3653753399848938\n",
            "Epoch = 13, pairs = [[64, 68], [10, 12], [31, 32], [12, 13]], loss = 0.30433645844459534\n",
            "Epoch = 13, pairs = [[3, 4], [34, 36], [56, 64], [36, 44]], loss = 0.5670356154441833\n",
            "Epoch = 13, pairs = [[67, 69], [24, 40], [56, 58], [84, 88]], loss = 0.7067734003067017\n",
            "Epoch = 13, pairs = [[13, 14], [23, 24], [44, 48], [51, 53]], loss = 0.3098772168159485\n",
            "Epoch = 13, pairs = [[2, 6], [40, 48], [89, 90], [79, 81]], loss = 0.45205092430114746\n",
            "Epoch = 13, pairs = [[70, 74], [55, 56], [62, 66], [53, 55]], loss = 0.45273882150650024\n",
            "Epoch = 13, pairs = [[37, 38], [14, 15], [0, 8], [80, 81]], loss = 0.31426623463630676\n",
            "Epoch = 13, pairs = [[13, 15], [62, 64], [0, 32], [0, 1]], loss = 0.4939534366130829\n",
            "Epoch = 13, pairs = [[74, 75], [85, 87], [44, 45], [28, 30]], loss = 0.2885960340499878\n",
            "Epoch = 13, pairs = [[14, 18], [39, 41], [6, 10], [52, 53]], loss = 0.41010743379592896\n",
            "Epoch = 13, pairs = [[78, 80], [48, 64], [87, 89], [28, 29]], loss = 0.6110947728157043\n",
            "Epoch = 13, pairs = [[75, 77], [8, 24], [25, 27], [24, 26]], loss = 0.4739108681678772\n",
            "Epoch = 13, pairs = [[29, 31], [63, 65], [53, 54], [47, 48]], loss = 0.29945093393325806\n",
            "Epoch = 13, pairs = [[2, 3], [46, 47], [38, 40], [52, 54]], loss = 0.2899966239929199\n",
            "Epoch = 13, pairs = [[24, 28], [7, 9], [55, 57], [3, 5]], loss = 0.3776945471763611\n",
            "Epoch = 13, pairs = [[5, 6], [36, 37], [60, 68], [86, 90]], loss = 0.5106181502342224\n",
            "Epoch = 13, pairs = [[4, 5], [44, 52], [34, 38], [48, 80]], loss = 1.0837255716323853\n",
            "Epoch = 13, pairs = [[76, 77], [61, 63], [50, 54], [45, 46]], loss = 0.33851802349090576\n",
            "Epoch = 13, pairs = [[22, 26], [21, 23], [84, 86], [4, 6]], loss = 0.33795487880706787\n",
            "Epoch = 13, pairs = [[88, 89], [18, 20], [82, 84], [85, 86]], loss = 0.34708499908447266\n",
            "Epoch = 13, pairs = [[1, 2], [39, 40], [82, 83], [9, 10]], loss = 0.32432490587234497\n",
            "Epoch = 13, pairs = [[78, 82], [58, 60], [42, 44], [45, 47]], loss = 0.6037819385528564\n",
            "Epoch = 13, pairs = [[72, 80], [80, 84], [58, 59], [18, 19]], loss = 0.8912714719772339\n",
            "Epoch = 13, pairs = [[6, 7], [29, 30], [37, 39], [26, 30]], loss = 0.39731985330581665\n",
            "Epoch = 13, pairs = [[18, 22], [49, 51], [32, 33], [2, 4]], loss = 0.44619616866111755\n",
            "Epoch 13 took 84.84s.\n",
            "( 0,  1): reprojection: 0.230943, disparity: 0.062293\n",
            "( 0,  2): reprojection: 0.345288, disparity: 0.065241\n",
            "( 0,  4): reprojection: 0.389398, disparity: 0.079943\n",
            "( 0,  8): reprojection: 0.514198, disparity: 0.091919\n",
            "( 0, 16): reprojection: 1.140836, disparity: 0.098383\n",
            "( 0, 32): reprojection: 2.757822, disparity: 0.156672\n",
            "( 1,  2): reprojection: 0.194008, disparity: 0.047197\n",
            "( 1,  3): reprojection: 0.253380, disparity: 0.054024\n",
            "( 2,  3): reprojection: 0.114110, disparity: 0.047024\n",
            "( 2,  4): reprojection: 0.191091, disparity: 0.055109\n",
            "( 2,  6): reprojection: 0.388383, disparity: 0.066053\n",
            "( 3,  4): reprojection: 0.112484, disparity: 0.047908\n",
            "( 3,  5): reprojection: 0.209780, disparity: 0.053696\n",
            "( 4,  5): reprojection: 0.158331, disparity: 0.048831\n",
            "( 4,  6): reprojection: 0.236404, disparity: 0.050026\n",
            "( 4,  8): reprojection: 0.320345, disparity: 0.065065\n",
            "( 4, 12): reprojection: 0.524759, disparity: 0.074771\n",
            "( 5,  6): reprojection: 0.130204, disparity: 0.048468\n",
            "( 5,  7): reprojection: 0.176791, disparity: 0.051152\n",
            "( 6,  7): reprojection: 0.096813, disparity: 0.047116\n",
            "( 6,  8): reprojection: 0.169088, disparity: 0.054321\n",
            "( 6, 10): reprojection: 0.419965, disparity: 0.064401\n",
            "( 7,  8): reprojection: 0.099376, disparity: 0.047774\n",
            "( 7,  9): reprojection: 0.213875, disparity: 0.054040\n",
            "( 8,  9): reprojection: 0.171104, disparity: 0.049977\n",
            "( 8, 10): reprojection: 0.399909, disparity: 0.056512\n",
            "( 8, 12): reprojection: 0.573388, disparity: 0.065330\n",
            "( 8, 16): reprojection: 0.908035, disparity: 0.081698\n",
            "( 8, 24): reprojection: 1.508053, disparity: 0.098932\n",
            "( 9, 10): reprojection: 0.277786, disparity: 0.049720\n",
            "( 9, 11): reprojection: 0.432792, disparity: 0.057191\n",
            "(10, 11): reprojection: 0.191991, disparity: 0.046057\n",
            "(10, 12): reprojection: 0.270352, disparity: 0.053825\n",
            "(10, 14): reprojection: 0.499164, disparity: 0.071955\n",
            "(11, 12): reprojection: 0.170788, disparity: 0.046246\n",
            "(11, 13): reprojection: 0.295206, disparity: 0.056688\n",
            "(12, 13): reprojection: 0.175966, disparity: 0.047636\n",
            "(12, 14): reprojection: 0.343442, disparity: 0.056349\n",
            "(12, 16): reprojection: 0.584408, disparity: 0.081680\n",
            "(12, 20): reprojection: 1.041942, disparity: 0.090400\n",
            "(13, 14): reprojection: 0.212653, disparity: 0.048849\n",
            "(13, 15): reprojection: 0.297999, disparity: 0.060234\n",
            "(14, 15): reprojection: 0.218390, disparity: 0.050210\n",
            "(14, 16): reprojection: 0.505115, disparity: 0.065120\n",
            "(14, 18): reprojection: 0.508924, disparity: 0.073635\n",
            "(15, 16): reprojection: 0.326653, disparity: 0.053243\n",
            "(15, 17): reprojection: 0.435998, disparity: 0.066483\n",
            "(16, 17): reprojection: 0.161767, disparity: 0.051067\n",
            "(16, 18): reprojection: 0.394059, disparity: 0.062560\n",
            "(16, 20): reprojection: 0.701419, disparity: 0.078558\n",
            "(16, 24): reprojection: 1.201755, disparity: 0.098010\n",
            "(16, 32): reprojection: 1.912409, disparity: 0.123845\n",
            "(16, 48): reprojection: 4.227634, disparity: 0.259821\n",
            "(17, 18): reprojection: 0.373211, disparity: 0.054656\n",
            "(17, 19): reprojection: 0.580793, disparity: 0.066721\n",
            "(18, 19): reprojection: 0.247468, disparity: 0.051555\n",
            "(18, 20): reprojection: 0.363427, disparity: 0.061983\n",
            "(18, 22): reprojection: 0.648359, disparity: 0.073924\n",
            "(19, 20): reprojection: 0.141390, disparity: 0.050946\n",
            "(19, 21): reprojection: 0.319321, disparity: 0.060604\n",
            "(20, 21): reprojection: 0.228112, disparity: 0.052799\n",
            "(20, 22): reprojection: 0.358446, disparity: 0.064780\n",
            "(20, 24): reprojection: 0.531154, disparity: 0.076505\n",
            "(20, 28): reprojection: 0.879141, disparity: 0.105665\n",
            "(21, 22): reprojection: 0.183226, disparity: 0.054811\n",
            "(21, 23): reprojection: 0.296393, disparity: 0.064990\n",
            "(22, 23): reprojection: 0.151032, disparity: 0.055167\n",
            "(22, 24): reprojection: 0.223011, disparity: 0.065920\n",
            "(22, 26): reprojection: 0.480057, disparity: 0.080598\n",
            "(23, 24): reprojection: 0.132297, disparity: 0.054697\n",
            "(23, 25): reprojection: 0.255271, disparity: 0.064802\n",
            "(24, 25): reprojection: 0.173973, disparity: 0.056381\n",
            "(24, 26): reprojection: 0.278282, disparity: 0.070003\n",
            "(24, 28): reprojection: 0.475364, disparity: 0.082162\n",
            "(24, 32): reprojection: 0.958167, disparity: 0.097736\n",
            "(24, 40): reprojection: 1.683588, disparity: 0.172485\n",
            "(25, 26): reprojection: 0.151631, disparity: 0.061457\n",
            "(25, 27): reprojection: 0.299241, disparity: 0.073076\n",
            "(26, 27): reprojection: 0.178877, disparity: 0.057550\n",
            "(26, 28): reprojection: 0.246820, disparity: 0.068561\n",
            "(26, 30): reprojection: 0.504732, disparity: 0.084964\n",
            "(27, 28): reprojection: 0.189830, disparity: 0.060410\n",
            "(27, 29): reprojection: 0.338562, disparity: 0.070661\n",
            "(28, 29): reprojection: 0.232598, disparity: 0.060181\n",
            "(28, 30): reprojection: 0.327039, disparity: 0.071182\n",
            "(28, 32): reprojection: 0.583027, disparity: 0.080031\n",
            "(28, 36): reprojection: 0.921633, disparity: 0.116386\n",
            "(29, 30): reprojection: 0.175150, disparity: 0.055242\n",
            "(29, 31): reprojection: 0.345878, disparity: 0.065056\n",
            "(30, 31): reprojection: 0.229924, disparity: 0.055061\n",
            "(30, 32): reprojection: 0.317458, disparity: 0.067884\n",
            "(30, 34): reprojection: 0.574794, disparity: 0.090146\n",
            "(31, 32): reprojection: 0.154333, disparity: 0.056505\n",
            "(31, 33): reprojection: 0.355389, disparity: 0.069154\n",
            "(32, 33): reprojection: 0.269333, disparity: 0.060830\n",
            "(32, 34): reprojection: 0.290778, disparity: 0.070698\n",
            "(32, 36): reprojection: 0.457515, disparity: 0.089681\n",
            "(32, 40): reprojection: 0.804582, disparity: 0.132459\n",
            "(32, 48): reprojection: 1.708827, disparity: 0.226800\n",
            "(32, 64): reprojection: 5.159156, disparity: 0.183205\n",
            "(33, 34): reprojection: 0.186253, disparity: 0.058163\n",
            "(33, 35): reprojection: 0.310523, disparity: 0.068554\n",
            "(34, 35): reprojection: 0.214071, disparity: 0.057509\n",
            "(34, 36): reprojection: 0.330414, disparity: 0.070067\n",
            "(34, 38): reprojection: 0.526037, disparity: 0.083735\n",
            "(35, 36): reprojection: 0.264050, disparity: 0.062047\n",
            "(35, 37): reprojection: 0.330061, disparity: 0.072204\n",
            "(36, 37): reprojection: 0.286852, disparity: 0.062577\n",
            "(36, 38): reprojection: 0.447579, disparity: 0.069816\n",
            "(36, 40): reprojection: 0.527185, disparity: 0.091760\n",
            "(36, 44): reprojection: 1.005148, disparity: 0.154711\n",
            "(37, 38): reprojection: 0.253667, disparity: 0.059046\n",
            "(37, 39): reprojection: 0.299249, disparity: 0.070237\n",
            "(38, 39): reprojection: 0.216388, disparity: 0.064284\n",
            "(38, 40): reprojection: 0.463003, disparity: 0.086273\n",
            "(38, 42): reprojection: 0.911725, disparity: 0.125721\n",
            "(39, 40): reprojection: 0.320013, disparity: 0.063738\n",
            "(39, 41): reprojection: 0.584362, disparity: 0.092218\n",
            "(40, 41): reprojection: 0.329292, disparity: 0.076452\n",
            "(40, 42): reprojection: 0.536596, disparity: 0.091469\n",
            "(40, 44): reprojection: 0.701926, disparity: 0.125936\n",
            "(40, 48): reprojection: 1.031881, disparity: 0.183068\n",
            "(40, 56): reprojection: 1.945050, disparity: 0.206839\n",
            "(41, 42): reprojection: 0.255494, disparity: 0.063177\n",
            "(41, 43): reprojection: 0.345387, disparity: 0.085724\n",
            "(42, 43): reprojection: 0.190306, disparity: 0.069081\n",
            "(42, 44): reprojection: 0.359781, disparity: 0.088152\n",
            "(42, 46): reprojection: 0.592123, disparity: 0.106997\n",
            "(43, 44): reprojection: 0.216673, disparity: 0.067589\n",
            "(43, 45): reprojection: 0.338776, disparity: 0.079616\n",
            "(44, 45): reprojection: 0.184813, disparity: 0.065122\n",
            "(44, 46): reprojection: 0.345645, disparity: 0.079736\n",
            "(44, 48): reprojection: 0.546989, disparity: 0.112579\n",
            "(44, 52): reprojection: 0.930194, disparity: 0.120317\n",
            "(45, 46): reprojection: 0.217759, disparity: 0.067761\n",
            "(45, 47): reprojection: 0.355803, disparity: 0.087237\n",
            "(46, 47): reprojection: 0.210914, disparity: 0.064649\n",
            "(46, 48): reprojection: 0.365541, disparity: 0.086878\n",
            "(46, 50): reprojection: 0.574492, disparity: 0.106127\n",
            "(47, 48): reprojection: 0.232149, disparity: 0.067393\n",
            "(47, 49): reprojection: 0.334787, disparity: 0.080137\n",
            "(48, 49): reprojection: 0.212990, disparity: 0.071189\n",
            "(48, 50): reprojection: 0.337300, disparity: 0.086977\n",
            "(48, 52): reprojection: 0.528927, disparity: 0.092903\n",
            "(48, 56): reprojection: 1.132778, disparity: 0.115076\n",
            "(48, 64): reprojection: 2.639165, disparity: 0.152664\n",
            "(48, 80): reprojection: 7.563105, disparity: 0.187861\n",
            "(49, 50): reprojection: 0.196311, disparity: 0.072383\n",
            "(49, 51): reprojection: 0.331061, disparity: 0.085172\n",
            "(50, 51): reprojection: 0.190747, disparity: 0.067773\n",
            "(50, 52): reprojection: 0.336898, disparity: 0.073999\n",
            "(50, 54): reprojection: 0.639197, disparity: 0.086821\n",
            "(51, 52): reprojection: 0.214296, disparity: 0.065135\n",
            "(51, 53): reprojection: 0.357629, disparity: 0.071732\n",
            "(52, 53): reprojection: 0.213358, disparity: 0.067179\n",
            "(52, 54): reprojection: 0.332447, disparity: 0.076483\n",
            "(52, 56): reprojection: 0.772044, disparity: 0.085287\n",
            "(52, 60): reprojection: 1.227439, disparity: 0.116581\n",
            "(53, 54): reprojection: 0.197747, disparity: 0.065830\n",
            "(53, 55): reprojection: 0.433567, disparity: 0.077654\n",
            "(54, 55): reprojection: 0.342242, disparity: 0.066141\n",
            "(54, 56): reprojection: 0.684579, disparity: 0.072229\n",
            "(54, 58): reprojection: 0.644491, disparity: 0.079859\n",
            "(55, 56): reprojection: 0.379415, disparity: 0.068284\n",
            "(55, 57): reprojection: 0.482576, disparity: 0.072765\n",
            "(56, 57): reprojection: 0.352552, disparity: 0.066483\n",
            "(56, 58): reprojection: 0.645447, disparity: 0.078314\n",
            "(56, 60): reprojection: 1.002962, disparity: 0.099781\n",
            "(56, 64): reprojection: 1.485645, disparity: 0.104227\n",
            "(56, 72): reprojection: 3.768338, disparity: 0.156974\n",
            "(57, 58): reprojection: 0.411213, disparity: 0.071576\n",
            "(57, 59): reprojection: 0.607370, disparity: 0.079325\n",
            "(58, 59): reprojection: 0.282821, disparity: 0.072612\n",
            "(58, 60): reprojection: 0.452858, disparity: 0.090205\n",
            "(58, 62): reprojection: 0.775387, disparity: 0.099711\n",
            "(59, 60): reprojection: 0.229988, disparity: 0.082095\n",
            "(59, 61): reprojection: 0.381636, disparity: 0.081354\n",
            "(60, 61): reprojection: 0.271845, disparity: 0.074442\n",
            "(60, 62): reprojection: 0.408577, disparity: 0.081922\n",
            "(60, 64): reprojection: 0.755837, disparity: 0.094405\n",
            "(60, 68): reprojection: 1.659517, disparity: 0.137328\n",
            "(61, 62): reprojection: 0.238307, disparity: 0.073955\n",
            "(61, 63): reprojection: 0.451973, disparity: 0.086579\n",
            "(62, 63): reprojection: 0.286921, disparity: 0.070650\n",
            "(62, 64): reprojection: 0.449917, disparity: 0.080812\n",
            "(62, 66): reprojection: 0.827402, disparity: 0.114790\n",
            "(63, 64): reprojection: 0.308506, disparity: 0.074350\n",
            "(63, 65): reprojection: 0.514006, disparity: 0.080567\n",
            "(64, 65): reprojection: 0.303257, disparity: 0.077137\n",
            "(64, 66): reprojection: 0.544767, disparity: 0.084587\n",
            "(64, 68): reprojection: 0.743075, disparity: 0.109797\n",
            "(64, 72): reprojection: 1.447973, disparity: 0.134070\n",
            "(64, 80): reprojection: 2.895538, disparity: 0.182364\n",
            "(65, 66): reprojection: 0.331867, disparity: 0.072397\n",
            "(65, 67): reprojection: 0.579056, disparity: 0.083050\n",
            "(66, 67): reprojection: 0.322305, disparity: 0.073602\n",
            "(66, 68): reprojection: 0.545326, disparity: 0.082318\n",
            "(66, 70): reprojection: 0.912548, disparity: 0.101394\n",
            "(67, 68): reprojection: 0.413077, disparity: 0.071784\n",
            "(67, 69): reprojection: 0.530396, disparity: 0.077941\n",
            "(68, 69): reprojection: 0.298974, disparity: 0.068879\n",
            "(68, 70): reprojection: 0.631101, disparity: 0.079441\n",
            "(68, 72): reprojection: 0.918635, disparity: 0.093314\n",
            "(68, 76): reprojection: 1.500080, disparity: 0.121221\n",
            "(69, 70): reprojection: 0.415459, disparity: 0.070128\n",
            "(69, 71): reprojection: 0.618231, disparity: 0.078630\n",
            "(70, 71): reprojection: 0.314923, disparity: 0.068682\n",
            "(70, 72): reprojection: 0.427647, disparity: 0.074519\n",
            "(70, 74): reprojection: 0.789169, disparity: 0.091293\n",
            "(71, 72): reprojection: 0.324902, disparity: 0.068425\n",
            "(71, 73): reprojection: 0.525233, disparity: 0.078833\n",
            "(72, 73): reprojection: 0.298250, disparity: 0.068307\n",
            "(72, 74): reprojection: 0.467487, disparity: 0.081263\n",
            "(72, 76): reprojection: 0.724709, disparity: 0.093591\n",
            "(72, 80): reprojection: 1.202572, disparity: 0.120503\n",
            "(72, 88): reprojection: 2.127011, disparity: 0.130099\n",
            "(73, 74): reprojection: 0.375127, disparity: 0.070128\n",
            "(73, 75): reprojection: 0.578559, disparity: 0.082291\n",
            "(74, 75): reprojection: 0.305836, disparity: 0.068595\n",
            "(74, 76): reprojection: 0.418545, disparity: 0.074323\n",
            "(74, 78): reprojection: 0.679368, disparity: 0.081392\n",
            "(75, 76): reprojection: 0.299924, disparity: 0.063743\n",
            "(75, 77): reprojection: 0.506768, disparity: 0.072781\n",
            "(76, 77): reprojection: 0.299038, disparity: 0.063757\n",
            "(76, 78): reprojection: 0.472952, disparity: 0.067253\n",
            "(76, 80): reprojection: 0.611118, disparity: 0.086139\n",
            "(76, 84): reprojection: 1.099905, disparity: 0.096823\n",
            "(77, 78): reprojection: 0.288322, disparity: 0.062433\n",
            "(77, 79): reprojection: 0.369078, disparity: 0.072988\n",
            "(78, 79): reprojection: 0.242475, disparity: 0.067217\n",
            "(78, 80): reprojection: 0.331512, disparity: 0.076391\n",
            "(78, 82): reprojection: 0.585035, disparity: 0.081728\n",
            "(79, 80): reprojection: 0.188942, disparity: 0.065151\n",
            "(79, 81): reprojection: 0.331960, disparity: 0.068736\n",
            "(80, 81): reprojection: 0.199125, disparity: 0.061890\n",
            "(80, 82): reprojection: 0.350148, disparity: 0.066721\n",
            "(80, 84): reprojection: 0.666749, disparity: 0.072931\n",
            "(80, 88): reprojection: 1.116343, disparity: 0.090546\n",
            "(81, 82): reprojection: 0.235333, disparity: 0.062167\n",
            "(81, 83): reprojection: 0.415513, disparity: 0.068171\n",
            "(82, 83): reprojection: 0.199646, disparity: 0.059488\n",
            "(82, 84): reprojection: 0.354363, disparity: 0.067433\n",
            "(82, 86): reprojection: 0.600314, disparity: 0.079094\n",
            "(83, 84): reprojection: 0.197195, disparity: 0.058895\n",
            "(83, 85): reprojection: 0.409119, disparity: 0.070065\n",
            "(84, 85): reprojection: 0.213135, disparity: 0.058883\n",
            "(84, 86): reprojection: 0.322558, disparity: 0.065768\n",
            "(84, 88): reprojection: 0.647378, disparity: 0.077435\n",
            "(85, 86): reprojection: 0.194950, disparity: 0.057402\n",
            "(85, 87): reprojection: 0.334519, disparity: 0.064195\n",
            "(86, 87): reprojection: 0.260660, disparity: 0.060562\n",
            "(86, 88): reprojection: 0.410602, disparity: 0.074389\n",
            "(86, 90): reprojection: 0.559213, disparity: 0.078620\n",
            "(87, 88): reprojection: 0.216709, disparity: 0.061991\n",
            "(87, 89): reprojection: 0.331477, disparity: 0.069069\n",
            "(88, 89): reprojection: 0.160734, disparity: 0.056080\n",
            "(88, 90): reprojection: 0.330846, disparity: 0.061714\n",
            "(89, 90): reprojection: 0.214713, disparity: 0.053340\n",
            "(89, 91): reprojection: 0.452144, disparity: 0.070861\n",
            "(90, 91): reprojection: 0.362316, disparity: 0.066705\n",
            "Mean:     reprojection: 0.362316, disparity: 0.066705\n",
            "Done Validation for epoch 14 (3640 iterations)\n",
            "Epoch = 14, pairs = [[56, 72], [40, 56], [53, 55], [32, 36]], loss = 1.574662446975708\n",
            "Epoch = 14, pairs = [[5, 7], [16, 20], [26, 28], [59, 61]], loss = 0.4351712465286255\n",
            "Epoch = 14, pairs = [[26, 27], [20, 21], [70, 71], [89, 90]], loss = 0.26671692728996277\n",
            "Epoch = 14, pairs = [[28, 29], [24, 40], [78, 79], [20, 28]], loss = 1.044776439666748\n",
            "Epoch = 14, pairs = [[48, 56], [67, 69], [53, 54], [16, 32]], loss = 0.9794132709503174\n",
            "Epoch = 14, pairs = [[64, 68], [37, 39], [6, 7], [72, 80]], loss = 0.4883710741996765\n",
            "Epoch = 14, pairs = [[8, 12], [22, 24], [30, 32], [46, 48]], loss = 0.3848875164985657\n",
            "Epoch = 14, pairs = [[65, 67], [78, 80], [32, 33], [0, 32]], loss = 0.6335516571998596\n",
            "Epoch = 14, pairs = [[27, 29], [32, 34], [32, 64], [51, 53]], loss = 0.7621356248855591\n",
            "Epoch = 14, pairs = [[3, 4], [86, 87], [29, 31], [32, 40]], loss = 0.38573867082595825\n",
            "Epoch = 14, pairs = [[78, 82], [34, 36], [42, 46], [71, 72]], loss = 0.49400854110717773\n",
            "Epoch = 14, pairs = [[75, 77], [79, 81], [1, 2], [14, 18]], loss = 0.36770099401474\n",
            "Epoch = 14, pairs = [[52, 53], [52, 56], [31, 33], [14, 16]], loss = 0.433824360370636\n",
            "Epoch = 14, pairs = [[55, 57], [72, 76], [58, 60], [0, 2]], loss = 0.47862663865089417\n",
            "Epoch = 14, pairs = [[9, 11], [15, 17], [68, 72], [34, 35]], loss = 0.46729353070259094\n",
            "Epoch = 14, pairs = [[2, 6], [50, 52], [87, 89], [62, 63]], loss = 0.35702237486839294\n",
            "Epoch = 14, pairs = [[38, 40], [7, 8], [46, 47], [72, 74]], loss = 0.33005446195602417\n",
            "Epoch = 14, pairs = [[67, 68], [47, 48], [33, 35], [66, 70]], loss = 0.4338913559913635\n",
            "Epoch = 14, pairs = [[84, 85], [76, 78], [5, 6], [50, 54]], loss = 0.3375260531902313\n",
            "Epoch = 14, pairs = [[57, 58], [88, 90], [58, 59], [80, 82]], loss = 0.3597657084465027\n",
            "Epoch = 14, pairs = [[44, 46], [72, 73], [11, 12], [1, 3]], loss = 0.28870949149131775\n",
            "Epoch = 14, pairs = [[19, 21], [4, 5], [42, 43], [52, 54]], loss = 0.2705484628677368\n",
            "Epoch = 14, pairs = [[43, 44], [24, 25], [39, 41], [44, 45]], loss = 0.31283020973205566\n",
            "Epoch = 14, pairs = [[64, 80], [77, 79], [65, 66], [90, 91]], loss = 0.643303632736206\n",
            "Epoch = 14, pairs = [[68, 69], [56, 60], [84, 86], [9, 10]], loss = 0.42496833205223083\n",
            "Epoch = 14, pairs = [[48, 49], [20, 22], [25, 27], [35, 36]], loss = 0.30809029936790466\n",
            "Epoch = 14, pairs = [[38, 39], [40, 44], [73, 74], [57, 59]], loss = 0.449857234954834\n",
            "Epoch = 14, pairs = [[6, 10], [54, 55], [69, 70], [69, 71]], loss = 0.4210214614868164\n",
            "Epoch = 14, pairs = [[16, 18], [18, 19], [2, 3], [15, 16]], loss = 0.302670419216156\n",
            "Epoch = 14, pairs = [[48, 52], [80, 81], [13, 15], [30, 34]], loss = 0.41240012645721436\n",
            "Epoch = 14, pairs = [[0, 1], [35, 37], [89, 91], [60, 64]], loss = 0.43222370743751526\n",
            "Epoch = 14, pairs = [[28, 32], [10, 14], [40, 41], [61, 62]], loss = 0.411920964717865\n",
            "Epoch = 14, pairs = [[51, 52], [21, 23], [84, 88], [10, 12]], loss = 0.330394446849823\n",
            "Epoch = 14, pairs = [[63, 65], [44, 52], [64, 72], [43, 45]], loss = 0.5894637107849121\n",
            "Epoch = 14, pairs = [[45, 47], [52, 60], [64, 66], [22, 26]], loss = 0.48564887046813965\n",
            "Epoch = 14, pairs = [[54, 58], [8, 9], [81, 82], [24, 28]], loss = 0.31654658913612366\n",
            "Epoch = 14, pairs = [[76, 80], [27, 28], [80, 88], [16, 17]], loss = 0.41458678245544434\n",
            "Epoch = 14, pairs = [[60, 62], [16, 24], [26, 30], [48, 50]], loss = 0.505542516708374\n",
            "Epoch = 14, pairs = [[76, 84], [48, 64], [4, 8], [0, 4]], loss = 0.6486266851425171\n",
            "Epoch = 14, pairs = [[7, 9], [17, 18], [62, 66], [70, 74]], loss = 0.41992583870887756\n",
            "Epoch = 14, pairs = [[11, 13], [36, 40], [56, 58], [2, 4]], loss = 0.3881217837333679\n",
            "Epoch = 14, pairs = [[74, 76], [86, 88], [36, 44], [12, 20]], loss = 0.5682686567306519\n",
            "Epoch = 14, pairs = [[17, 19], [41, 43], [85, 86], [12, 16]], loss = 0.4205875098705292\n",
            "Epoch = 14, pairs = [[61, 63], [39, 40], [28, 36], [30, 31]], loss = 0.49362343549728394\n",
            "Epoch = 14, pairs = [[62, 64], [68, 76], [14, 15], [18, 22]], loss = 0.5242736339569092\n",
            "Epoch = 14, pairs = [[66, 68], [8, 10], [16, 48], [48, 80]], loss = 1.0828369855880737\n",
            "Epoch = 14, pairs = [[37, 38], [23, 25], [0, 16], [63, 64]], loss = 0.44626516103744507\n",
            "Epoch = 14, pairs = [[72, 88], [19, 20], [85, 87], [75, 76]], loss = 0.7256909608840942\n",
            "Epoch = 14, pairs = [[74, 78], [45, 46], [77, 78], [12, 13]], loss = 0.3647175133228302\n",
            "Epoch = 14, pairs = [[3, 5], [50, 51], [38, 42], [18, 20]], loss = 0.49015986919403076\n",
            "Epoch = 14, pairs = [[28, 30], [32, 48], [76, 77], [66, 67]], loss = 0.7181028127670288\n",
            "Epoch = 14, pairs = [[21, 22], [82, 83], [88, 89], [23, 24]], loss = 0.22616012394428253\n",
            "Epoch = 14, pairs = [[58, 62], [70, 72], [29, 30], [64, 65]], loss = 0.43943989276885986\n",
            "Epoch = 14, pairs = [[13, 14], [54, 56], [41, 42], [59, 60]], loss = 0.3963361978530884\n",
            "Epoch = 14, pairs = [[55, 56], [60, 68], [68, 70], [6, 8]], loss = 0.6262797117233276\n",
            "Epoch = 14, pairs = [[12, 14], [81, 83], [56, 57], [4, 6]], loss = 0.3834325671195984\n",
            "Epoch = 14, pairs = [[47, 49], [60, 61], [44, 48], [56, 64]], loss = 0.7071484923362732\n",
            "Epoch = 14, pairs = [[36, 38], [20, 24], [4, 12], [80, 84]], loss = 0.6053353548049927\n",
            "Epoch = 14, pairs = [[24, 32], [79, 80], [8, 16], [87, 88]], loss = 0.6311660408973694\n",
            "Epoch = 14, pairs = [[73, 75], [86, 90], [74, 75], [49, 50]], loss = 0.4243672788143158\n",
            "Epoch = 14, pairs = [[83, 84], [33, 34], [71, 73], [46, 50]], loss = 0.35857391357421875\n",
            "Epoch = 14, pairs = [[25, 26], [22, 23], [10, 11], [42, 44]], loss = 0.24793529510498047\n",
            "Epoch = 14, pairs = [[40, 48], [36, 37], [49, 51], [83, 85]], loss = 0.47994157671928406\n",
            "Epoch = 14, pairs = [[82, 86], [24, 26], [34, 38], [40, 42]], loss = 0.4205820858478546\n",
            "Epoch = 14, pairs = [[82, 84], [8, 24], [31, 32], [0, 8]], loss = 0.5387973189353943\n",
            "Epoch 14 took 85.02s.\n",
            "( 0,  1): reprojection: 0.231341, disparity: 0.052018\n",
            "( 0,  2): reprojection: 0.340612, disparity: 0.051908\n",
            "( 0,  4): reprojection: 0.372643, disparity: 0.060153\n",
            "( 0,  8): reprojection: 0.481329, disparity: 0.070702\n",
            "( 0, 16): reprojection: 0.676210, disparity: 0.081707\n",
            "( 0, 32): reprojection: 0.965797, disparity: 0.097600\n",
            "( 1,  2): reprojection: 0.192667, disparity: 0.044733\n",
            "( 1,  3): reprojection: 0.244347, disparity: 0.051521\n",
            "( 2,  3): reprojection: 0.106411, disparity: 0.044953\n",
            "( 2,  4): reprojection: 0.174903, disparity: 0.050292\n",
            "( 2,  6): reprojection: 0.348400, disparity: 0.063185\n",
            "( 3,  4): reprojection: 0.102843, disparity: 0.045193\n",
            "( 3,  5): reprojection: 0.203028, disparity: 0.050465\n",
            "( 4,  5): reprojection: 0.156264, disparity: 0.047919\n",
            "( 4,  6): reprojection: 0.225327, disparity: 0.048501\n",
            "( 4,  8): reprojection: 0.287694, disparity: 0.062174\n",
            "( 4, 12): reprojection: 0.442140, disparity: 0.069465\n",
            "( 5,  6): reprojection: 0.116917, disparity: 0.047255\n",
            "( 5,  7): reprojection: 0.153106, disparity: 0.049692\n",
            "( 6,  7): reprojection: 0.092802, disparity: 0.044566\n",
            "( 6,  8): reprojection: 0.152690, disparity: 0.052188\n",
            "( 6, 10): reprojection: 0.399312, disparity: 0.059404\n",
            "( 7,  8): reprojection: 0.085580, disparity: 0.045153\n",
            "( 7,  9): reprojection: 0.199959, disparity: 0.050163\n",
            "( 8,  9): reprojection: 0.160929, disparity: 0.047337\n",
            "( 8, 10): reprojection: 0.371188, disparity: 0.051629\n",
            "( 8, 12): reprojection: 0.528754, disparity: 0.058540\n",
            "( 8, 16): reprojection: 0.617189, disparity: 0.069764\n",
            "( 8, 24): reprojection: 0.745723, disparity: 0.085270\n",
            "( 9, 10): reprojection: 0.258117, disparity: 0.047964\n",
            "( 9, 11): reprojection: 0.385787, disparity: 0.053036\n",
            "(10, 11): reprojection: 0.171058, disparity: 0.042726\n",
            "(10, 12): reprojection: 0.211563, disparity: 0.049243\n",
            "(10, 14): reprojection: 0.340140, disparity: 0.063370\n",
            "(11, 12): reprojection: 0.138003, disparity: 0.042846\n",
            "(11, 13): reprojection: 0.223474, disparity: 0.052264\n",
            "(12, 13): reprojection: 0.123535, disparity: 0.043796\n",
            "(12, 14): reprojection: 0.248979, disparity: 0.050072\n",
            "(12, 16): reprojection: 0.368067, disparity: 0.070264\n",
            "(12, 20): reprojection: 0.624380, disparity: 0.078589\n",
            "(13, 14): reprojection: 0.184643, disparity: 0.045353\n",
            "(13, 15): reprojection: 0.198545, disparity: 0.053089\n",
            "(14, 15): reprojection: 0.177403, disparity: 0.046076\n",
            "(14, 16): reprojection: 0.415490, disparity: 0.055272\n",
            "(14, 18): reprojection: 0.325108, disparity: 0.062987\n",
            "(15, 16): reprojection: 0.288107, disparity: 0.049222\n",
            "(15, 17): reprojection: 0.368336, disparity: 0.056668\n",
            "(16, 17): reprojection: 0.136649, disparity: 0.046389\n",
            "(16, 18): reprojection: 0.328324, disparity: 0.056951\n",
            "(16, 20): reprojection: 0.599702, disparity: 0.072202\n",
            "(16, 24): reprojection: 0.842423, disparity: 0.089636\n",
            "(16, 32): reprojection: 1.152732, disparity: 0.110919\n",
            "(16, 48): reprojection: 1.304965, disparity: 0.231753\n",
            "(17, 18): reprojection: 0.319390, disparity: 0.051467\n",
            "(17, 19): reprojection: 0.498393, disparity: 0.063293\n",
            "(18, 19): reprojection: 0.222281, disparity: 0.047614\n",
            "(18, 20): reprojection: 0.324453, disparity: 0.058138\n",
            "(18, 22): reprojection: 0.501698, disparity: 0.065768\n",
            "(19, 20): reprojection: 0.120490, disparity: 0.047103\n",
            "(19, 21): reprojection: 0.276702, disparity: 0.054390\n",
            "(20, 21): reprojection: 0.183791, disparity: 0.048123\n",
            "(20, 22): reprojection: 0.276780, disparity: 0.055440\n",
            "(20, 24): reprojection: 0.352316, disparity: 0.066421\n",
            "(20, 28): reprojection: 0.474013, disparity: 0.102614\n",
            "(21, 22): reprojection: 0.143593, disparity: 0.048671\n",
            "(21, 23): reprojection: 0.203459, disparity: 0.061426\n",
            "(22, 23): reprojection: 0.116696, disparity: 0.051030\n",
            "(22, 24): reprojection: 0.170863, disparity: 0.060717\n",
            "(22, 26): reprojection: 0.333413, disparity: 0.074620\n",
            "(23, 24): reprojection: 0.114378, disparity: 0.049380\n",
            "(23, 25): reprojection: 0.193381, disparity: 0.057266\n",
            "(24, 25): reprojection: 0.129109, disparity: 0.052978\n",
            "(24, 26): reprojection: 0.188346, disparity: 0.062026\n",
            "(24, 28): reprojection: 0.329704, disparity: 0.072027\n",
            "(24, 32): reprojection: 0.662075, disparity: 0.085407\n",
            "(24, 40): reprojection: 0.927674, disparity: 0.151530\n",
            "(25, 26): reprojection: 0.134581, disparity: 0.053795\n",
            "(25, 27): reprojection: 0.216304, disparity: 0.065127\n",
            "(26, 27): reprojection: 0.133560, disparity: 0.054935\n",
            "(26, 28): reprojection: 0.187148, disparity: 0.060738\n",
            "(26, 30): reprojection: 0.336179, disparity: 0.068579\n",
            "(27, 28): reprojection: 0.138317, disparity: 0.057062\n",
            "(27, 29): reprojection: 0.264688, disparity: 0.062098\n",
            "(28, 29): reprojection: 0.176891, disparity: 0.056057\n",
            "(28, 30): reprojection: 0.255249, disparity: 0.060621\n",
            "(28, 32): reprojection: 0.390265, disparity: 0.071019\n",
            "(28, 36): reprojection: 0.588786, disparity: 0.096953\n",
            "(29, 30): reprojection: 0.134225, disparity: 0.050381\n",
            "(29, 31): reprojection: 0.261116, disparity: 0.060747\n",
            "(30, 31): reprojection: 0.190876, disparity: 0.049799\n",
            "(30, 32): reprojection: 0.250921, disparity: 0.059544\n",
            "(30, 34): reprojection: 0.411229, disparity: 0.075356\n",
            "(31, 32): reprojection: 0.129055, disparity: 0.053216\n",
            "(31, 33): reprojection: 0.282676, disparity: 0.060484\n",
            "(32, 33): reprojection: 0.228341, disparity: 0.054872\n",
            "(32, 34): reprojection: 0.230712, disparity: 0.061390\n",
            "(32, 36): reprojection: 0.336402, disparity: 0.072476\n",
            "(32, 40): reprojection: 0.446547, disparity: 0.128869\n",
            "(32, 48): reprojection: 0.883763, disparity: 0.182637\n",
            "(32, 64): reprojection: 1.300687, disparity: 0.128666\n",
            "(33, 34): reprojection: 0.157804, disparity: 0.051951\n",
            "(33, 35): reprojection: 0.260417, disparity: 0.063784\n",
            "(34, 35): reprojection: 0.180812, disparity: 0.053505\n",
            "(34, 36): reprojection: 0.264999, disparity: 0.061810\n",
            "(34, 38): reprojection: 0.339349, disparity: 0.074452\n",
            "(35, 36): reprojection: 0.224410, disparity: 0.054252\n",
            "(35, 37): reprojection: 0.258713, disparity: 0.065841\n",
            "(36, 37): reprojection: 0.249031, disparity: 0.059112\n",
            "(36, 38): reprojection: 0.400192, disparity: 0.060706\n",
            "(36, 40): reprojection: 0.336237, disparity: 0.090997\n",
            "(36, 44): reprojection: 0.649953, disparity: 0.135127\n",
            "(37, 38): reprojection: 0.226267, disparity: 0.054323\n",
            "(37, 39): reprojection: 0.246712, disparity: 0.062912\n",
            "(38, 39): reprojection: 0.186429, disparity: 0.055735\n",
            "(38, 40): reprojection: 0.372180, disparity: 0.081458\n",
            "(38, 42): reprojection: 0.695655, disparity: 0.113890\n",
            "(39, 40): reprojection: 0.279033, disparity: 0.065770\n",
            "(39, 41): reprojection: 0.494891, disparity: 0.079483\n",
            "(40, 41): reprojection: 0.292148, disparity: 0.060254\n",
            "(40, 42): reprojection: 0.440138, disparity: 0.069249\n",
            "(40, 44): reprojection: 0.559588, disparity: 0.079978\n",
            "(40, 48): reprojection: 0.759582, disparity: 0.112609\n",
            "(40, 56): reprojection: 0.778593, disparity: 0.141196\n",
            "(41, 42): reprojection: 0.205022, disparity: 0.058372\n",
            "(41, 43): reprojection: 0.297584, disparity: 0.068773\n",
            "(42, 43): reprojection: 0.183467, disparity: 0.059798\n",
            "(42, 44): reprojection: 0.334668, disparity: 0.068211\n",
            "(42, 46): reprojection: 0.547978, disparity: 0.087457\n",
            "(43, 44): reprojection: 0.194404, disparity: 0.059573\n",
            "(43, 45): reprojection: 0.297292, disparity: 0.077232\n",
            "(44, 45): reprojection: 0.149466, disparity: 0.059030\n",
            "(44, 46): reprojection: 0.294843, disparity: 0.072327\n",
            "(44, 48): reprojection: 0.431701, disparity: 0.090443\n",
            "(44, 52): reprojection: 0.619344, disparity: 0.110198\n",
            "(45, 46): reprojection: 0.204676, disparity: 0.063044\n",
            "(45, 47): reprojection: 0.329681, disparity: 0.079670\n",
            "(46, 47): reprojection: 0.181602, disparity: 0.059755\n",
            "(46, 48): reprojection: 0.294432, disparity: 0.074096\n",
            "(46, 50): reprojection: 0.414630, disparity: 0.084645\n",
            "(47, 48): reprojection: 0.201750, disparity: 0.058065\n",
            "(47, 49): reprojection: 0.255817, disparity: 0.070095\n",
            "(48, 49): reprojection: 0.169043, disparity: 0.070807\n",
            "(48, 50): reprojection: 0.232661, disparity: 0.077860\n",
            "(48, 52): reprojection: 0.380021, disparity: 0.097094\n",
            "(48, 56): reprojection: 0.700674, disparity: 0.124047\n",
            "(48, 64): reprojection: 1.080091, disparity: 0.176312\n",
            "(48, 80): reprojection: 1.473044, disparity: 0.147390\n",
            "(49, 50): reprojection: 0.139131, disparity: 0.063536\n",
            "(49, 51): reprojection: 0.239789, disparity: 0.073857\n",
            "(50, 51): reprojection: 0.156598, disparity: 0.062257\n",
            "(50, 52): reprojection: 0.265413, disparity: 0.074521\n",
            "(50, 54): reprojection: 0.445848, disparity: 0.098546\n",
            "(51, 52): reprojection: 0.179450, disparity: 0.062139\n",
            "(51, 53): reprojection: 0.316524, disparity: 0.068743\n",
            "(52, 53): reprojection: 0.182906, disparity: 0.060402\n",
            "(52, 54): reprojection: 0.265168, disparity: 0.073392\n",
            "(52, 56): reprojection: 0.572379, disparity: 0.078087\n",
            "(52, 60): reprojection: 0.698940, disparity: 0.113174\n",
            "(53, 54): reprojection: 0.139274, disparity: 0.065575\n",
            "(53, 55): reprojection: 0.340596, disparity: 0.072747\n",
            "(54, 55): reprojection: 0.308970, disparity: 0.059082\n",
            "(54, 56): reprojection: 0.568440, disparity: 0.064941\n",
            "(54, 58): reprojection: 0.413881, disparity: 0.074549\n",
            "(55, 56): reprojection: 0.343658, disparity: 0.060679\n",
            "(55, 57): reprojection: 0.379992, disparity: 0.066295\n",
            "(56, 57): reprojection: 0.301773, disparity: 0.062038\n",
            "(56, 58): reprojection: 0.552848, disparity: 0.075351\n",
            "(56, 60): reprojection: 0.706877, disparity: 0.093012\n",
            "(56, 64): reprojection: 0.876726, disparity: 0.102723\n",
            "(56, 72): reprojection: 0.997047, disparity: 0.122854\n",
            "(57, 58): reprojection: 0.360740, disparity: 0.070299\n",
            "(57, 59): reprojection: 0.497604, disparity: 0.078383\n",
            "(58, 59): reprojection: 0.256294, disparity: 0.067497\n",
            "(58, 60): reprojection: 0.384588, disparity: 0.079869\n",
            "(58, 62): reprojection: 0.508536, disparity: 0.094663\n",
            "(59, 60): reprojection: 0.189934, disparity: 0.074257\n",
            "(59, 61): reprojection: 0.320590, disparity: 0.078138\n",
            "(60, 61): reprojection: 0.227939, disparity: 0.074074\n",
            "(60, 62): reprojection: 0.317563, disparity: 0.084522\n",
            "(60, 64): reprojection: 0.521390, disparity: 0.092187\n",
            "(60, 68): reprojection: 0.706418, disparity: 0.115756\n",
            "(61, 62): reprojection: 0.223867, disparity: 0.073974\n",
            "(61, 63): reprojection: 0.325101, disparity: 0.081335\n",
            "(62, 63): reprojection: 0.228971, disparity: 0.067914\n",
            "(62, 64): reprojection: 0.343073, disparity: 0.074548\n",
            "(62, 66): reprojection: 0.466857, disparity: 0.094242\n",
            "(63, 64): reprojection: 0.224585, disparity: 0.069496\n",
            "(63, 65): reprojection: 0.325020, disparity: 0.074928\n",
            "(64, 65): reprojection: 0.211473, disparity: 0.072025\n",
            "(64, 66): reprojection: 0.349013, disparity: 0.073423\n",
            "(64, 68): reprojection: 0.427560, disparity: 0.087043\n",
            "(64, 72): reprojection: 0.680183, disparity: 0.097641\n",
            "(64, 80): reprojection: 0.944821, disparity: 0.121792\n",
            "(65, 66): reprojection: 0.277917, disparity: 0.062810\n",
            "(65, 67): reprojection: 0.420051, disparity: 0.071663\n",
            "(66, 67): reprojection: 0.255078, disparity: 0.067506\n",
            "(66, 68): reprojection: 0.378967, disparity: 0.073520\n",
            "(66, 70): reprojection: 0.477809, disparity: 0.086899\n",
            "(67, 68): reprojection: 0.348772, disparity: 0.067900\n",
            "(67, 69): reprojection: 0.375895, disparity: 0.069865\n",
            "(68, 69): reprojection: 0.205072, disparity: 0.066907\n",
            "(68, 70): reprojection: 0.453904, disparity: 0.074703\n",
            "(68, 72): reprojection: 0.480073, disparity: 0.083258\n",
            "(68, 76): reprojection: 0.647399, disparity: 0.095070\n",
            "(69, 70): reprojection: 0.320256, disparity: 0.064759\n",
            "(69, 71): reprojection: 0.471002, disparity: 0.068477\n",
            "(70, 71): reprojection: 0.218754, disparity: 0.064190\n",
            "(70, 72): reprojection: 0.266680, disparity: 0.065690\n",
            "(70, 74): reprojection: 0.383628, disparity: 0.078133\n",
            "(71, 72): reprojection: 0.236734, disparity: 0.065150\n",
            "(71, 73): reprojection: 0.360309, disparity: 0.070363\n",
            "(72, 73): reprojection: 0.231984, disparity: 0.062724\n",
            "(72, 74): reprojection: 0.295346, disparity: 0.069330\n",
            "(72, 76): reprojection: 0.439576, disparity: 0.073747\n",
            "(72, 80): reprojection: 0.554498, disparity: 0.090588\n",
            "(72, 88): reprojection: 0.882445, disparity: 0.102245\n",
            "(73, 74): reprojection: 0.315777, disparity: 0.063104\n",
            "(73, 75): reprojection: 0.425608, disparity: 0.073970\n",
            "(74, 75): reprojection: 0.234773, disparity: 0.063739\n",
            "(74, 76): reprojection: 0.258199, disparity: 0.068012\n",
            "(74, 78): reprojection: 0.353832, disparity: 0.073026\n",
            "(75, 76): reprojection: 0.232171, disparity: 0.061278\n",
            "(75, 77): reprojection: 0.353289, disparity: 0.067399\n",
            "(76, 77): reprojection: 0.224633, disparity: 0.060297\n",
            "(76, 78): reprojection: 0.319606, disparity: 0.061837\n",
            "(76, 80): reprojection: 0.395462, disparity: 0.073109\n",
            "(76, 84): reprojection: 0.593335, disparity: 0.079564\n",
            "(77, 78): reprojection: 0.220524, disparity: 0.060730\n",
            "(77, 79): reprojection: 0.258824, disparity: 0.064228\n",
            "(78, 79): reprojection: 0.198042, disparity: 0.061017\n",
            "(78, 80): reprojection: 0.252292, disparity: 0.067643\n",
            "(78, 82): reprojection: 0.403090, disparity: 0.070697\n",
            "(79, 80): reprojection: 0.145825, disparity: 0.061489\n",
            "(79, 81): reprojection: 0.240580, disparity: 0.063647\n",
            "(80, 81): reprojection: 0.150037, disparity: 0.057802\n",
            "(80, 82): reprojection: 0.262164, disparity: 0.062512\n",
            "(80, 84): reprojection: 0.465507, disparity: 0.063730\n",
            "(80, 88): reprojection: 0.703537, disparity: 0.081312\n",
            "(81, 82): reprojection: 0.185067, disparity: 0.058341\n",
            "(81, 83): reprojection: 0.309315, disparity: 0.062869\n",
            "(82, 83): reprojection: 0.156556, disparity: 0.056311\n",
            "(82, 84): reprojection: 0.252712, disparity: 0.061734\n",
            "(82, 86): reprojection: 0.465995, disparity: 0.068843\n",
            "(83, 84): reprojection: 0.147396, disparity: 0.056673\n",
            "(83, 85): reprojection: 0.303654, disparity: 0.066523\n",
            "(84, 85): reprojection: 0.170132, disparity: 0.056141\n",
            "(84, 86): reprojection: 0.263094, disparity: 0.061612\n",
            "(84, 88): reprojection: 0.497353, disparity: 0.068567\n",
            "(85, 86): reprojection: 0.170291, disparity: 0.056035\n",
            "(85, 87): reprojection: 0.285067, disparity: 0.059136\n",
            "(86, 87): reprojection: 0.237551, disparity: 0.058094\n",
            "(86, 88): reprojection: 0.334002, disparity: 0.066922\n",
            "(86, 90): reprojection: 0.480542, disparity: 0.067186\n",
            "(87, 88): reprojection: 0.179622, disparity: 0.056374\n",
            "(87, 89): reprojection: 0.275568, disparity: 0.061726\n",
            "(88, 89): reprojection: 0.138174, disparity: 0.052341\n",
            "(88, 90): reprojection: 0.306631, disparity: 0.058551\n",
            "(89, 90): reprojection: 0.213858, disparity: 0.050719\n",
            "(89, 91): reprojection: 0.419146, disparity: 0.066145\n",
            "(90, 91): reprojection: 0.338125, disparity: 0.060998\n",
            "Mean:     reprojection: 0.338125, disparity: 0.060998\n",
            "Done Validation for epoch 15 (3900 iterations)\n",
            "Epoch = 15, pairs = [[72, 74], [71, 72], [4, 5], [4, 8]], loss = 0.3179156184196472\n",
            "Epoch = 15, pairs = [[52, 54], [34, 36], [38, 40], [5, 6]], loss = 0.321584016084671\n",
            "Epoch = 15, pairs = [[25, 26], [6, 10], [24, 28], [11, 13]], loss = 0.31259703636169434\n",
            "Epoch = 15, pairs = [[28, 30], [42, 46], [51, 52], [70, 74]], loss = 0.43514764308929443\n",
            "Epoch = 15, pairs = [[0, 32], [85, 87], [36, 37], [54, 58]], loss = 0.7118918895721436\n",
            "Epoch = 15, pairs = [[20, 24], [12, 14], [50, 54], [39, 40]], loss = 0.3962036371231079\n",
            "Epoch = 15, pairs = [[16, 17], [77, 78], [6, 8], [0, 4]], loss = 0.27414533495903015\n",
            "Epoch = 15, pairs = [[80, 82], [9, 10], [57, 59], [86, 87]], loss = 0.4168165326118469\n",
            "Epoch = 15, pairs = [[24, 25], [81, 83], [16, 32], [83, 85]], loss = 0.8229573965072632\n",
            "Epoch = 15, pairs = [[16, 24], [84, 85], [78, 79], [40, 42]], loss = 0.5681332349777222\n",
            "Epoch = 15, pairs = [[42, 43], [89, 91], [75, 76], [32, 48]], loss = 0.5479758381843567\n",
            "Epoch = 15, pairs = [[22, 24], [67, 68], [7, 9], [69, 70]], loss = 0.30436819791793823\n",
            "Epoch = 15, pairs = [[54, 55], [60, 61], [48, 80], [83, 84]], loss = 0.8363224267959595\n",
            "Epoch = 15, pairs = [[13, 14], [33, 34], [64, 66], [43, 44]], loss = 0.30448973178863525\n",
            "Epoch = 15, pairs = [[87, 89], [14, 15], [76, 80], [24, 32]], loss = 0.4865623116493225\n",
            "Epoch = 15, pairs = [[16, 48], [1, 2], [61, 63], [88, 90]], loss = 0.7646631002426147\n",
            "Epoch = 15, pairs = [[71, 73], [65, 67], [15, 16], [2, 6]], loss = 0.4289672374725342\n",
            "Epoch = 15, pairs = [[31, 32], [22, 26], [79, 80], [7, 8]], loss = 0.25908684730529785\n",
            "Epoch = 15, pairs = [[28, 32], [68, 76], [76, 77], [90, 91]], loss = 0.5093340277671814\n",
            "Epoch = 15, pairs = [[19, 20], [4, 12], [86, 88], [53, 55]], loss = 0.3552972078323364\n",
            "Epoch = 15, pairs = [[26, 30], [63, 64], [14, 16], [38, 42]], loss = 0.49239861965179443\n",
            "Epoch = 15, pairs = [[64, 72], [34, 35], [30, 32], [41, 42]], loss = 0.40166714787483215\n",
            "Epoch = 15, pairs = [[25, 27], [62, 66], [52, 56], [29, 30]], loss = 0.4364868402481079\n",
            "Epoch = 15, pairs = [[29, 31], [9, 11], [3, 4], [40, 44]], loss = 0.3834872245788574\n",
            "Epoch = 15, pairs = [[46, 48], [64, 68], [59, 61], [82, 84]], loss = 0.43310245871543884\n",
            "Epoch = 15, pairs = [[72, 88], [8, 12], [85, 86], [68, 69]], loss = 0.5416078567504883\n",
            "Epoch = 15, pairs = [[37, 39], [46, 50], [36, 40], [15, 17]], loss = 0.43633532524108887\n",
            "Epoch = 15, pairs = [[44, 52], [8, 24], [18, 22], [26, 27]], loss = 0.6096539497375488\n",
            "Epoch = 15, pairs = [[47, 48], [34, 38], [70, 72], [73, 75]], loss = 0.3924277424812317\n",
            "Epoch = 15, pairs = [[78, 82], [56, 57], [17, 18], [27, 29]], loss = 0.37391066551208496\n",
            "Epoch = 15, pairs = [[72, 73], [0, 2], [78, 80], [32, 36]], loss = 0.3509315252304077\n",
            "Epoch = 15, pairs = [[58, 60], [58, 59], [18, 20], [56, 60]], loss = 0.49446454644203186\n",
            "Epoch = 15, pairs = [[48, 56], [48, 52], [56, 64], [52, 60]], loss = 0.7780362963676453\n",
            "Epoch = 15, pairs = [[72, 80], [80, 88], [51, 53], [1, 3]], loss = 0.5324698686599731\n",
            "Epoch = 15, pairs = [[33, 35], [70, 71], [47, 49], [20, 22]], loss = 0.32567617297172546\n",
            "Epoch = 15, pairs = [[37, 38], [14, 18], [8, 9], [80, 81]], loss = 0.28771212697029114\n",
            "Epoch = 15, pairs = [[50, 51], [48, 50], [55, 56], [31, 33]], loss = 0.36370158195495605\n",
            "Epoch = 15, pairs = [[62, 63], [19, 21], [57, 58], [39, 41]], loss = 0.4142395257949829\n",
            "Epoch = 15, pairs = [[84, 86], [84, 88], [76, 84], [20, 28]], loss = 0.5606516599655151\n",
            "Epoch = 15, pairs = [[44, 48], [32, 34], [4, 6], [72, 76]], loss = 0.4388974905014038\n",
            "Epoch = 15, pairs = [[69, 71], [28, 36], [63, 65], [45, 47]], loss = 0.5344708561897278\n",
            "Epoch = 15, pairs = [[26, 28], [2, 3], [40, 56], [3, 5]], loss = 0.4268346428871155\n",
            "Epoch = 15, pairs = [[74, 76], [30, 31], [61, 62], [27, 28]], loss = 0.2561202645301819\n",
            "Epoch = 15, pairs = [[64, 80], [64, 65], [0, 16], [0, 1]], loss = 0.7349770069122314\n",
            "Epoch = 15, pairs = [[45, 46], [21, 22], [68, 70], [82, 86]], loss = 0.381025493144989\n",
            "Epoch = 15, pairs = [[12, 16], [48, 49], [52, 53], [41, 43]], loss = 0.32537710666656494\n",
            "Epoch = 15, pairs = [[55, 57], [65, 66], [54, 56], [24, 26]], loss = 0.4129455089569092\n",
            "Epoch = 15, pairs = [[60, 64], [11, 12], [80, 84], [18, 19]], loss = 0.3699178993701935\n",
            "Epoch = 15, pairs = [[16, 18], [56, 72], [28, 29], [66, 70]], loss = 0.6594969630241394\n",
            "Epoch = 15, pairs = [[82, 83], [32, 64], [32, 33], [10, 14]], loss = 0.6427416801452637\n",
            "Epoch = 15, pairs = [[30, 34], [12, 13], [89, 90], [36, 38]], loss = 0.3272289037704468\n",
            "Epoch = 15, pairs = [[32, 40], [58, 62], [86, 90], [75, 77]], loss = 0.5480954647064209\n",
            "Epoch = 15, pairs = [[24, 40], [49, 51], [0, 8], [22, 23]], loss = 0.5351707339286804\n",
            "Epoch = 15, pairs = [[46, 47], [50, 52], [43, 45], [60, 68]], loss = 0.41763925552368164\n",
            "Epoch = 15, pairs = [[68, 72], [8, 10], [38, 39], [56, 58]], loss = 0.49555206298828125\n",
            "Epoch = 15, pairs = [[44, 46], [20, 21], [6, 7], [40, 41]], loss = 0.28352195024490356\n",
            "Epoch = 15, pairs = [[74, 78], [17, 19], [2, 4], [35, 36]], loss = 0.4094499349594116\n",
            "Epoch = 15, pairs = [[5, 7], [67, 69], [66, 68], [40, 48]], loss = 0.5057514905929565\n",
            "Epoch = 15, pairs = [[53, 54], [13, 15], [8, 16], [48, 64]], loss = 0.6674784421920776\n",
            "Epoch = 15, pairs = [[62, 64], [12, 20], [87, 88], [16, 20]], loss = 0.5130313634872437\n",
            "Epoch = 15, pairs = [[42, 44], [73, 74], [10, 11], [49, 50]], loss = 0.29579371213912964\n",
            "Epoch = 15, pairs = [[23, 25], [66, 67], [60, 62], [88, 89]], loss = 0.28992530703544617\n",
            "Epoch = 15, pairs = [[79, 81], [36, 44], [35, 37], [59, 60]], loss = 0.44881489872932434\n",
            "Epoch = 15, pairs = [[10, 12], [21, 23], [74, 75], [77, 79]], loss = 0.2890697121620178\n",
            "Epoch = 15, pairs = [[76, 78], [44, 45], [23, 24], [81, 82]], loss = 0.2649745047092438\n",
            "Epoch 15 took 84.22s.\n",
            "( 0,  1): reprojection: 0.234849, disparity: 0.049051\n",
            "( 0,  2): reprojection: 0.352071, disparity: 0.052459\n",
            "( 0,  4): reprojection: 0.409097, disparity: 0.060668\n",
            "( 0,  8): reprojection: 0.549702, disparity: 0.080747\n",
            "( 0, 16): reprojection: 0.969579, disparity: 0.092899\n",
            "( 0, 32): reprojection: 1.505007, disparity: 0.120381\n",
            "( 1,  2): reprojection: 0.197881, disparity: 0.044690\n",
            "( 1,  3): reprojection: 0.256792, disparity: 0.048281\n",
            "( 2,  3): reprojection: 0.113054, disparity: 0.042665\n",
            "( 2,  4): reprojection: 0.187305, disparity: 0.048117\n",
            "( 2,  6): reprojection: 0.362811, disparity: 0.058457\n",
            "( 3,  4): reprojection: 0.110705, disparity: 0.044102\n",
            "( 3,  5): reprojection: 0.208386, disparity: 0.048571\n",
            "( 4,  5): reprojection: 0.156424, disparity: 0.045832\n",
            "( 4,  6): reprojection: 0.224625, disparity: 0.048006\n",
            "( 4,  8): reprojection: 0.317550, disparity: 0.059651\n",
            "( 4, 12): reprojection: 0.391547, disparity: 0.071564\n",
            "( 5,  6): reprojection: 0.115392, disparity: 0.045221\n",
            "( 5,  7): reprojection: 0.170511, disparity: 0.048137\n",
            "( 6,  7): reprojection: 0.105134, disparity: 0.042830\n",
            "( 6,  8): reprojection: 0.180241, disparity: 0.049677\n",
            "( 6, 10): reprojection: 0.400560, disparity: 0.059115\n",
            "( 7,  8): reprojection: 0.094990, disparity: 0.044777\n",
            "( 7,  9): reprojection: 0.200270, disparity: 0.050644\n",
            "( 8,  9): reprojection: 0.159756, disparity: 0.045947\n",
            "( 8, 10): reprojection: 0.355849, disparity: 0.051572\n",
            "( 8, 12): reprojection: 0.484642, disparity: 0.059636\n",
            "( 8, 16): reprojection: 0.656155, disparity: 0.071827\n",
            "( 8, 24): reprojection: 0.963809, disparity: 0.081949\n",
            "( 9, 10): reprojection: 0.242543, disparity: 0.045504\n",
            "( 9, 11): reprojection: 0.358180, disparity: 0.050923\n",
            "(10, 11): reprojection: 0.152322, disparity: 0.042152\n",
            "(10, 12): reprojection: 0.196475, disparity: 0.047788\n",
            "(10, 14): reprojection: 0.351383, disparity: 0.062814\n",
            "(11, 12): reprojection: 0.138027, disparity: 0.041630\n",
            "(11, 13): reprojection: 0.227807, disparity: 0.050843\n",
            "(12, 13): reprojection: 0.134025, disparity: 0.042725\n",
            "(12, 14): reprojection: 0.258543, disparity: 0.049871\n",
            "(12, 16): reprojection: 0.494326, disparity: 0.068667\n",
            "(12, 20): reprojection: 0.795015, disparity: 0.070889\n",
            "(13, 14): reprojection: 0.178441, disparity: 0.044132\n",
            "(13, 15): reprojection: 0.255843, disparity: 0.052221\n",
            "(14, 15): reprojection: 0.203666, disparity: 0.046486\n",
            "(14, 16): reprojection: 0.449526, disparity: 0.054893\n",
            "(14, 18): reprojection: 0.412597, disparity: 0.061997\n",
            "(15, 16): reprojection: 0.303577, disparity: 0.047278\n",
            "(15, 17): reprojection: 0.396044, disparity: 0.054298\n",
            "(16, 17): reprojection: 0.147395, disparity: 0.045931\n",
            "(16, 18): reprojection: 0.313879, disparity: 0.052784\n",
            "(16, 20): reprojection: 0.592686, disparity: 0.061797\n",
            "(16, 24): reprojection: 0.872833, disparity: 0.072899\n",
            "(16, 32): reprojection: 1.602606, disparity: 0.092662\n",
            "(16, 48): reprojection: 1.351394, disparity: 0.167896\n",
            "(17, 18): reprojection: 0.290550, disparity: 0.047823\n",
            "(17, 19): reprojection: 0.464223, disparity: 0.055851\n",
            "(18, 19): reprojection: 0.217462, disparity: 0.045887\n",
            "(18, 20): reprojection: 0.316702, disparity: 0.054694\n",
            "(18, 22): reprojection: 0.522310, disparity: 0.059389\n",
            "(19, 20): reprojection: 0.135179, disparity: 0.045981\n",
            "(19, 21): reprojection: 0.268381, disparity: 0.053956\n",
            "(20, 21): reprojection: 0.180872, disparity: 0.046731\n",
            "(20, 22): reprojection: 0.281234, disparity: 0.055032\n",
            "(20, 24): reprojection: 0.436878, disparity: 0.062405\n",
            "(20, 28): reprojection: 0.858259, disparity: 0.079025\n",
            "(21, 22): reprojection: 0.159166, disparity: 0.048264\n",
            "(21, 23): reprojection: 0.255713, disparity: 0.056264\n",
            "(22, 23): reprojection: 0.133982, disparity: 0.048097\n",
            "(22, 24): reprojection: 0.221849, disparity: 0.057608\n",
            "(22, 26): reprojection: 0.413617, disparity: 0.069382\n",
            "(23, 24): reprojection: 0.124743, disparity: 0.048776\n",
            "(23, 25): reprojection: 0.202808, disparity: 0.057252\n",
            "(24, 25): reprojection: 0.127306, disparity: 0.050073\n",
            "(24, 26): reprojection: 0.213226, disparity: 0.059028\n",
            "(24, 28): reprojection: 0.439077, disparity: 0.069612\n",
            "(24, 32): reprojection: 0.899277, disparity: 0.084279\n",
            "(24, 40): reprojection: 1.299827, disparity: 0.128326\n",
            "(25, 26): reprojection: 0.144900, disparity: 0.053549\n",
            "(25, 27): reprojection: 0.275438, disparity: 0.061428\n",
            "(26, 27): reprojection: 0.166476, disparity: 0.052310\n",
            "(26, 28): reprojection: 0.230088, disparity: 0.059710\n",
            "(26, 30): reprojection: 0.423644, disparity: 0.067159\n",
            "(27, 28): reprojection: 0.137546, disparity: 0.053162\n",
            "(27, 29): reprojection: 0.258531, disparity: 0.056667\n",
            "(28, 29): reprojection: 0.166041, disparity: 0.052326\n",
            "(28, 30): reprojection: 0.258665, disparity: 0.059529\n",
            "(28, 32): reprojection: 0.531012, disparity: 0.068012\n",
            "(28, 36): reprojection: 0.911633, disparity: 0.084272\n",
            "(29, 30): reprojection: 0.140000, disparity: 0.050098\n",
            "(29, 31): reprojection: 0.321254, disparity: 0.054671\n",
            "(30, 31): reprojection: 0.211213, disparity: 0.050485\n",
            "(30, 32): reprojection: 0.310087, disparity: 0.058701\n",
            "(30, 34): reprojection: 0.550209, disparity: 0.076129\n",
            "(31, 32): reprojection: 0.140341, disparity: 0.050717\n",
            "(31, 33): reprojection: 0.340985, disparity: 0.059501\n",
            "(32, 33): reprojection: 0.253167, disparity: 0.054757\n",
            "(32, 34): reprojection: 0.287903, disparity: 0.058316\n",
            "(32, 36): reprojection: 0.446554, disparity: 0.067914\n",
            "(32, 40): reprojection: 0.712009, disparity: 0.092346\n",
            "(32, 48): reprojection: 1.185749, disparity: 0.123604\n",
            "(32, 64): reprojection: 1.184382, disparity: 0.143372\n",
            "(33, 34): reprojection: 0.139571, disparity: 0.050838\n",
            "(33, 35): reprojection: 0.278338, disparity: 0.057334\n",
            "(34, 35): reprojection: 0.188562, disparity: 0.050430\n",
            "(34, 36): reprojection: 0.283585, disparity: 0.057486\n",
            "(34, 38): reprojection: 0.467416, disparity: 0.070684\n",
            "(35, 36): reprojection: 0.219434, disparity: 0.052739\n",
            "(35, 37): reprojection: 0.320333, disparity: 0.061952\n",
            "(36, 37): reprojection: 0.278691, disparity: 0.055863\n",
            "(36, 38): reprojection: 0.420918, disparity: 0.061215\n",
            "(36, 40): reprojection: 0.370173, disparity: 0.070874\n",
            "(36, 44): reprojection: 0.713133, disparity: 0.094373\n",
            "(37, 38): reprojection: 0.228095, disparity: 0.052200\n",
            "(37, 39): reprojection: 0.266605, disparity: 0.058362\n",
            "(38, 39): reprojection: 0.180554, disparity: 0.053013\n",
            "(38, 40): reprojection: 0.335873, disparity: 0.065158\n",
            "(38, 42): reprojection: 0.573585, disparity: 0.079726\n",
            "(39, 40): reprojection: 0.260161, disparity: 0.055362\n",
            "(39, 41): reprojection: 0.448744, disparity: 0.063800\n",
            "(40, 41): reprojection: 0.263320, disparity: 0.055782\n",
            "(40, 42): reprojection: 0.383065, disparity: 0.063943\n",
            "(40, 44): reprojection: 0.505348, disparity: 0.074552\n",
            "(40, 48): reprojection: 0.822099, disparity: 0.094030\n",
            "(40, 56): reprojection: 1.057670, disparity: 0.113876\n",
            "(41, 42): reprojection: 0.186578, disparity: 0.056594\n",
            "(41, 43): reprojection: 0.289226, disparity: 0.064823\n",
            "(42, 43): reprojection: 0.168949, disparity: 0.056147\n",
            "(42, 44): reprojection: 0.317339, disparity: 0.064936\n",
            "(42, 46): reprojection: 0.553784, disparity: 0.075446\n",
            "(43, 44): reprojection: 0.196886, disparity: 0.055918\n",
            "(43, 45): reprojection: 0.314573, disparity: 0.064419\n",
            "(44, 45): reprojection: 0.153401, disparity: 0.054900\n",
            "(44, 46): reprojection: 0.301914, disparity: 0.065942\n",
            "(44, 48): reprojection: 0.504242, disparity: 0.083623\n",
            "(44, 52): reprojection: 0.793056, disparity: 0.110453\n",
            "(45, 46): reprojection: 0.209974, disparity: 0.058867\n",
            "(45, 47): reprojection: 0.343016, disparity: 0.069125\n",
            "(46, 47): reprojection: 0.184154, disparity: 0.056655\n",
            "(46, 48): reprojection: 0.309592, disparity: 0.068273\n",
            "(46, 50): reprojection: 0.442553, disparity: 0.080326\n",
            "(47, 48): reprojection: 0.213211, disparity: 0.056507\n",
            "(47, 49): reprojection: 0.288752, disparity: 0.064169\n",
            "(48, 49): reprojection: 0.166649, disparity: 0.059896\n",
            "(48, 50): reprojection: 0.281664, disparity: 0.072300\n",
            "(48, 52): reprojection: 0.443516, disparity: 0.084788\n",
            "(48, 56): reprojection: 0.939516, disparity: 0.096339\n",
            "(48, 64): reprojection: 1.230538, disparity: 0.109226\n",
            "(48, 80): reprojection: 1.127015, disparity: 0.111140\n",
            "(49, 50): reprojection: 0.155633, disparity: 0.059950\n",
            "(49, 51): reprojection: 0.281505, disparity: 0.065456\n",
            "(50, 51): reprojection: 0.180184, disparity: 0.058797\n",
            "(50, 52): reprojection: 0.290500, disparity: 0.063839\n",
            "(50, 54): reprojection: 0.417304, disparity: 0.072952\n",
            "(51, 52): reprojection: 0.159668, disparity: 0.057818\n",
            "(51, 53): reprojection: 0.304845, disparity: 0.061676\n",
            "(52, 53): reprojection: 0.171504, disparity: 0.058019\n",
            "(52, 54): reprojection: 0.252275, disparity: 0.064172\n",
            "(52, 56): reprojection: 0.586675, disparity: 0.067774\n",
            "(52, 60): reprojection: 0.825373, disparity: 0.099795\n",
            "(53, 54): reprojection: 0.120006, disparity: 0.058371\n",
            "(53, 55): reprojection: 0.341121, disparity: 0.065079\n",
            "(54, 55): reprojection: 0.300154, disparity: 0.058658\n",
            "(54, 56): reprojection: 0.574227, disparity: 0.064143\n",
            "(54, 58): reprojection: 0.479539, disparity: 0.068914\n",
            "(55, 56): reprojection: 0.343692, disparity: 0.060953\n",
            "(55, 57): reprojection: 0.421808, disparity: 0.067766\n",
            "(56, 57): reprojection: 0.290236, disparity: 0.063027\n",
            "(56, 58): reprojection: 0.525486, disparity: 0.070205\n",
            "(56, 60): reprojection: 0.710091, disparity: 0.106038\n",
            "(56, 64): reprojection: 1.008066, disparity: 0.095729\n",
            "(56, 72): reprojection: 1.062757, disparity: 0.144668\n",
            "(57, 58): reprojection: 0.335697, disparity: 0.065231\n",
            "(57, 59): reprojection: 0.463638, disparity: 0.074172\n",
            "(58, 59): reprojection: 0.241950, disparity: 0.067914\n",
            "(58, 60): reprojection: 0.393637, disparity: 0.082667\n",
            "(58, 62): reprojection: 0.542074, disparity: 0.093563\n",
            "(59, 60): reprojection: 0.198874, disparity: 0.074376\n",
            "(59, 61): reprojection: 0.357295, disparity: 0.075321\n",
            "(60, 61): reprojection: 0.234491, disparity: 0.069965\n",
            "(60, 62): reprojection: 0.349221, disparity: 0.077219\n",
            "(60, 64): reprojection: 0.538900, disparity: 0.087964\n",
            "(60, 68): reprojection: 0.658825, disparity: 0.103230\n",
            "(61, 62): reprojection: 0.199884, disparity: 0.071178\n",
            "(61, 63): reprojection: 0.281257, disparity: 0.076680\n",
            "(62, 63): reprojection: 0.208186, disparity: 0.067720\n",
            "(62, 64): reprojection: 0.300883, disparity: 0.074127\n",
            "(62, 66): reprojection: 0.526023, disparity: 0.095023\n",
            "(63, 64): reprojection: 0.190983, disparity: 0.065023\n",
            "(63, 65): reprojection: 0.306781, disparity: 0.071514\n",
            "(64, 65): reprojection: 0.190949, disparity: 0.070144\n",
            "(64, 66): reprojection: 0.350674, disparity: 0.077179\n",
            "(64, 68): reprojection: 0.493848, disparity: 0.095839\n",
            "(64, 72): reprojection: 0.928612, disparity: 0.096841\n",
            "(64, 80): reprojection: 1.094011, disparity: 0.112000\n",
            "(65, 66): reprojection: 0.254124, disparity: 0.065381\n",
            "(65, 67): reprojection: 0.376269, disparity: 0.073706\n",
            "(66, 67): reprojection: 0.234357, disparity: 0.067455\n",
            "(66, 68): reprojection: 0.336986, disparity: 0.069179\n",
            "(66, 70): reprojection: 0.479837, disparity: 0.079620\n",
            "(67, 68): reprojection: 0.313483, disparity: 0.064178\n",
            "(67, 69): reprojection: 0.325221, disparity: 0.067621\n",
            "(68, 69): reprojection: 0.210707, disparity: 0.063355\n",
            "(68, 70): reprojection: 0.447935, disparity: 0.069674\n",
            "(68, 72): reprojection: 0.590925, disparity: 0.075369\n",
            "(68, 76): reprojection: 0.793302, disparity: 0.083608\n",
            "(69, 70): reprojection: 0.291815, disparity: 0.062561\n",
            "(69, 71): reprojection: 0.425527, disparity: 0.065375\n",
            "(70, 71): reprojection: 0.188366, disparity: 0.061084\n",
            "(70, 72): reprojection: 0.305432, disparity: 0.064751\n",
            "(70, 74): reprojection: 0.468897, disparity: 0.073888\n",
            "(71, 72): reprojection: 0.223869, disparity: 0.062820\n",
            "(71, 73): reprojection: 0.350581, disparity: 0.068115\n",
            "(72, 73): reprojection: 0.219878, disparity: 0.062361\n",
            "(72, 74): reprojection: 0.307489, disparity: 0.067380\n",
            "(72, 76): reprojection: 0.469394, disparity: 0.071989\n",
            "(72, 80): reprojection: 0.786039, disparity: 0.088810\n",
            "(72, 88): reprojection: 1.006252, disparity: 0.101527\n",
            "(73, 74): reprojection: 0.315688, disparity: 0.060251\n",
            "(73, 75): reprojection: 0.441181, disparity: 0.065501\n",
            "(74, 75): reprojection: 0.241846, disparity: 0.059771\n",
            "(74, 76): reprojection: 0.270964, disparity: 0.063552\n",
            "(74, 78): reprojection: 0.428523, disparity: 0.069675\n",
            "(75, 76): reprojection: 0.212447, disparity: 0.059637\n",
            "(75, 77): reprojection: 0.336774, disparity: 0.065277\n",
            "(76, 77): reprojection: 0.220766, disparity: 0.058456\n",
            "(76, 78): reprojection: 0.346258, disparity: 0.061230\n",
            "(76, 80): reprojection: 0.492691, disparity: 0.072797\n",
            "(76, 84): reprojection: 0.835394, disparity: 0.090985\n",
            "(77, 78): reprojection: 0.221826, disparity: 0.058620\n",
            "(77, 79): reprojection: 0.274285, disparity: 0.062392\n",
            "(78, 79): reprojection: 0.207933, disparity: 0.059474\n",
            "(78, 80): reprojection: 0.263818, disparity: 0.064836\n",
            "(78, 82): reprojection: 0.444108, disparity: 0.075893\n",
            "(79, 80): reprojection: 0.143600, disparity: 0.059063\n",
            "(79, 81): reprojection: 0.245887, disparity: 0.064344\n",
            "(80, 81): reprojection: 0.150967, disparity: 0.059379\n",
            "(80, 82): reprojection: 0.308750, disparity: 0.068572\n",
            "(80, 84): reprojection: 0.534958, disparity: 0.074281\n",
            "(80, 88): reprojection: 0.774338, disparity: 0.082597\n",
            "(81, 82): reprojection: 0.197767, disparity: 0.057648\n",
            "(81, 83): reprojection: 0.310590, disparity: 0.064629\n",
            "(82, 83): reprojection: 0.152549, disparity: 0.056063\n",
            "(82, 84): reprojection: 0.291260, disparity: 0.062350\n",
            "(82, 86): reprojection: 0.469180, disparity: 0.071621\n",
            "(83, 84): reprojection: 0.157521, disparity: 0.054978\n",
            "(83, 85): reprojection: 0.315795, disparity: 0.062899\n",
            "(84, 85): reprojection: 0.175105, disparity: 0.053809\n",
            "(84, 86): reprojection: 0.251510, disparity: 0.059762\n",
            "(84, 88): reprojection: 0.469208, disparity: 0.064505\n",
            "(85, 86): reprojection: 0.158539, disparity: 0.052673\n",
            "(85, 87): reprojection: 0.272495, disparity: 0.057642\n",
            "(86, 87): reprojection: 0.231149, disparity: 0.057121\n",
            "(86, 88): reprojection: 0.317017, disparity: 0.062042\n",
            "(86, 90): reprojection: 0.504874, disparity: 0.064542\n",
            "(87, 88): reprojection: 0.169601, disparity: 0.054958\n",
            "(87, 89): reprojection: 0.282057, disparity: 0.060723\n",
            "(88, 89): reprojection: 0.147558, disparity: 0.051939\n",
            "(88, 90): reprojection: 0.314710, disparity: 0.056780\n",
            "(89, 90): reprojection: 0.213358, disparity: 0.048747\n",
            "(89, 91): reprojection: 0.400408, disparity: 0.061133\n",
            "(90, 91): reprojection: 0.317563, disparity: 0.056622\n",
            "Mean:     reprojection: 0.317563, disparity: 0.056622\n",
            "Done Validation for epoch 16 (4160 iterations)\n",
            "Epoch = 16, pairs = [[12, 14], [52, 56], [46, 50], [17, 18]], loss = 0.4622771441936493\n",
            "Epoch = 16, pairs = [[82, 83], [11, 13], [56, 60], [16, 18]], loss = 0.42422565817832947\n",
            "Epoch = 16, pairs = [[54, 55], [86, 88], [70, 74], [44, 48]], loss = 0.46435844898223877\n",
            "Epoch = 16, pairs = [[48, 49], [13, 15], [20, 24], [79, 80]], loss = 0.2895582318305969\n",
            "Epoch = 16, pairs = [[29, 30], [55, 56], [8, 16], [64, 80]], loss = 0.580562174320221\n",
            "Epoch = 16, pairs = [[72, 80], [79, 81], [89, 91], [36, 38]], loss = 0.4426186978816986\n",
            "Epoch = 16, pairs = [[76, 77], [57, 58], [10, 14], [76, 84]], loss = 0.4041319191455841\n",
            "Epoch = 16, pairs = [[16, 24], [60, 64], [50, 54], [53, 54]], loss = 0.5329449772834778\n",
            "Epoch = 16, pairs = [[65, 67], [49, 50], [38, 42], [8, 10]], loss = 0.48167675733566284\n",
            "Epoch = 16, pairs = [[34, 36], [30, 34], [37, 39], [2, 6]], loss = 0.40036651492118835\n",
            "Epoch = 16, pairs = [[8, 9], [62, 63], [77, 78], [0, 8]], loss = 0.3354566693305969\n",
            "Epoch = 16, pairs = [[48, 64], [23, 24], [58, 60], [61, 63]], loss = 0.5407386422157288\n",
            "Epoch = 16, pairs = [[0, 16], [59, 60], [49, 51], [63, 65]], loss = 0.4379626214504242\n",
            "Epoch = 16, pairs = [[44, 45], [19, 20], [6, 8], [12, 20]], loss = 0.32779330015182495\n",
            "Epoch = 16, pairs = [[82, 84], [73, 74], [6, 7], [24, 26]], loss = 0.26760828495025635\n",
            "Epoch = 16, pairs = [[80, 81], [22, 23], [32, 40], [52, 54]], loss = 0.3063938617706299\n",
            "Epoch = 16, pairs = [[58, 59], [18, 22], [32, 64], [16, 20]], loss = 0.6916301250457764\n",
            "Epoch = 16, pairs = [[26, 30], [50, 51], [51, 53], [88, 89]], loss = 0.28540757298469543\n",
            "Epoch = 16, pairs = [[52, 53], [48, 56], [85, 87], [76, 78]], loss = 0.4287627041339874\n",
            "Epoch = 16, pairs = [[45, 47], [47, 49], [43, 44], [82, 86]], loss = 0.3806686997413635\n",
            "Epoch = 16, pairs = [[24, 25], [46, 48], [28, 32], [20, 21]], loss = 0.34357285499572754\n",
            "Epoch = 16, pairs = [[25, 26], [12, 13], [27, 28], [17, 19]], loss = 0.27498292922973633\n",
            "Epoch = 16, pairs = [[24, 28], [28, 30], [0, 32], [64, 72]], loss = 0.762035608291626\n",
            "Epoch = 16, pairs = [[83, 84], [69, 70], [48, 50], [42, 46]], loss = 0.3771395683288574\n",
            "Epoch = 16, pairs = [[47, 48], [3, 5], [14, 16], [68, 76]], loss = 0.45886027812957764\n",
            "Epoch = 16, pairs = [[60, 68], [28, 29], [87, 89], [84, 88]], loss = 0.6149696707725525\n",
            "Epoch = 16, pairs = [[70, 71], [84, 86], [62, 64], [39, 40]], loss = 0.3720620274543762\n",
            "Epoch = 16, pairs = [[10, 11], [16, 17], [39, 41], [12, 16]], loss = 0.3649437725543976\n",
            "Epoch = 16, pairs = [[67, 69], [67, 68], [40, 56], [78, 79]], loss = 0.5208936929702759\n",
            "Epoch = 16, pairs = [[35, 37], [73, 75], [33, 34], [4, 6]], loss = 0.32144278287887573\n",
            "Epoch = 16, pairs = [[4, 12], [44, 46], [18, 19], [24, 40]], loss = 0.6688418984413147\n",
            "Epoch = 16, pairs = [[56, 58], [80, 88], [24, 32], [30, 32]], loss = 0.6116994023323059\n",
            "Epoch = 16, pairs = [[11, 12], [31, 32], [5, 6], [83, 85]], loss = 0.2189057320356369\n",
            "Epoch = 16, pairs = [[66, 67], [85, 86], [88, 90], [78, 82]], loss = 0.3419923186302185\n",
            "Epoch = 16, pairs = [[8, 24], [14, 15], [72, 73], [86, 90]], loss = 0.5140845775604248\n",
            "Epoch = 16, pairs = [[20, 22], [28, 36], [21, 22], [54, 58]], loss = 0.43821975588798523\n",
            "Epoch = 16, pairs = [[20, 28], [66, 68], [9, 11], [72, 88]], loss = 0.5578828454017639\n",
            "Epoch = 16, pairs = [[36, 40], [69, 71], [74, 76], [36, 37]], loss = 0.3637559413909912\n",
            "Epoch = 16, pairs = [[74, 75], [60, 62], [66, 70], [19, 21]], loss = 0.37222105264663696\n",
            "Epoch = 16, pairs = [[16, 32], [50, 52], [0, 2], [68, 70]], loss = 0.5676085948944092\n",
            "Epoch = 16, pairs = [[72, 76], [26, 27], [30, 31], [15, 16]], loss = 0.29487675428390503\n",
            "Epoch = 16, pairs = [[35, 36], [38, 39], [32, 36], [33, 35]], loss = 0.3190893232822418\n",
            "Epoch = 16, pairs = [[71, 73], [56, 72], [70, 72], [84, 85]], loss = 0.5009987950325012\n",
            "Epoch = 16, pairs = [[38, 40], [31, 33], [61, 62], [53, 55]], loss = 0.3634841740131378\n",
            "Epoch = 16, pairs = [[57, 59], [81, 82], [42, 44], [4, 8]], loss = 0.3775402307510376\n",
            "Epoch = 16, pairs = [[32, 48], [7, 8], [32, 33], [51, 52]], loss = 0.4160677492618561\n",
            "Epoch = 16, pairs = [[5, 7], [64, 65], [1, 3], [43, 45]], loss = 0.2849235534667969\n",
            "Epoch = 16, pairs = [[64, 66], [48, 52], [55, 57], [34, 35]], loss = 0.3801712393760681\n",
            "Epoch = 16, pairs = [[6, 10], [80, 82], [58, 62], [34, 38]], loss = 0.43374061584472656\n",
            "Epoch = 16, pairs = [[46, 47], [54, 56], [1, 2], [48, 80]], loss = 0.6102995872497559\n",
            "Epoch = 16, pairs = [[74, 78], [9, 10], [75, 76], [10, 12]], loss = 0.30946415662765503\n",
            "Epoch = 16, pairs = [[40, 48], [64, 68], [3, 4], [63, 64]], loss = 0.45408087968826294\n",
            "Epoch = 16, pairs = [[87, 88], [68, 69], [8, 12], [22, 24]], loss = 0.3152827024459839\n",
            "Epoch = 16, pairs = [[13, 14], [41, 43], [15, 17], [37, 38]], loss = 0.3319116532802582\n",
            "Epoch = 16, pairs = [[81, 83], [62, 66], [71, 72], [40, 42]], loss = 0.4325981140136719\n",
            "Epoch = 16, pairs = [[0, 4], [60, 61], [89, 90], [0, 1]], loss = 0.3148579001426697\n",
            "Epoch = 16, pairs = [[59, 61], [45, 46], [7, 9], [22, 26]], loss = 0.35385558009147644\n",
            "Epoch = 16, pairs = [[2, 4], [27, 29], [42, 43], [23, 25]], loss = 0.2613915503025055\n",
            "Epoch = 16, pairs = [[29, 31], [72, 74], [56, 57], [40, 41]], loss = 0.3547206521034241\n",
            "Epoch = 16, pairs = [[26, 28], [2, 3], [52, 60], [4, 5]], loss = 0.33405643701553345\n",
            "Epoch = 16, pairs = [[36, 44], [86, 87], [21, 23], [32, 34]], loss = 0.4098220467567444\n",
            "Epoch = 16, pairs = [[44, 52], [18, 20], [41, 42], [90, 91]], loss = 0.4075532853603363\n",
            "Epoch = 16, pairs = [[77, 79], [16, 48], [25, 27], [68, 72]], loss = 0.7715560793876648\n",
            "Epoch = 16, pairs = [[78, 80], [80, 84], [40, 44], [75, 77]], loss = 0.46371668577194214\n",
            "Epoch = 16, pairs = [[76, 80], [65, 66], [14, 18], [56, 64]], loss = 0.5149219632148743\n",
            "Epoch 16 took 85.04s.\n",
            "( 0,  1): reprojection: 0.232559, disparity: 0.047800\n",
            "( 0,  2): reprojection: 0.342698, disparity: 0.048155\n",
            "( 0,  4): reprojection: 0.370223, disparity: 0.055543\n",
            "( 0,  8): reprojection: 0.441492, disparity: 0.065542\n",
            "( 0, 16): reprojection: 0.665560, disparity: 0.076908\n",
            "( 0, 32): reprojection: 1.060078, disparity: 0.093620\n",
            "( 1,  2): reprojection: 0.192377, disparity: 0.043262\n",
            "( 1,  3): reprojection: 0.243728, disparity: 0.045950\n",
            "( 2,  3): reprojection: 0.107269, disparity: 0.041069\n",
            "( 2,  4): reprojection: 0.170963, disparity: 0.046130\n",
            "( 2,  6): reprojection: 0.340798, disparity: 0.053765\n",
            "( 3,  4): reprojection: 0.106355, disparity: 0.042738\n",
            "( 3,  5): reprojection: 0.194773, disparity: 0.047039\n",
            "( 4,  5): reprojection: 0.154804, disparity: 0.043693\n",
            "( 4,  6): reprojection: 0.216528, disparity: 0.043666\n",
            "( 4,  8): reprojection: 0.270425, disparity: 0.053557\n",
            "( 4, 12): reprojection: 0.384865, disparity: 0.061495\n",
            "( 5,  6): reprojection: 0.119667, disparity: 0.042251\n",
            "( 5,  7): reprojection: 0.150646, disparity: 0.045165\n",
            "( 6,  7): reprojection: 0.093253, disparity: 0.041989\n",
            "( 6,  8): reprojection: 0.145833, disparity: 0.046189\n",
            "( 6, 10): reprojection: 0.374157, disparity: 0.053951\n",
            "( 7,  8): reprojection: 0.084330, disparity: 0.042157\n",
            "( 7,  9): reprojection: 0.185211, disparity: 0.047217\n",
            "( 8,  9): reprojection: 0.161607, disparity: 0.044695\n",
            "( 8, 10): reprojection: 0.364306, disparity: 0.048562\n",
            "( 8, 12): reprojection: 0.495571, disparity: 0.054966\n",
            "( 8, 16): reprojection: 0.636601, disparity: 0.067910\n",
            "( 8, 24): reprojection: 0.765844, disparity: 0.077338\n",
            "( 9, 10): reprojection: 0.255934, disparity: 0.043611\n",
            "( 9, 11): reprojection: 0.382999, disparity: 0.049018\n",
            "(10, 11): reprojection: 0.168143, disparity: 0.040731\n",
            "(10, 12): reprojection: 0.213437, disparity: 0.046862\n",
            "(10, 14): reprojection: 0.342746, disparity: 0.058782\n",
            "(11, 12): reprojection: 0.138868, disparity: 0.041382\n",
            "(11, 13): reprojection: 0.221646, disparity: 0.051034\n",
            "(12, 13): reprojection: 0.139942, disparity: 0.042641\n",
            "(12, 14): reprojection: 0.271986, disparity: 0.049436\n",
            "(12, 16): reprojection: 0.381834, disparity: 0.069211\n",
            "(12, 20): reprojection: 0.633419, disparity: 0.070786\n",
            "(13, 14): reprojection: 0.185004, disparity: 0.043573\n",
            "(13, 15): reprojection: 0.212210, disparity: 0.051852\n",
            "(14, 15): reprojection: 0.179721, disparity: 0.044955\n",
            "(14, 16): reprojection: 0.412880, disparity: 0.053179\n",
            "(14, 18): reprojection: 0.328650, disparity: 0.060889\n",
            "(15, 16): reprojection: 0.289515, disparity: 0.045634\n",
            "(15, 17): reprojection: 0.365780, disparity: 0.053220\n",
            "(16, 17): reprojection: 0.134458, disparity: 0.045492\n",
            "(16, 18): reprojection: 0.327832, disparity: 0.054828\n",
            "(16, 20): reprojection: 0.561137, disparity: 0.064233\n",
            "(16, 24): reprojection: 0.812746, disparity: 0.081853\n",
            "(16, 32): reprojection: 1.156041, disparity: 0.101038\n",
            "(16, 48): reprojection: 1.230755, disparity: 0.168518\n",
            "(17, 18): reprojection: 0.331727, disparity: 0.048335\n",
            "(17, 19): reprojection: 0.500157, disparity: 0.056931\n",
            "(18, 19): reprojection: 0.213732, disparity: 0.045731\n",
            "(18, 20): reprojection: 0.286455, disparity: 0.052314\n",
            "(18, 22): reprojection: 0.486281, disparity: 0.060753\n",
            "(19, 20): reprojection: 0.107085, disparity: 0.044983\n",
            "(19, 21): reprojection: 0.247731, disparity: 0.052426\n",
            "(20, 21): reprojection: 0.197112, disparity: 0.046379\n",
            "(20, 22): reprojection: 0.292355, disparity: 0.054655\n",
            "(20, 24): reprojection: 0.337085, disparity: 0.062889\n",
            "(20, 28): reprojection: 0.485984, disparity: 0.088785\n",
            "(21, 22): reprojection: 0.152806, disparity: 0.047686\n",
            "(21, 23): reprojection: 0.210526, disparity: 0.056630\n",
            "(22, 23): reprojection: 0.116647, disparity: 0.047416\n",
            "(22, 24): reprojection: 0.143118, disparity: 0.055485\n",
            "(22, 26): reprojection: 0.279584, disparity: 0.064228\n",
            "(23, 24): reprojection: 0.097688, disparity: 0.047614\n",
            "(23, 25): reprojection: 0.174688, disparity: 0.055060\n",
            "(24, 25): reprojection: 0.132918, disparity: 0.050121\n",
            "(24, 26): reprojection: 0.179504, disparity: 0.057119\n",
            "(24, 28): reprojection: 0.278075, disparity: 0.066125\n",
            "(24, 32): reprojection: 0.582874, disparity: 0.079216\n",
            "(24, 40): reprojection: 0.871714, disparity: 0.117743\n",
            "(25, 26): reprojection: 0.118327, disparity: 0.051412\n",
            "(25, 27): reprojection: 0.195834, disparity: 0.058465\n",
            "(26, 27): reprojection: 0.132808, disparity: 0.050207\n",
            "(26, 28): reprojection: 0.160330, disparity: 0.055928\n",
            "(26, 30): reprojection: 0.319720, disparity: 0.064708\n",
            "(27, 28): reprojection: 0.146842, disparity: 0.050735\n",
            "(27, 29): reprojection: 0.263065, disparity: 0.056644\n",
            "(28, 29): reprojection: 0.189406, disparity: 0.051708\n",
            "(28, 30): reprojection: 0.246215, disparity: 0.057284\n",
            "(28, 32): reprojection: 0.397483, disparity: 0.065647\n",
            "(28, 36): reprojection: 0.611049, disparity: 0.081586\n",
            "(29, 30): reprojection: 0.135864, disparity: 0.048477\n",
            "(29, 31): reprojection: 0.258455, disparity: 0.054894\n",
            "(30, 31): reprojection: 0.193715, disparity: 0.048834\n",
            "(30, 32): reprojection: 0.251080, disparity: 0.057271\n",
            "(30, 34): reprojection: 0.412798, disparity: 0.070292\n",
            "(31, 32): reprojection: 0.127122, disparity: 0.049833\n",
            "(31, 33): reprojection: 0.287608, disparity: 0.055759\n",
            "(32, 33): reprojection: 0.237069, disparity: 0.051006\n",
            "(32, 34): reprojection: 0.234893, disparity: 0.056053\n",
            "(32, 36): reprojection: 0.325016, disparity: 0.065516\n",
            "(32, 40): reprojection: 0.450421, disparity: 0.087923\n",
            "(32, 48): reprojection: 0.886989, disparity: 0.116121\n",
            "(32, 64): reprojection: 1.227673, disparity: 0.190287\n",
            "(33, 34): reprojection: 0.161459, disparity: 0.048839\n",
            "(33, 35): reprojection: 0.256582, disparity: 0.053424\n",
            "(34, 35): reprojection: 0.188963, disparity: 0.048034\n",
            "(34, 36): reprojection: 0.273652, disparity: 0.057307\n",
            "(34, 38): reprojection: 0.330983, disparity: 0.068150\n",
            "(35, 36): reprojection: 0.234089, disparity: 0.052592\n",
            "(35, 37): reprojection: 0.253950, disparity: 0.061402\n",
            "(36, 37): reprojection: 0.242136, disparity: 0.057880\n",
            "(36, 38): reprojection: 0.372452, disparity: 0.059950\n",
            "(36, 40): reprojection: 0.333572, disparity: 0.069218\n",
            "(36, 44): reprojection: 0.665118, disparity: 0.089196\n",
            "(37, 38): reprojection: 0.223155, disparity: 0.052580\n",
            "(37, 39): reprojection: 0.238979, disparity: 0.059380\n",
            "(38, 39): reprojection: 0.196618, disparity: 0.054913\n",
            "(38, 40): reprojection: 0.386362, disparity: 0.063549\n",
            "(38, 42): reprojection: 0.714199, disparity: 0.078764\n",
            "(39, 40): reprojection: 0.287669, disparity: 0.052797\n",
            "(39, 41): reprojection: 0.509165, disparity: 0.061900\n",
            "(40, 41): reprojection: 0.300056, disparity: 0.055803\n",
            "(40, 42): reprojection: 0.444655, disparity: 0.064848\n",
            "(40, 44): reprojection: 0.566438, disparity: 0.077224\n",
            "(40, 48): reprojection: 0.743419, disparity: 0.089198\n",
            "(40, 56): reprojection: 0.768412, disparity: 0.126019\n",
            "(41, 42): reprojection: 0.206237, disparity: 0.056176\n",
            "(41, 43): reprojection: 0.293953, disparity: 0.061649\n",
            "(42, 43): reprojection: 0.180762, disparity: 0.055004\n",
            "(42, 44): reprojection: 0.329395, disparity: 0.064415\n",
            "(42, 46): reprojection: 0.513278, disparity: 0.069969\n",
            "(43, 44): reprojection: 0.201603, disparity: 0.057485\n",
            "(43, 45): reprojection: 0.299376, disparity: 0.062966\n",
            "(44, 45): reprojection: 0.151535, disparity: 0.053471\n",
            "(44, 46): reprojection: 0.281268, disparity: 0.061559\n",
            "(44, 48): reprojection: 0.400834, disparity: 0.074957\n",
            "(44, 52): reprojection: 0.542363, disparity: 0.087585\n",
            "(45, 46): reprojection: 0.200032, disparity: 0.057815\n",
            "(45, 47): reprojection: 0.313488, disparity: 0.068753\n",
            "(46, 47): reprojection: 0.187015, disparity: 0.056241\n",
            "(46, 48): reprojection: 0.286018, disparity: 0.071241\n",
            "(46, 50): reprojection: 0.380746, disparity: 0.082828\n",
            "(47, 48): reprojection: 0.197794, disparity: 0.058329\n",
            "(47, 49): reprojection: 0.246507, disparity: 0.066973\n",
            "(48, 49): reprojection: 0.165493, disparity: 0.062172\n",
            "(48, 50): reprojection: 0.229631, disparity: 0.071407\n",
            "(48, 52): reprojection: 0.328742, disparity: 0.077137\n",
            "(48, 56): reprojection: 0.634216, disparity: 0.088193\n",
            "(48, 64): reprojection: 1.012674, disparity: 0.132181\n",
            "(48, 80): reprojection: 1.073649, disparity: 0.117034\n",
            "(49, 50): reprojection: 0.139727, disparity: 0.060993\n",
            "(49, 51): reprojection: 0.223743, disparity: 0.068263\n",
            "(50, 51): reprojection: 0.143870, disparity: 0.057580\n",
            "(50, 52): reprojection: 0.242655, disparity: 0.064302\n",
            "(50, 54): reprojection: 0.412729, disparity: 0.077369\n",
            "(51, 52): reprojection: 0.176730, disparity: 0.058111\n",
            "(51, 53): reprojection: 0.289153, disparity: 0.064037\n",
            "(52, 53): reprojection: 0.175126, disparity: 0.062766\n",
            "(52, 54): reprojection: 0.247624, disparity: 0.067981\n",
            "(52, 56): reprojection: 0.529444, disparity: 0.069628\n",
            "(52, 60): reprojection: 0.589281, disparity: 0.090642\n",
            "(53, 54): reprojection: 0.141875, disparity: 0.057397\n",
            "(53, 55): reprojection: 0.315109, disparity: 0.063998\n",
            "(54, 55): reprojection: 0.297721, disparity: 0.057224\n",
            "(54, 56): reprojection: 0.562762, disparity: 0.062670\n",
            "(54, 58): reprojection: 0.359017, disparity: 0.066357\n",
            "(55, 56): reprojection: 0.344911, disparity: 0.060350\n",
            "(55, 57): reprojection: 0.369251, disparity: 0.063942\n",
            "(56, 57): reprojection: 0.302660, disparity: 0.059755\n",
            "(56, 58): reprojection: 0.537319, disparity: 0.070236\n",
            "(56, 60): reprojection: 0.682763, disparity: 0.084012\n",
            "(56, 64): reprojection: 0.800310, disparity: 0.088250\n",
            "(56, 72): reprojection: 1.144059, disparity: 0.124661\n",
            "(57, 58): reprojection: 0.358173, disparity: 0.064775\n",
            "(57, 59): reprojection: 0.485481, disparity: 0.069512\n",
            "(58, 59): reprojection: 0.235492, disparity: 0.062572\n",
            "(58, 60): reprojection: 0.341197, disparity: 0.068442\n",
            "(58, 62): reprojection: 0.461220, disparity: 0.082890\n",
            "(59, 60): reprojection: 0.180491, disparity: 0.067308\n",
            "(59, 61): reprojection: 0.285882, disparity: 0.069910\n",
            "(60, 61): reprojection: 0.214960, disparity: 0.067287\n",
            "(60, 62): reprojection: 0.276805, disparity: 0.070399\n",
            "(60, 64): reprojection: 0.457438, disparity: 0.075085\n",
            "(60, 68): reprojection: 0.756089, disparity: 0.101663\n",
            "(61, 62): reprojection: 0.198920, disparity: 0.065522\n",
            "(61, 63): reprojection: 0.313409, disparity: 0.072984\n",
            "(62, 63): reprojection: 0.227901, disparity: 0.065474\n",
            "(62, 64): reprojection: 0.337021, disparity: 0.068134\n",
            "(62, 66): reprojection: 0.455757, disparity: 0.082632\n",
            "(63, 64): reprojection: 0.222246, disparity: 0.062716\n",
            "(63, 65): reprojection: 0.348953, disparity: 0.066534\n",
            "(64, 65): reprojection: 0.221530, disparity: 0.065940\n",
            "(64, 66): reprojection: 0.360259, disparity: 0.066433\n",
            "(64, 68): reprojection: 0.439597, disparity: 0.077389\n",
            "(64, 72): reprojection: 0.702586, disparity: 0.089162\n",
            "(64, 80): reprojection: 1.017346, disparity: 0.095948\n",
            "(65, 66): reprojection: 0.267604, disparity: 0.060194\n",
            "(65, 67): reprojection: 0.424824, disparity: 0.064286\n",
            "(66, 67): reprojection: 0.266661, disparity: 0.063200\n",
            "(66, 68): reprojection: 0.386282, disparity: 0.068297\n",
            "(66, 70): reprojection: 0.502451, disparity: 0.080479\n",
            "(67, 68): reprojection: 0.343104, disparity: 0.062297\n",
            "(67, 69): reprojection: 0.346199, disparity: 0.063604\n",
            "(68, 69): reprojection: 0.203944, disparity: 0.061956\n",
            "(68, 70): reprojection: 0.439094, disparity: 0.064700\n",
            "(68, 72): reprojection: 0.499449, disparity: 0.078236\n",
            "(68, 76): reprojection: 0.643568, disparity: 0.077463\n",
            "(69, 70): reprojection: 0.319578, disparity: 0.060308\n",
            "(69, 71): reprojection: 0.445776, disparity: 0.063751\n",
            "(70, 71): reprojection: 0.234342, disparity: 0.061015\n",
            "(70, 72): reprojection: 0.266168, disparity: 0.062154\n",
            "(70, 74): reprojection: 0.370505, disparity: 0.069466\n",
            "(71, 72): reprojection: 0.235577, disparity: 0.061359\n",
            "(71, 73): reprojection: 0.338518, disparity: 0.065452\n",
            "(72, 73): reprojection: 0.221410, disparity: 0.059440\n",
            "(72, 74): reprojection: 0.288679, disparity: 0.063257\n",
            "(72, 76): reprojection: 0.382185, disparity: 0.063757\n",
            "(72, 80): reprojection: 0.543872, disparity: 0.077036\n",
            "(72, 88): reprojection: 0.806633, disparity: 0.082806\n",
            "(73, 74): reprojection: 0.310839, disparity: 0.058177\n",
            "(73, 75): reprojection: 0.428144, disparity: 0.062943\n",
            "(74, 75): reprojection: 0.238245, disparity: 0.057697\n",
            "(74, 76): reprojection: 0.259573, disparity: 0.059874\n",
            "(74, 78): reprojection: 0.363243, disparity: 0.065093\n",
            "(75, 76): reprojection: 0.236677, disparity: 0.056553\n",
            "(75, 77): reprojection: 0.359382, disparity: 0.062793\n",
            "(76, 77): reprojection: 0.227484, disparity: 0.056415\n",
            "(76, 78): reprojection: 0.329500, disparity: 0.057863\n",
            "(76, 80): reprojection: 0.375725, disparity: 0.062775\n",
            "(76, 84): reprojection: 0.531749, disparity: 0.071289\n",
            "(77, 78): reprojection: 0.227258, disparity: 0.055798\n",
            "(77, 79): reprojection: 0.247593, disparity: 0.058625\n",
            "(78, 79): reprojection: 0.191079, disparity: 0.056914\n",
            "(78, 80): reprojection: 0.235974, disparity: 0.060783\n",
            "(78, 82): reprojection: 0.364853, disparity: 0.065066\n",
            "(79, 80): reprojection: 0.145886, disparity: 0.056515\n",
            "(79, 81): reprojection: 0.235569, disparity: 0.059060\n",
            "(80, 81): reprojection: 0.146194, disparity: 0.054067\n",
            "(80, 82): reprojection: 0.253066, disparity: 0.057453\n",
            "(80, 84): reprojection: 0.415574, disparity: 0.059390\n",
            "(80, 88): reprojection: 0.669989, disparity: 0.074738\n",
            "(81, 82): reprojection: 0.177796, disparity: 0.053594\n",
            "(81, 83): reprojection: 0.284659, disparity: 0.058259\n",
            "(82, 83): reprojection: 0.145271, disparity: 0.053157\n",
            "(82, 84): reprojection: 0.232983, disparity: 0.057823\n",
            "(82, 86): reprojection: 0.422213, disparity: 0.065691\n",
            "(83, 84): reprojection: 0.138583, disparity: 0.052411\n",
            "(83, 85): reprojection: 0.299010, disparity: 0.060343\n",
            "(84, 85): reprojection: 0.171983, disparity: 0.052002\n",
            "(84, 86): reprojection: 0.255746, disparity: 0.057320\n",
            "(84, 88): reprojection: 0.500395, disparity: 0.062959\n",
            "(85, 86): reprojection: 0.168893, disparity: 0.050855\n",
            "(85, 87): reprojection: 0.274421, disparity: 0.055561\n",
            "(86, 87): reprojection: 0.231414, disparity: 0.053289\n",
            "(86, 88): reprojection: 0.339235, disparity: 0.060594\n",
            "(86, 90): reprojection: 0.485283, disparity: 0.062514\n",
            "(87, 88): reprojection: 0.185799, disparity: 0.052278\n",
            "(87, 89): reprojection: 0.280463, disparity: 0.057931\n",
            "(88, 89): reprojection: 0.139701, disparity: 0.050322\n",
            "(88, 90): reprojection: 0.306354, disparity: 0.054303\n",
            "(89, 90): reprojection: 0.213990, disparity: 0.047042\n",
            "(89, 91): reprojection: 0.423516, disparity: 0.056922\n",
            "(90, 91): reprojection: 0.341458, disparity: 0.054368\n",
            "Mean:     reprojection: 0.341458, disparity: 0.054368\n",
            "Done Validation for epoch 17 (4420 iterations)\n",
            "Epoch = 17, pairs = [[17, 18], [50, 54], [64, 80], [49, 50]], loss = 0.5527719259262085\n",
            "Epoch = 17, pairs = [[6, 10], [71, 73], [11, 13], [83, 84]], loss = 0.3335772752761841\n",
            "Epoch = 17, pairs = [[82, 86], [9, 11], [35, 36], [40, 48]], loss = 0.5085446238517761\n",
            "Epoch = 17, pairs = [[51, 53], [64, 65], [36, 37], [70, 72]], loss = 0.3202701807022095\n",
            "Epoch = 17, pairs = [[66, 68], [39, 41], [1, 2], [68, 70]], loss = 0.4433691203594208\n",
            "Epoch = 17, pairs = [[8, 12], [76, 77], [10, 11], [12, 20]], loss = 0.468392938375473\n",
            "Epoch = 17, pairs = [[13, 15], [32, 33], [18, 19], [57, 59]], loss = 0.3534054458141327\n",
            "Epoch = 17, pairs = [[4, 8], [20, 24], [19, 20], [47, 48]], loss = 0.31292930245399475\n",
            "Epoch = 17, pairs = [[63, 64], [45, 47], [34, 38], [60, 64]], loss = 0.446296751499176\n",
            "Epoch = 17, pairs = [[56, 72], [82, 84], [75, 76], [35, 37]], loss = 0.5757458209991455\n",
            "Epoch = 17, pairs = [[88, 89], [80, 84], [50, 51], [72, 76]], loss = 0.3809041976928711\n",
            "Epoch = 17, pairs = [[62, 63], [2, 3], [50, 52], [48, 50]], loss = 0.27667874097824097\n",
            "Epoch = 17, pairs = [[14, 18], [81, 83], [78, 80], [44, 52]], loss = 0.4298102855682373\n",
            "Epoch = 17, pairs = [[76, 80], [41, 43], [64, 66], [48, 49]], loss = 0.37471264600753784\n",
            "Epoch = 17, pairs = [[84, 85], [17, 19], [6, 7], [25, 26]], loss = 0.280308336019516\n",
            "Epoch = 17, pairs = [[87, 88], [72, 74], [2, 4], [37, 38]], loss = 0.2786898612976074\n",
            "Epoch = 17, pairs = [[33, 35], [51, 52], [66, 67], [37, 39]], loss = 0.30848050117492676\n",
            "Epoch = 17, pairs = [[12, 16], [28, 29], [42, 46], [27, 29]], loss = 0.42452701926231384\n",
            "Epoch = 17, pairs = [[0, 1], [74, 78], [3, 4], [30, 34]], loss = 0.322509765625\n",
            "Epoch = 17, pairs = [[59, 61], [16, 20], [32, 64], [77, 79]], loss = 0.7243638038635254\n",
            "Epoch = 17, pairs = [[86, 90], [7, 9], [34, 35], [40, 42]], loss = 0.3783784508705139\n",
            "Epoch = 17, pairs = [[69, 71], [48, 64], [86, 88], [76, 78]], loss = 0.6416634321212769\n",
            "Epoch = 17, pairs = [[24, 40], [88, 90], [54, 56], [62, 66]], loss = 0.7010612487792969\n",
            "Epoch = 17, pairs = [[20, 22], [38, 42], [20, 28], [29, 31]], loss = 0.5128136873245239\n",
            "Epoch = 17, pairs = [[40, 44], [28, 36], [32, 48], [43, 44]], loss = 0.6967841982841492\n",
            "Epoch = 17, pairs = [[19, 21], [87, 89], [23, 25], [13, 14]], loss = 0.2711043357849121\n",
            "Epoch = 17, pairs = [[24, 28], [58, 62], [42, 44], [80, 82]], loss = 0.42032676935195923\n",
            "Epoch = 17, pairs = [[5, 7], [8, 9], [78, 82], [76, 84]], loss = 0.39100950956344604\n",
            "Epoch = 17, pairs = [[24, 32], [58, 59], [16, 24], [18, 20]], loss = 0.5418058037757874\n",
            "Epoch = 17, pairs = [[38, 39], [86, 87], [41, 42], [52, 54]], loss = 0.27841076254844666\n",
            "Epoch = 17, pairs = [[56, 58], [8, 16], [70, 71], [28, 32]], loss = 0.5287027359008789\n",
            "Epoch = 17, pairs = [[8, 24], [62, 64], [36, 38], [22, 24]], loss = 0.5216579437255859\n",
            "Epoch = 17, pairs = [[73, 75], [66, 70], [1, 3], [60, 62]], loss = 0.4282694160938263\n",
            "Epoch = 17, pairs = [[78, 79], [0, 16], [89, 90], [22, 23]], loss = 0.3717009723186493\n",
            "Epoch = 17, pairs = [[27, 28], [12, 13], [29, 30], [67, 69]], loss = 0.24080461263656616\n",
            "Epoch = 17, pairs = [[11, 12], [42, 43], [38, 40], [53, 54]], loss = 0.26168394088745117\n",
            "Epoch = 17, pairs = [[89, 91], [32, 40], [72, 73], [68, 76]], loss = 0.5625319480895996\n",
            "Epoch = 17, pairs = [[16, 32], [10, 14], [31, 32], [52, 53]], loss = 0.4958113431930542\n",
            "Epoch = 17, pairs = [[48, 80], [68, 72], [85, 87], [24, 26]], loss = 0.5673344135284424\n",
            "Epoch = 17, pairs = [[82, 83], [15, 17], [36, 40], [0, 4]], loss = 0.3612482249736786\n",
            "Epoch = 17, pairs = [[60, 68], [32, 34], [69, 70], [65, 66]], loss = 0.4832286834716797\n",
            "Epoch = 17, pairs = [[23, 24], [5, 6], [63, 65], [52, 60]], loss = 0.35274720191955566\n",
            "Epoch = 17, pairs = [[48, 52], [64, 72], [84, 86], [75, 77]], loss = 0.4775896668434143\n",
            "Epoch = 17, pairs = [[74, 76], [25, 27], [83, 85], [6, 8]], loss = 0.286517858505249\n",
            "Epoch = 17, pairs = [[54, 58], [9, 10], [40, 56], [4, 12]], loss = 0.6025800704956055\n",
            "Epoch = 17, pairs = [[67, 68], [56, 64], [24, 25], [58, 60]], loss = 0.49453651905059814\n",
            "Epoch = 17, pairs = [[8, 10], [48, 56], [3, 5], [45, 46]], loss = 0.4303358793258667\n",
            "Epoch = 17, pairs = [[21, 23], [84, 88], [4, 6], [43, 45]], loss = 0.35757240653038025\n",
            "Epoch = 17, pairs = [[14, 15], [0, 8], [16, 17], [15, 16]], loss = 0.3182489275932312\n",
            "Epoch = 17, pairs = [[2, 6], [59, 60], [14, 16], [44, 46]], loss = 0.3727197051048279\n",
            "Epoch = 17, pairs = [[56, 60], [79, 81], [40, 41], [30, 32]], loss = 0.43160712718963623\n",
            "Epoch = 17, pairs = [[0, 32], [60, 61], [77, 78], [85, 86]], loss = 0.5032862424850464\n",
            "Epoch = 17, pairs = [[21, 22], [34, 36], [90, 91], [26, 28]], loss = 0.282845675945282\n",
            "Epoch = 17, pairs = [[46, 50], [65, 67], [49, 51], [36, 44]], loss = 0.4977889060974121\n",
            "Epoch = 17, pairs = [[30, 31], [57, 58], [18, 22], [47, 49]], loss = 0.3805740773677826\n",
            "Epoch = 17, pairs = [[61, 62], [31, 33], [12, 14], [16, 48]], loss = 0.6665592193603516\n",
            "Epoch = 17, pairs = [[72, 80], [26, 30], [56, 57], [0, 2]], loss = 0.43448400497436523\n",
            "Epoch = 17, pairs = [[71, 72], [46, 47], [53, 55], [39, 40]], loss = 0.3109208643436432\n",
            "Epoch = 17, pairs = [[54, 55], [73, 74], [4, 5], [55, 57]], loss = 0.3429137170314789\n",
            "Epoch = 17, pairs = [[33, 34], [68, 69], [74, 75], [46, 48]], loss = 0.281554639339447\n",
            "Epoch = 17, pairs = [[44, 48], [16, 18], [22, 26], [7, 8]], loss = 0.3664557933807373\n",
            "Epoch = 17, pairs = [[61, 63], [32, 36], [10, 12], [52, 56]], loss = 0.43050867319107056\n",
            "Epoch = 17, pairs = [[26, 27], [28, 30], [55, 56], [81, 82]], loss = 0.2854403853416443\n",
            "Epoch = 17, pairs = [[20, 21], [80, 88], [80, 81], [72, 88]], loss = 0.5732976198196411\n",
            "Epoch = 17, pairs = [[70, 74], [64, 68], [79, 80], [44, 45]], loss = 0.3476759195327759\n",
            "Epoch 17 took 84.34s.\n",
            "( 0,  1): reprojection: 0.236130, disparity: 0.049643\n",
            "( 0,  2): reprojection: 0.354876, disparity: 0.049717\n",
            "( 0,  4): reprojection: 0.399207, disparity: 0.065793\n",
            "( 0,  8): reprojection: 0.481751, disparity: 0.076461\n",
            "( 0, 16): reprojection: 0.844548, disparity: 0.110971\n",
            "( 0, 32): reprojection: 1.522069, disparity: 0.174657\n",
            "( 1,  2): reprojection: 0.197515, disparity: 0.043401\n",
            "( 1,  3): reprojection: 0.252149, disparity: 0.046829\n",
            "( 2,  3): reprojection: 0.109237, disparity: 0.042163\n",
            "( 2,  4): reprojection: 0.174610, disparity: 0.049723\n",
            "( 2,  6): reprojection: 0.335827, disparity: 0.069577\n",
            "( 3,  4): reprojection: 0.112462, disparity: 0.045119\n",
            "( 3,  5): reprojection: 0.192658, disparity: 0.051208\n",
            "( 4,  5): reprojection: 0.151078, disparity: 0.047513\n",
            "( 4,  6): reprojection: 0.212168, disparity: 0.048032\n",
            "( 4,  8): reprojection: 0.282402, disparity: 0.059445\n",
            "( 4, 12): reprojection: 0.347041, disparity: 0.067674\n",
            "( 5,  6): reprojection: 0.122275, disparity: 0.044573\n",
            "( 5,  7): reprojection: 0.164193, disparity: 0.046497\n",
            "( 6,  7): reprojection: 0.102117, disparity: 0.041846\n",
            "( 6,  8): reprojection: 0.167433, disparity: 0.047073\n",
            "( 6, 10): reprojection: 0.375501, disparity: 0.057179\n",
            "( 7,  8): reprojection: 0.094864, disparity: 0.042824\n",
            "( 7,  9): reprojection: 0.192994, disparity: 0.048124\n",
            "( 8,  9): reprojection: 0.164948, disparity: 0.044762\n",
            "( 8, 10): reprojection: 0.351535, disparity: 0.052610\n",
            "( 8, 12): reprojection: 0.461873, disparity: 0.060686\n",
            "( 8, 16): reprojection: 0.657253, disparity: 0.080073\n",
            "( 8, 24): reprojection: 0.932126, disparity: 0.099883\n",
            "( 9, 10): reprojection: 0.244634, disparity: 0.046171\n",
            "( 9, 11): reprojection: 0.359321, disparity: 0.052128\n",
            "(10, 11): reprojection: 0.156925, disparity: 0.040431\n",
            "(10, 12): reprojection: 0.197175, disparity: 0.047581\n",
            "(10, 14): reprojection: 0.348654, disparity: 0.067178\n",
            "(11, 12): reprojection: 0.134914, disparity: 0.041339\n",
            "(11, 13): reprojection: 0.222965, disparity: 0.051264\n",
            "(12, 13): reprojection: 0.148651, disparity: 0.043275\n",
            "(12, 14): reprojection: 0.285970, disparity: 0.050808\n",
            "(12, 16): reprojection: 0.490165, disparity: 0.076676\n",
            "(12, 20): reprojection: 0.768151, disparity: 0.083981\n",
            "(13, 14): reprojection: 0.184516, disparity: 0.043817\n",
            "(13, 15): reprojection: 0.264013, disparity: 0.053092\n",
            "(14, 15): reprojection: 0.201299, disparity: 0.045387\n",
            "(14, 16): reprojection: 0.442425, disparity: 0.058862\n",
            "(14, 18): reprojection: 0.415133, disparity: 0.063022\n",
            "(15, 16): reprojection: 0.303569, disparity: 0.048443\n",
            "(15, 17): reprojection: 0.389059, disparity: 0.056810\n",
            "(16, 17): reprojection: 0.141223, disparity: 0.046354\n",
            "(16, 18): reprojection: 0.325558, disparity: 0.057793\n",
            "(16, 20): reprojection: 0.545391, disparity: 0.073801\n",
            "(16, 24): reprojection: 0.836583, disparity: 0.098408\n",
            "(16, 32): reprojection: 1.493345, disparity: 0.134321\n",
            "(16, 48): reprojection: 1.424042, disparity: 0.373317\n",
            "(17, 18): reprojection: 0.322353, disparity: 0.049922\n",
            "(17, 19): reprojection: 0.486996, disparity: 0.061516\n",
            "(18, 19): reprojection: 0.211018, disparity: 0.046405\n",
            "(18, 20): reprojection: 0.277912, disparity: 0.054457\n",
            "(18, 22): reprojection: 0.507899, disparity: 0.067123\n",
            "(19, 20): reprojection: 0.113623, disparity: 0.046180\n",
            "(19, 21): reprojection: 0.252215, disparity: 0.053910\n",
            "(20, 21): reprojection: 0.202561, disparity: 0.049120\n",
            "(20, 22): reprojection: 0.308791, disparity: 0.058216\n",
            "(20, 24): reprojection: 0.409492, disparity: 0.074463\n",
            "(20, 28): reprojection: 0.708831, disparity: 0.108046\n",
            "(21, 22): reprojection: 0.170225, disparity: 0.048267\n",
            "(21, 23): reprojection: 0.253694, disparity: 0.057707\n",
            "(22, 23): reprojection: 0.129676, disparity: 0.048390\n",
            "(22, 24): reprojection: 0.181582, disparity: 0.061329\n",
            "(22, 26): reprojection: 0.334004, disparity: 0.074720\n",
            "(23, 24): reprojection: 0.111370, disparity: 0.048776\n",
            "(23, 25): reprojection: 0.198820, disparity: 0.057806\n",
            "(24, 25): reprojection: 0.143767, disparity: 0.052514\n",
            "(24, 26): reprojection: 0.190952, disparity: 0.066817\n",
            "(24, 28): reprojection: 0.345275, disparity: 0.075096\n",
            "(24, 32): reprojection: 0.750282, disparity: 0.096571\n",
            "(24, 40): reprojection: 1.133691, disparity: 0.141937\n",
            "(25, 26): reprojection: 0.127227, disparity: 0.054831\n",
            "(25, 27): reprojection: 0.218537, disparity: 0.068925\n",
            "(26, 27): reprojection: 0.150050, disparity: 0.054801\n",
            "(26, 28): reprojection: 0.193403, disparity: 0.063995\n",
            "(26, 30): reprojection: 0.373836, disparity: 0.077147\n",
            "(27, 28): reprojection: 0.150266, disparity: 0.053916\n",
            "(27, 29): reprojection: 0.266234, disparity: 0.062584\n",
            "(28, 29): reprojection: 0.190538, disparity: 0.053762\n",
            "(28, 30): reprojection: 0.256260, disparity: 0.059934\n",
            "(28, 32): reprojection: 0.491497, disparity: 0.075091\n",
            "(28, 36): reprojection: 0.809365, disparity: 0.097008\n",
            "(29, 30): reprojection: 0.142544, disparity: 0.049839\n",
            "(29, 31): reprojection: 0.300561, disparity: 0.056915\n",
            "(30, 31): reprojection: 0.206484, disparity: 0.049070\n",
            "(30, 32): reprojection: 0.294010, disparity: 0.057860\n",
            "(30, 34): reprojection: 0.526787, disparity: 0.067839\n",
            "(31, 32): reprojection: 0.142190, disparity: 0.050176\n",
            "(31, 33): reprojection: 0.331576, disparity: 0.057507\n",
            "(32, 33): reprojection: 0.259190, disparity: 0.051236\n",
            "(32, 34): reprojection: 0.273633, disparity: 0.056212\n",
            "(32, 36): reprojection: 0.414452, disparity: 0.069202\n",
            "(32, 40): reprojection: 0.642888, disparity: 0.099767\n",
            "(32, 48): reprojection: 1.093585, disparity: 0.171508\n",
            "(32, 64): reprojection: 1.299606, disparity: 0.245298\n",
            "(33, 34): reprojection: 0.153380, disparity: 0.049158\n",
            "(33, 35): reprojection: 0.275005, disparity: 0.053640\n",
            "(34, 35): reprojection: 0.199669, disparity: 0.049251\n",
            "(34, 36): reprojection: 0.302273, disparity: 0.060256\n",
            "(34, 38): reprojection: 0.387020, disparity: 0.071683\n",
            "(35, 36): reprojection: 0.242581, disparity: 0.054595\n",
            "(35, 37): reprojection: 0.310290, disparity: 0.065847\n",
            "(36, 37): reprojection: 0.267801, disparity: 0.055611\n",
            "(36, 38): reprojection: 0.377764, disparity: 0.062490\n",
            "(36, 40): reprojection: 0.352857, disparity: 0.078460\n",
            "(36, 44): reprojection: 0.701166, disparity: 0.125730\n",
            "(37, 38): reprojection: 0.217677, disparity: 0.052848\n",
            "(37, 39): reprojection: 0.252063, disparity: 0.060164\n",
            "(38, 39): reprojection: 0.202723, disparity: 0.052823\n",
            "(38, 40): reprojection: 0.385995, disparity: 0.067382\n",
            "(38, 42): reprojection: 0.677372, disparity: 0.091558\n",
            "(39, 40): reprojection: 0.288539, disparity: 0.056127\n",
            "(39, 41): reprojection: 0.503342, disparity: 0.073164\n",
            "(40, 41): reprojection: 0.290472, disparity: 0.064115\n",
            "(40, 42): reprojection: 0.420365, disparity: 0.076801\n",
            "(40, 44): reprojection: 0.554370, disparity: 0.110390\n",
            "(40, 48): reprojection: 0.792704, disparity: 0.125480\n",
            "(40, 56): reprojection: 0.786141, disparity: 0.146476\n",
            "(41, 42): reprojection: 0.198456, disparity: 0.057696\n",
            "(41, 43): reprojection: 0.301409, disparity: 0.072963\n",
            "(42, 43): reprojection: 0.175320, disparity: 0.061076\n",
            "(42, 44): reprojection: 0.329291, disparity: 0.081036\n",
            "(42, 46): reprojection: 0.525328, disparity: 0.084900\n",
            "(43, 44): reprojection: 0.212286, disparity: 0.061298\n",
            "(43, 45): reprojection: 0.321389, disparity: 0.069309\n",
            "(44, 45): reprojection: 0.159081, disparity: 0.056585\n",
            "(44, 46): reprojection: 0.289633, disparity: 0.066405\n",
            "(44, 48): reprojection: 0.444155, disparity: 0.085039\n",
            "(44, 52): reprojection: 0.651767, disparity: 0.100309\n",
            "(45, 46): reprojection: 0.203881, disparity: 0.059578\n",
            "(45, 47): reprojection: 0.333612, disparity: 0.074158\n",
            "(46, 47): reprojection: 0.197549, disparity: 0.058393\n",
            "(46, 48): reprojection: 0.291736, disparity: 0.073539\n",
            "(46, 50): reprojection: 0.406928, disparity: 0.083341\n",
            "(47, 48): reprojection: 0.203157, disparity: 0.058598\n",
            "(47, 49): reprojection: 0.269131, disparity: 0.068296\n",
            "(48, 49): reprojection: 0.176151, disparity: 0.062722\n",
            "(48, 50): reprojection: 0.265581, disparity: 0.072501\n",
            "(48, 52): reprojection: 0.363925, disparity: 0.080638\n",
            "(48, 56): reprojection: 0.765392, disparity: 0.097387\n",
            "(48, 64): reprojection: 0.944629, disparity: 0.119112\n",
            "(48, 80): reprojection: 1.539352, disparity: 0.099548\n",
            "(49, 50): reprojection: 0.155115, disparity: 0.060758\n",
            "(49, 51): reprojection: 0.247976, disparity: 0.069249\n",
            "(50, 51): reprojection: 0.154591, disparity: 0.058805\n",
            "(50, 52): reprojection: 0.242160, disparity: 0.068411\n",
            "(50, 54): reprojection: 0.376425, disparity: 0.081203\n",
            "(51, 52): reprojection: 0.160978, disparity: 0.057833\n",
            "(51, 53): reprojection: 0.280693, disparity: 0.063974\n",
            "(52, 53): reprojection: 0.166484, disparity: 0.057896\n",
            "(52, 54): reprojection: 0.246338, disparity: 0.065966\n",
            "(52, 56): reprojection: 0.524710, disparity: 0.070567\n",
            "(52, 60): reprojection: 0.618433, disparity: 0.101918\n",
            "(53, 54): reprojection: 0.141252, disparity: 0.059431\n",
            "(53, 55): reprojection: 0.328735, disparity: 0.065567\n",
            "(54, 55): reprojection: 0.294234, disparity: 0.055907\n",
            "(54, 56): reprojection: 0.561205, disparity: 0.062274\n",
            "(54, 58): reprojection: 0.415380, disparity: 0.068683\n",
            "(55, 56): reprojection: 0.345649, disparity: 0.059687\n",
            "(55, 57): reprojection: 0.391597, disparity: 0.065754\n",
            "(56, 57): reprojection: 0.304906, disparity: 0.059759\n",
            "(56, 58): reprojection: 0.537263, disparity: 0.072763\n",
            "(56, 60): reprojection: 0.661070, disparity: 0.090555\n",
            "(56, 64): reprojection: 0.873947, disparity: 0.095196\n",
            "(56, 72): reprojection: 1.113658, disparity: 0.108855\n",
            "(57, 58): reprojection: 0.349955, disparity: 0.064347\n",
            "(57, 59): reprojection: 0.465466, disparity: 0.071354\n",
            "(58, 59): reprojection: 0.229881, disparity: 0.063571\n",
            "(58, 60): reprojection: 0.353910, disparity: 0.072335\n",
            "(58, 62): reprojection: 0.422351, disparity: 0.090854\n",
            "(59, 60): reprojection: 0.186670, disparity: 0.066613\n",
            "(59, 61): reprojection: 0.305231, disparity: 0.071120\n",
            "(60, 61): reprojection: 0.214492, disparity: 0.068196\n",
            "(60, 62): reprojection: 0.285784, disparity: 0.071298\n",
            "(60, 64): reprojection: 0.468469, disparity: 0.080626\n",
            "(60, 68): reprojection: 0.640472, disparity: 0.107372\n",
            "(61, 62): reprojection: 0.185338, disparity: 0.063969\n",
            "(61, 63): reprojection: 0.290225, disparity: 0.073531\n",
            "(62, 63): reprojection: 0.226710, disparity: 0.062292\n",
            "(62, 64): reprojection: 0.344281, disparity: 0.068014\n",
            "(62, 66): reprojection: 0.522818, disparity: 0.093937\n",
            "(63, 64): reprojection: 0.213722, disparity: 0.061898\n",
            "(63, 65): reprojection: 0.339195, disparity: 0.069501\n",
            "(64, 65): reprojection: 0.222419, disparity: 0.066728\n",
            "(64, 66): reprojection: 0.368652, disparity: 0.067962\n",
            "(64, 68): reprojection: 0.500265, disparity: 0.080935\n",
            "(64, 72): reprojection: 0.841723, disparity: 0.087212\n",
            "(64, 80): reprojection: 1.327514, disparity: 0.103573\n",
            "(65, 66): reprojection: 0.257858, disparity: 0.058307\n",
            "(65, 67): reprojection: 0.405159, disparity: 0.065417\n",
            "(66, 67): reprojection: 0.256996, disparity: 0.063037\n",
            "(66, 68): reprojection: 0.368535, disparity: 0.073083\n",
            "(66, 70): reprojection: 0.451968, disparity: 0.085624\n",
            "(67, 68): reprojection: 0.334282, disparity: 0.065312\n",
            "(67, 69): reprojection: 0.311428, disparity: 0.065190\n",
            "(68, 69): reprojection: 0.193930, disparity: 0.061361\n",
            "(68, 70): reprojection: 0.419918, disparity: 0.070317\n",
            "(68, 72): reprojection: 0.520440, disparity: 0.086125\n",
            "(68, 76): reprojection: 0.732884, disparity: 0.087591\n",
            "(69, 70): reprojection: 0.296383, disparity: 0.062222\n",
            "(69, 71): reprojection: 0.409504, disparity: 0.066207\n",
            "(70, 71): reprojection: 0.215543, disparity: 0.060805\n",
            "(70, 72): reprojection: 0.288657, disparity: 0.062924\n",
            "(70, 74): reprojection: 0.430677, disparity: 0.073392\n",
            "(71, 72): reprojection: 0.229618, disparity: 0.061067\n",
            "(71, 73): reprojection: 0.334113, disparity: 0.066254\n",
            "(72, 73): reprojection: 0.221984, disparity: 0.059413\n",
            "(72, 74): reprojection: 0.300999, disparity: 0.064785\n",
            "(72, 76): reprojection: 0.397342, disparity: 0.065138\n",
            "(72, 80): reprojection: 0.761953, disparity: 0.078443\n",
            "(72, 88): reprojection: 0.932822, disparity: 0.090245\n",
            "(73, 74): reprojection: 0.315167, disparity: 0.057127\n",
            "(73, 75): reprojection: 0.433590, disparity: 0.065511\n",
            "(74, 75): reprojection: 0.238951, disparity: 0.058349\n",
            "(74, 76): reprojection: 0.272184, disparity: 0.061603\n",
            "(74, 78): reprojection: 0.443414, disparity: 0.069237\n",
            "(75, 76): reprojection: 0.231043, disparity: 0.056435\n",
            "(75, 77): reprojection: 0.361452, disparity: 0.062565\n",
            "(76, 77): reprojection: 0.235605, disparity: 0.056446\n",
            "(76, 78): reprojection: 0.369888, disparity: 0.058245\n",
            "(76, 80): reprojection: 0.475455, disparity: 0.066944\n",
            "(76, 84): reprojection: 0.696338, disparity: 0.071434\n",
            "(77, 78): reprojection: 0.238264, disparity: 0.055724\n",
            "(77, 79): reprojection: 0.267086, disparity: 0.059408\n",
            "(78, 79): reprojection: 0.196657, disparity: 0.057052\n",
            "(78, 80): reprojection: 0.243188, disparity: 0.062289\n",
            "(78, 82): reprojection: 0.381266, disparity: 0.066206\n",
            "(79, 80): reprojection: 0.147316, disparity: 0.055954\n",
            "(79, 81): reprojection: 0.246046, disparity: 0.056999\n",
            "(80, 81): reprojection: 0.151570, disparity: 0.053510\n",
            "(80, 82): reprojection: 0.277430, disparity: 0.057745\n",
            "(80, 84): reprojection: 0.443066, disparity: 0.062066\n",
            "(80, 88): reprojection: 0.672405, disparity: 0.076072\n",
            "(81, 82): reprojection: 0.178260, disparity: 0.052462\n",
            "(81, 83): reprojection: 0.267967, disparity: 0.057920\n",
            "(82, 83): reprojection: 0.140562, disparity: 0.051727\n",
            "(82, 84): reprojection: 0.251205, disparity: 0.056885\n",
            "(82, 86): reprojection: 0.407432, disparity: 0.065131\n",
            "(83, 84): reprojection: 0.140378, disparity: 0.051850\n",
            "(83, 85): reprojection: 0.292297, disparity: 0.060024\n",
            "(84, 85): reprojection: 0.169370, disparity: 0.051196\n",
            "(84, 86): reprojection: 0.240517, disparity: 0.056389\n",
            "(84, 88): reprojection: 0.468557, disparity: 0.062316\n",
            "(85, 86): reprojection: 0.159379, disparity: 0.050469\n",
            "(85, 87): reprojection: 0.263670, disparity: 0.054697\n",
            "(86, 87): reprojection: 0.225830, disparity: 0.051590\n",
            "(86, 88): reprojection: 0.320075, disparity: 0.059806\n",
            "(86, 90): reprojection: 0.494299, disparity: 0.063487\n",
            "(87, 88): reprojection: 0.175115, disparity: 0.052090\n",
            "(87, 89): reprojection: 0.281272, disparity: 0.058123\n",
            "(88, 89): reprojection: 0.145601, disparity: 0.049420\n",
            "(88, 90): reprojection: 0.308551, disparity: 0.054267\n",
            "(89, 90): reprojection: 0.212668, disparity: 0.046634\n",
            "(89, 91): reprojection: 0.413129, disparity: 0.058434\n",
            "(90, 91): reprojection: 0.329083, disparity: 0.053973\n",
            "Mean:     reprojection: 0.329083, disparity: 0.053973\n",
            "Done Validation for epoch 18 (4680 iterations)\n",
            "Epoch = 18, pairs = [[32, 48], [64, 66], [80, 81], [56, 64]], loss = 0.7226236462593079\n",
            "Epoch = 18, pairs = [[25, 26], [42, 46], [6, 7], [29, 31]], loss = 0.3249017000198364\n",
            "Epoch = 18, pairs = [[57, 58], [23, 24], [49, 51], [87, 88]], loss = 0.27531105279922485\n",
            "Epoch = 18, pairs = [[0, 1], [27, 29], [83, 84], [84, 86]], loss = 0.27765482664108276\n",
            "Epoch = 18, pairs = [[61, 63], [62, 66], [32, 33], [72, 80]], loss = 0.44533035159111023\n",
            "Epoch = 18, pairs = [[4, 8], [16, 48], [77, 78], [46, 48]], loss = 0.7018405795097351\n",
            "Epoch = 18, pairs = [[76, 80], [52, 60], [2, 6], [56, 58]], loss = 0.5336485505104065\n",
            "Epoch = 18, pairs = [[8, 12], [72, 76], [48, 64], [49, 50]], loss = 0.5796612501144409\n",
            "Epoch = 18, pairs = [[16, 32], [68, 72], [4, 6], [82, 84]], loss = 0.5490226745605469\n",
            "Epoch = 18, pairs = [[26, 27], [57, 59], [10, 14], [32, 64]], loss = 0.6311140656471252\n",
            "Epoch = 18, pairs = [[35, 37], [6, 8], [78, 79], [62, 64]], loss = 0.29952138662338257\n",
            "Epoch = 18, pairs = [[39, 41], [55, 56], [70, 74], [66, 67]], loss = 0.42676910758018494\n",
            "Epoch = 18, pairs = [[33, 35], [24, 32], [88, 90], [32, 40]], loss = 0.5434513092041016\n",
            "Epoch = 18, pairs = [[84, 85], [56, 57], [26, 30], [47, 48]], loss = 0.3270724415779114\n",
            "Epoch = 18, pairs = [[80, 88], [28, 29], [69, 71], [30, 34]], loss = 0.5591458082199097\n",
            "Epoch = 18, pairs = [[90, 91], [48, 52], [86, 87], [26, 28]], loss = 0.3509095013141632\n",
            "Epoch = 18, pairs = [[40, 42], [5, 6], [43, 45], [1, 3]], loss = 0.3396393060684204\n",
            "Epoch = 18, pairs = [[0, 4], [48, 56], [15, 16], [42, 44]], loss = 0.4976505935192108\n",
            "Epoch = 18, pairs = [[55, 57], [60, 68], [78, 82], [54, 56]], loss = 0.5501869916915894\n",
            "Epoch = 18, pairs = [[51, 52], [36, 40], [12, 13], [24, 28]], loss = 0.30988550186157227\n",
            "Epoch = 18, pairs = [[40, 41], [14, 16], [12, 16], [22, 24]], loss = 0.4134604334831238\n",
            "Epoch = 18, pairs = [[74, 76], [63, 65], [72, 88], [79, 81]], loss = 0.4837695360183716\n",
            "Epoch = 18, pairs = [[19, 21], [75, 77], [20, 24], [12, 20]], loss = 0.4712695777416229\n",
            "Epoch = 18, pairs = [[36, 37], [73, 74], [16, 17], [71, 73]], loss = 0.3146764636039734\n",
            "Epoch = 18, pairs = [[41, 42], [18, 20], [11, 13], [33, 34]], loss = 0.26034802198410034\n",
            "Epoch = 18, pairs = [[18, 22], [28, 30], [0, 2], [48, 80]], loss = 0.5965552926063538\n",
            "Epoch = 18, pairs = [[34, 38], [11, 12], [61, 62], [45, 46]], loss = 0.29553404450416565\n",
            "Epoch = 18, pairs = [[58, 59], [5, 7], [40, 48], [40, 56]], loss = 0.6264054179191589\n",
            "Epoch = 18, pairs = [[12, 14], [41, 43], [4, 5], [16, 18]], loss = 0.3209071755409241\n",
            "Epoch = 18, pairs = [[85, 87], [81, 83], [52, 54], [76, 78]], loss = 0.36233216524124146\n",
            "Epoch = 18, pairs = [[35, 36], [76, 77], [53, 55], [1, 2]], loss = 0.2955912947654724\n",
            "Epoch = 18, pairs = [[60, 64], [37, 38], [6, 10], [53, 54]], loss = 0.36638861894607544\n",
            "Epoch = 18, pairs = [[80, 84], [45, 47], [40, 44], [72, 73]], loss = 0.4496539235115051\n",
            "Epoch = 18, pairs = [[87, 89], [74, 75], [58, 60], [68, 69]], loss = 0.3300049901008606\n",
            "Epoch = 18, pairs = [[46, 47], [13, 14], [65, 67], [78, 80]], loss = 0.31656694412231445\n",
            "Epoch = 18, pairs = [[2, 4], [34, 36], [64, 72], [54, 55]], loss = 0.4330677390098572\n",
            "Epoch = 18, pairs = [[66, 70], [42, 43], [89, 90], [44, 48]], loss = 0.3788190484046936\n",
            "Epoch = 18, pairs = [[82, 83], [8, 24], [23, 25], [70, 71]], loss = 0.3938862681388855\n",
            "Epoch = 18, pairs = [[22, 26], [68, 76], [77, 79], [56, 72]], loss = 0.5926156044006348\n",
            "Epoch = 18, pairs = [[21, 23], [17, 18], [18, 19], [22, 23]], loss = 0.26523327827453613\n",
            "Epoch = 18, pairs = [[56, 60], [65, 66], [54, 58], [64, 80]], loss = 0.6673292517662048\n",
            "Epoch = 18, pairs = [[37, 39], [64, 68], [76, 84], [39, 40]], loss = 0.44490355253219604\n",
            "Epoch = 18, pairs = [[50, 54], [81, 82], [36, 44], [10, 12]], loss = 0.4508135914802551\n",
            "Epoch = 18, pairs = [[50, 52], [15, 17], [0, 16], [59, 61]], loss = 0.4710761308670044\n",
            "Epoch = 18, pairs = [[38, 42], [20, 22], [80, 82], [50, 51]], loss = 0.39241713285446167\n",
            "Epoch = 18, pairs = [[21, 22], [9, 11], [17, 19], [20, 28]], loss = 0.4446483254432678\n",
            "Epoch = 18, pairs = [[86, 88], [46, 50], [29, 30], [44, 52]], loss = 0.42993998527526855\n",
            "Epoch = 18, pairs = [[79, 80], [82, 86], [74, 78], [47, 49]], loss = 0.35749590396881104\n",
            "Epoch = 18, pairs = [[48, 49], [66, 68], [8, 16], [60, 62]], loss = 0.4222722053527832\n",
            "Epoch = 18, pairs = [[4, 12], [0, 8], [8, 10], [70, 72]], loss = 0.4428061842918396\n",
            "Epoch = 18, pairs = [[85, 86], [52, 53], [38, 40], [34, 35]], loss = 0.2830235958099365\n",
            "Epoch = 18, pairs = [[83, 85], [7, 8], [14, 15], [25, 27]], loss = 0.24012506008148193\n",
            "Epoch = 18, pairs = [[3, 5], [24, 25], [58, 62], [7, 9]], loss = 0.2910701334476471\n",
            "Epoch = 18, pairs = [[16, 24], [64, 65], [27, 28], [31, 33]], loss = 0.4105398952960968\n",
            "Epoch = 18, pairs = [[73, 75], [28, 36], [60, 61], [24, 40]], loss = 0.7134712338447571\n",
            "Epoch = 18, pairs = [[72, 74], [68, 70], [67, 68], [9, 10]], loss = 0.39579129219055176\n",
            "Epoch = 18, pairs = [[16, 20], [31, 32], [44, 45], [88, 89]], loss = 0.3017053008079529\n",
            "Epoch = 18, pairs = [[59, 60], [3, 4], [62, 63], [38, 39]], loss = 0.23283380270004272\n",
            "Epoch = 18, pairs = [[48, 50], [13, 15], [84, 88], [69, 70]], loss = 0.3785300850868225\n",
            "Epoch = 18, pairs = [[2, 3], [30, 32], [20, 21], [75, 76]], loss = 0.23917484283447266\n",
            "Epoch = 18, pairs = [[19, 20], [32, 36], [36, 38], [63, 64]], loss = 0.3129255771636963\n",
            "Epoch = 18, pairs = [[44, 46], [10, 11], [52, 56], [32, 34]], loss = 0.35214704275131226\n",
            "Epoch = 18, pairs = [[51, 53], [67, 69], [8, 9], [30, 31]], loss = 0.30466872453689575\n",
            "Epoch = 18, pairs = [[14, 18], [0, 32], [71, 72], [86, 90]], loss = 0.6234769821166992\n",
            "Epoch = 18, pairs = [[24, 26], [43, 44], [28, 32], [89, 91]], loss = 0.36139976978302\n",
            "Epoch 18 took 84.97s.\n",
            "( 0,  1): reprojection: 0.236851, disparity: 0.049664\n",
            "( 0,  2): reprojection: 0.354676, disparity: 0.049459\n",
            "( 0,  4): reprojection: 0.399205, disparity: 0.059128\n",
            "( 0,  8): reprojection: 0.481807, disparity: 0.063485\n",
            "( 0, 16): reprojection: 0.853719, disparity: 0.085408\n",
            "( 0, 32): reprojection: 1.392290, disparity: 0.082484\n",
            "( 1,  2): reprojection: 0.197012, disparity: 0.041938\n",
            "( 1,  3): reprojection: 0.251204, disparity: 0.045075\n",
            "( 2,  3): reprojection: 0.109116, disparity: 0.041266\n",
            "( 2,  4): reprojection: 0.173408, disparity: 0.044594\n",
            "( 2,  6): reprojection: 0.336798, disparity: 0.053572\n",
            "( 3,  4): reprojection: 0.110324, disparity: 0.042132\n",
            "( 3,  5): reprojection: 0.194523, disparity: 0.045496\n",
            "( 4,  5): reprojection: 0.152320, disparity: 0.043320\n",
            "( 4,  6): reprojection: 0.213255, disparity: 0.043489\n",
            "( 4,  8): reprojection: 0.285363, disparity: 0.054899\n",
            "( 4, 12): reprojection: 0.382030, disparity: 0.062763\n",
            "( 5,  6): reprojection: 0.118510, disparity: 0.042009\n",
            "( 5,  7): reprojection: 0.156660, disparity: 0.045195\n",
            "( 6,  7): reprojection: 0.101280, disparity: 0.041301\n",
            "( 6,  8): reprojection: 0.171100, disparity: 0.046381\n",
            "( 6, 10): reprojection: 0.388505, disparity: 0.054599\n",
            "( 7,  8): reprojection: 0.091683, disparity: 0.042529\n",
            "( 7,  9): reprojection: 0.188458, disparity: 0.047172\n",
            "( 8,  9): reprojection: 0.161576, disparity: 0.043675\n",
            "( 8, 10): reprojection: 0.353291, disparity: 0.047163\n",
            "( 8, 12): reprojection: 0.487698, disparity: 0.055253\n",
            "( 8, 16): reprojection: 0.640196, disparity: 0.067344\n",
            "( 8, 24): reprojection: 0.870387, disparity: 0.074499\n",
            "( 9, 10): reprojection: 0.242212, disparity: 0.042898\n",
            "( 9, 11): reprojection: 0.361524, disparity: 0.047963\n",
            "(10, 11): reprojection: 0.155876, disparity: 0.039576\n",
            "(10, 12): reprojection: 0.203564, disparity: 0.045709\n",
            "(10, 14): reprojection: 0.354461, disparity: 0.058157\n",
            "(11, 12): reprojection: 0.135651, disparity: 0.040156\n",
            "(11, 13): reprojection: 0.213489, disparity: 0.049166\n",
            "(12, 13): reprojection: 0.141449, disparity: 0.041662\n",
            "(12, 14): reprojection: 0.276565, disparity: 0.047793\n",
            "(12, 16): reprojection: 0.475241, disparity: 0.068424\n",
            "(12, 20): reprojection: 0.743997, disparity: 0.068176\n",
            "(13, 14): reprojection: 0.183703, disparity: 0.042842\n",
            "(13, 15): reprojection: 0.260664, disparity: 0.048737\n",
            "(14, 15): reprojection: 0.195401, disparity: 0.044000\n",
            "(14, 16): reprojection: 0.431173, disparity: 0.053317\n",
            "(14, 18): reprojection: 0.372080, disparity: 0.059260\n",
            "(15, 16): reprojection: 0.299155, disparity: 0.045691\n",
            "(15, 17): reprojection: 0.377382, disparity: 0.053374\n",
            "(16, 17): reprojection: 0.140729, disparity: 0.044343\n",
            "(16, 18): reprojection: 0.325196, disparity: 0.052674\n",
            "(16, 20): reprojection: 0.554933, disparity: 0.062370\n",
            "(16, 24): reprojection: 0.851400, disparity: 0.074785\n",
            "(16, 32): reprojection: 1.438586, disparity: 0.094554\n",
            "(16, 48): reprojection: 1.819431, disparity: 0.142596\n",
            "(17, 18): reprojection: 0.313608, disparity: 0.046530\n",
            "(17, 19): reprojection: 0.483819, disparity: 0.055721\n",
            "(18, 19): reprojection: 0.211272, disparity: 0.045347\n",
            "(18, 20): reprojection: 0.283512, disparity: 0.051098\n",
            "(18, 22): reprojection: 0.509238, disparity: 0.058106\n",
            "(19, 20): reprojection: 0.112815, disparity: 0.045200\n",
            "(19, 21): reprojection: 0.245846, disparity: 0.050299\n",
            "(20, 21): reprojection: 0.199862, disparity: 0.046168\n",
            "(20, 22): reprojection: 0.305321, disparity: 0.053642\n",
            "(20, 24): reprojection: 0.427755, disparity: 0.061597\n",
            "(20, 28): reprojection: 0.755313, disparity: 0.085704\n",
            "(21, 22): reprojection: 0.164987, disparity: 0.046259\n",
            "(21, 23): reprojection: 0.264447, disparity: 0.053345\n",
            "(22, 23): reprojection: 0.126148, disparity: 0.047280\n",
            "(22, 24): reprojection: 0.190336, disparity: 0.055947\n",
            "(22, 26): reprojection: 0.326541, disparity: 0.067518\n",
            "(23, 24): reprojection: 0.106367, disparity: 0.046642\n",
            "(23, 25): reprojection: 0.187511, disparity: 0.054408\n",
            "(24, 25): reprojection: 0.141264, disparity: 0.050267\n",
            "(24, 26): reprojection: 0.198970, disparity: 0.059727\n",
            "(24, 28): reprojection: 0.328055, disparity: 0.066931\n",
            "(24, 32): reprojection: 0.673685, disparity: 0.081811\n",
            "(24, 40): reprojection: 1.028262, disparity: 0.118024\n",
            "(25, 26): reprojection: 0.129395, disparity: 0.052184\n",
            "(25, 27): reprojection: 0.235686, disparity: 0.061093\n",
            "(26, 27): reprojection: 0.145555, disparity: 0.052537\n",
            "(26, 28): reprojection: 0.182375, disparity: 0.059474\n",
            "(26, 30): reprojection: 0.345988, disparity: 0.070336\n",
            "(27, 28): reprojection: 0.148554, disparity: 0.050203\n",
            "(27, 29): reprojection: 0.263378, disparity: 0.058151\n",
            "(28, 29): reprojection: 0.185449, disparity: 0.050761\n",
            "(28, 30): reprojection: 0.245708, disparity: 0.055511\n",
            "(28, 32): reprojection: 0.491367, disparity: 0.067708\n",
            "(28, 36): reprojection: 0.767272, disparity: 0.083727\n",
            "(29, 30): reprojection: 0.136060, disparity: 0.048179\n",
            "(29, 31): reprojection: 0.294056, disparity: 0.053937\n",
            "(30, 31): reprojection: 0.199952, disparity: 0.048133\n",
            "(30, 32): reprojection: 0.284696, disparity: 0.055856\n",
            "(30, 34): reprojection: 0.540767, disparity: 0.068367\n",
            "(31, 32): reprojection: 0.132045, disparity: 0.049651\n",
            "(31, 33): reprojection: 0.330492, disparity: 0.055963\n",
            "(32, 33): reprojection: 0.258251, disparity: 0.050133\n",
            "(32, 34): reprojection: 0.276640, disparity: 0.054296\n",
            "(32, 36): reprojection: 0.418662, disparity: 0.063433\n",
            "(32, 40): reprojection: 0.575396, disparity: 0.083397\n",
            "(32, 48): reprojection: 1.146549, disparity: 0.138411\n",
            "(32, 64): reprojection: 1.428728, disparity: 0.138012\n",
            "(33, 34): reprojection: 0.147811, disparity: 0.048195\n",
            "(33, 35): reprojection: 0.277872, disparity: 0.053878\n",
            "(34, 35): reprojection: 0.195692, disparity: 0.047942\n",
            "(34, 36): reprojection: 0.303188, disparity: 0.054719\n",
            "(34, 38): reprojection: 0.419114, disparity: 0.065238\n",
            "(35, 36): reprojection: 0.235787, disparity: 0.050069\n",
            "(35, 37): reprojection: 0.289318, disparity: 0.061079\n",
            "(36, 37): reprojection: 0.272274, disparity: 0.054191\n",
            "(36, 38): reprojection: 0.380401, disparity: 0.058597\n",
            "(36, 40): reprojection: 0.352552, disparity: 0.065762\n",
            "(36, 44): reprojection: 0.748844, disparity: 0.088865\n",
            "(37, 38): reprojection: 0.222311, disparity: 0.051321\n",
            "(37, 39): reprojection: 0.257038, disparity: 0.056009\n",
            "(38, 39): reprojection: 0.198226, disparity: 0.049968\n",
            "(38, 40): reprojection: 0.368278, disparity: 0.059833\n",
            "(38, 42): reprojection: 0.658251, disparity: 0.075482\n",
            "(39, 40): reprojection: 0.281267, disparity: 0.053087\n",
            "(39, 41): reprojection: 0.491738, disparity: 0.062658\n",
            "(40, 41): reprojection: 0.289681, disparity: 0.056016\n",
            "(40, 42): reprojection: 0.408977, disparity: 0.062515\n",
            "(40, 44): reprojection: 0.552496, disparity: 0.075713\n",
            "(40, 48): reprojection: 0.819312, disparity: 0.102635\n",
            "(40, 56): reprojection: 0.886534, disparity: 0.141302\n",
            "(41, 42): reprojection: 0.192028, disparity: 0.054192\n",
            "(41, 43): reprojection: 0.307187, disparity: 0.059559\n",
            "(42, 43): reprojection: 0.175248, disparity: 0.054044\n",
            "(42, 44): reprojection: 0.328031, disparity: 0.062387\n",
            "(42, 46): reprojection: 0.531620, disparity: 0.067454\n",
            "(43, 44): reprojection: 0.213627, disparity: 0.054581\n",
            "(43, 45): reprojection: 0.316821, disparity: 0.061836\n",
            "(44, 45): reprojection: 0.155095, disparity: 0.053634\n",
            "(44, 46): reprojection: 0.285604, disparity: 0.060886\n",
            "(44, 48): reprojection: 0.443131, disparity: 0.083533\n",
            "(44, 52): reprojection: 0.647609, disparity: 0.103422\n",
            "(45, 46): reprojection: 0.204424, disparity: 0.055888\n",
            "(45, 47): reprojection: 0.331583, disparity: 0.065494\n",
            "(46, 47): reprojection: 0.195323, disparity: 0.055726\n",
            "(46, 48): reprojection: 0.298878, disparity: 0.076668\n",
            "(46, 50): reprojection: 0.406395, disparity: 0.086737\n",
            "(47, 48): reprojection: 0.207070, disparity: 0.061440\n",
            "(47, 49): reprojection: 0.269174, disparity: 0.071942\n",
            "(48, 49): reprojection: 0.170627, disparity: 0.059996\n",
            "(48, 50): reprojection: 0.266143, disparity: 0.066151\n",
            "(48, 52): reprojection: 0.364521, disparity: 0.076642\n",
            "(48, 56): reprojection: 0.731561, disparity: 0.089155\n",
            "(48, 64): reprojection: 1.008736, disparity: 0.125261\n",
            "(48, 80): reprojection: 1.959378, disparity: 0.099571\n",
            "(49, 50): reprojection: 0.159451, disparity: 0.058281\n",
            "(49, 51): reprojection: 0.250498, disparity: 0.065386\n",
            "(50, 51): reprojection: 0.152132, disparity: 0.057392\n",
            "(50, 52): reprojection: 0.257720, disparity: 0.064877\n",
            "(50, 54): reprojection: 0.387700, disparity: 0.071458\n",
            "(51, 52): reprojection: 0.162111, disparity: 0.057148\n",
            "(51, 53): reprojection: 0.293028, disparity: 0.060514\n",
            "(52, 53): reprojection: 0.169205, disparity: 0.055677\n",
            "(52, 54): reprojection: 0.251954, disparity: 0.060455\n",
            "(52, 56): reprojection: 0.518513, disparity: 0.063580\n",
            "(52, 60): reprojection: 0.661010, disparity: 0.084365\n",
            "(53, 54): reprojection: 0.137546, disparity: 0.055991\n",
            "(53, 55): reprojection: 0.324349, disparity: 0.064053\n",
            "(54, 55): reprojection: 0.295394, disparity: 0.056132\n",
            "(54, 56): reprojection: 0.555626, disparity: 0.060763\n",
            "(54, 58): reprojection: 0.397844, disparity: 0.066914\n",
            "(55, 56): reprojection: 0.346946, disparity: 0.058926\n",
            "(55, 57): reprojection: 0.399804, disparity: 0.062937\n",
            "(56, 57): reprojection: 0.302577, disparity: 0.058029\n",
            "(56, 58): reprojection: 0.537818, disparity: 0.066653\n",
            "(56, 60): reprojection: 0.666843, disparity: 0.082081\n",
            "(56, 64): reprojection: 0.926950, disparity: 0.087174\n",
            "(56, 72): reprojection: 1.193016, disparity: 0.110109\n",
            "(57, 58): reprojection: 0.348612, disparity: 0.062559\n",
            "(57, 59): reprojection: 0.463838, disparity: 0.067569\n",
            "(58, 59): reprojection: 0.234225, disparity: 0.062146\n",
            "(58, 60): reprojection: 0.363284, disparity: 0.068108\n",
            "(58, 62): reprojection: 0.456934, disparity: 0.080910\n",
            "(59, 60): reprojection: 0.190844, disparity: 0.063841\n",
            "(59, 61): reprojection: 0.308995, disparity: 0.067589\n",
            "(60, 61): reprojection: 0.217343, disparity: 0.067522\n",
            "(60, 62): reprojection: 0.300796, disparity: 0.068201\n",
            "(60, 64): reprojection: 0.479814, disparity: 0.075766\n",
            "(60, 68): reprojection: 0.676702, disparity: 0.094837\n",
            "(61, 62): reprojection: 0.193315, disparity: 0.063939\n",
            "(61, 63): reprojection: 0.287724, disparity: 0.070333\n",
            "(62, 63): reprojection: 0.222841, disparity: 0.061791\n",
            "(62, 64): reprojection: 0.341501, disparity: 0.068064\n",
            "(62, 66): reprojection: 0.530136, disparity: 0.085536\n",
            "(63, 64): reprojection: 0.213847, disparity: 0.065094\n",
            "(63, 65): reprojection: 0.342752, disparity: 0.065966\n",
            "(64, 65): reprojection: 0.224324, disparity: 0.067024\n",
            "(64, 66): reprojection: 0.370607, disparity: 0.070078\n",
            "(64, 68): reprojection: 0.490103, disparity: 0.083232\n",
            "(64, 72): reprojection: 0.824694, disparity: 0.093874\n",
            "(64, 80): reprojection: 1.385982, disparity: 0.103452\n",
            "(65, 66): reprojection: 0.262711, disparity: 0.060257\n",
            "(65, 67): reprojection: 0.405976, disparity: 0.067454\n",
            "(66, 67): reprojection: 0.258280, disparity: 0.063018\n",
            "(66, 68): reprojection: 0.371213, disparity: 0.065972\n",
            "(66, 70): reprojection: 0.471263, disparity: 0.076239\n",
            "(67, 68): reprojection: 0.335218, disparity: 0.061470\n",
            "(67, 69): reprojection: 0.313879, disparity: 0.062873\n",
            "(68, 69): reprojection: 0.199444, disparity: 0.059214\n",
            "(68, 70): reprojection: 0.427392, disparity: 0.064457\n",
            "(68, 72): reprojection: 0.536215, disparity: 0.075697\n",
            "(68, 76): reprojection: 0.722907, disparity: 0.072981\n",
            "(69, 70): reprojection: 0.292587, disparity: 0.059001\n",
            "(69, 71): reprojection: 0.412759, disparity: 0.061462\n",
            "(70, 71): reprojection: 0.210995, disparity: 0.058738\n",
            "(70, 72): reprojection: 0.288012, disparity: 0.061800\n",
            "(70, 74): reprojection: 0.449303, disparity: 0.067255\n",
            "(71, 72): reprojection: 0.225643, disparity: 0.059300\n",
            "(71, 73): reprojection: 0.340119, disparity: 0.063840\n",
            "(72, 73): reprojection: 0.224881, disparity: 0.059055\n",
            "(72, 74): reprojection: 0.308727, disparity: 0.062179\n",
            "(72, 76): reprojection: 0.387645, disparity: 0.061319\n",
            "(72, 80): reprojection: 0.770239, disparity: 0.075581\n",
            "(72, 88): reprojection: 0.894645, disparity: 0.080835\n",
            "(73, 74): reprojection: 0.316515, disparity: 0.057197\n",
            "(73, 75): reprojection: 0.431711, disparity: 0.061249\n",
            "(74, 75): reprojection: 0.237173, disparity: 0.056054\n",
            "(74, 76): reprojection: 0.269719, disparity: 0.059192\n",
            "(74, 78): reprojection: 0.418720, disparity: 0.065109\n",
            "(75, 76): reprojection: 0.225343, disparity: 0.055019\n",
            "(75, 77): reprojection: 0.352355, disparity: 0.060038\n",
            "(76, 77): reprojection: 0.232782, disparity: 0.055692\n",
            "(76, 78): reprojection: 0.361706, disparity: 0.056989\n",
            "(76, 80): reprojection: 0.474904, disparity: 0.063823\n",
            "(76, 84): reprojection: 0.680044, disparity: 0.071620\n",
            "(77, 78): reprojection: 0.235109, disparity: 0.055688\n",
            "(77, 79): reprojection: 0.268334, disparity: 0.058957\n",
            "(78, 79): reprojection: 0.197144, disparity: 0.056810\n",
            "(78, 80): reprojection: 0.245629, disparity: 0.060413\n",
            "(78, 82): reprojection: 0.382048, disparity: 0.064506\n",
            "(79, 80): reprojection: 0.146959, disparity: 0.055110\n",
            "(79, 81): reprojection: 0.247558, disparity: 0.057317\n",
            "(80, 81): reprojection: 0.151979, disparity: 0.053560\n",
            "(80, 82): reprojection: 0.275908, disparity: 0.057384\n",
            "(80, 84): reprojection: 0.438899, disparity: 0.058534\n",
            "(80, 88): reprojection: 0.695288, disparity: 0.068448\n",
            "(81, 82): reprojection: 0.180610, disparity: 0.052482\n",
            "(81, 83): reprojection: 0.268972, disparity: 0.057341\n",
            "(82, 83): reprojection: 0.139289, disparity: 0.051993\n",
            "(82, 84): reprojection: 0.246766, disparity: 0.055743\n",
            "(82, 86): reprojection: 0.412484, disparity: 0.062994\n",
            "(83, 84): reprojection: 0.138700, disparity: 0.051088\n",
            "(83, 85): reprojection: 0.292937, disparity: 0.059063\n",
            "(84, 85): reprojection: 0.172079, disparity: 0.051006\n",
            "(84, 86): reprojection: 0.247918, disparity: 0.055766\n",
            "(84, 88): reprojection: 0.480207, disparity: 0.059589\n",
            "(85, 86): reprojection: 0.160838, disparity: 0.050213\n",
            "(85, 87): reprojection: 0.271387, disparity: 0.055315\n",
            "(86, 87): reprojection: 0.228570, disparity: 0.052899\n",
            "(86, 88): reprojection: 0.322231, disparity: 0.058352\n",
            "(86, 90): reprojection: 0.489122, disparity: 0.061946\n",
            "(87, 88): reprojection: 0.174576, disparity: 0.051308\n",
            "(87, 89): reprojection: 0.279716, disparity: 0.056037\n",
            "(88, 89): reprojection: 0.143088, disparity: 0.049002\n",
            "(88, 90): reprojection: 0.307841, disparity: 0.053439\n",
            "(89, 90): reprojection: 0.212872, disparity: 0.047456\n",
            "(89, 91): reprojection: 0.418446, disparity: 0.057304\n",
            "(90, 91): reprojection: 0.330769, disparity: 0.053613\n",
            "Mean:     reprojection: 0.330769, disparity: 0.053613\n",
            "Done Validation for epoch 19 (4940 iterations)\n",
            "Epoch = 19, pairs = [[68, 72], [19, 21], [40, 44], [78, 82]], loss = 0.49901437759399414\n",
            "Epoch = 19, pairs = [[29, 30], [77, 78], [24, 26], [0, 32]], loss = 0.5983927249908447\n",
            "Epoch = 19, pairs = [[45, 47], [79, 81], [39, 41], [52, 56]], loss = 0.4546738862991333\n",
            "Epoch = 19, pairs = [[40, 42], [83, 84], [53, 54], [28, 29]], loss = 0.2681884765625\n",
            "Epoch = 19, pairs = [[85, 86], [61, 62], [4, 5], [37, 39]], loss = 0.2587321400642395\n",
            "Epoch = 19, pairs = [[80, 84], [45, 46], [48, 52], [54, 56]], loss = 0.5077239274978638\n",
            "Epoch = 19, pairs = [[31, 33], [37, 38], [81, 83], [64, 72]], loss = 0.5533221364021301\n",
            "Epoch = 19, pairs = [[40, 48], [36, 38], [74, 76], [71, 73]], loss = 0.5881644487380981\n",
            "Epoch = 19, pairs = [[20, 28], [74, 78], [22, 24], [86, 88]], loss = 0.39444053173065186\n",
            "Epoch = 19, pairs = [[34, 38], [28, 36], [8, 9], [8, 12]], loss = 0.5139060616493225\n",
            "Epoch = 19, pairs = [[40, 56], [14, 15], [68, 69], [76, 80]], loss = 0.4892977774143219\n",
            "Epoch = 19, pairs = [[5, 6], [0, 8], [52, 60], [12, 13]], loss = 0.41323673725128174\n",
            "Epoch = 19, pairs = [[82, 86], [46, 48], [7, 9], [63, 65]], loss = 0.37976139783859253\n",
            "Epoch = 19, pairs = [[73, 74], [50, 52], [80, 88], [30, 34]], loss = 0.4882054924964905\n",
            "Epoch = 19, pairs = [[58, 59], [66, 70], [22, 26], [90, 91]], loss = 0.4334365725517273\n",
            "Epoch = 19, pairs = [[9, 10], [31, 32], [78, 80], [0, 2]], loss = 0.2986801266670227\n",
            "Epoch = 19, pairs = [[12, 16], [35, 37], [52, 53], [48, 56]], loss = 0.4362722933292389\n",
            "Epoch = 19, pairs = [[17, 18], [38, 42], [64, 80], [59, 61]], loss = 0.6547437310218811\n",
            "Epoch = 19, pairs = [[22, 23], [6, 7], [18, 19], [69, 71]], loss = 0.2766905426979065\n",
            "Epoch = 19, pairs = [[76, 78], [33, 34], [89, 91], [10, 12]], loss = 0.34220820665359497\n",
            "Epoch = 19, pairs = [[47, 48], [1, 2], [16, 20], [28, 32]], loss = 0.3968992829322815\n",
            "Epoch = 19, pairs = [[20, 22], [8, 16], [24, 28], [38, 39]], loss = 0.4197845458984375\n",
            "Epoch = 19, pairs = [[33, 35], [88, 89], [36, 37], [38, 40]], loss = 0.3189389407634735\n",
            "Epoch = 19, pairs = [[16, 24], [62, 64], [78, 79], [5, 7]], loss = 0.45516541600227356\n",
            "Epoch = 19, pairs = [[35, 36], [56, 72], [26, 30], [0, 4]], loss = 0.5972225666046143\n",
            "Epoch = 19, pairs = [[80, 81], [71, 72], [63, 64], [41, 43]], loss = 0.2775445580482483\n",
            "Epoch = 19, pairs = [[42, 46], [70, 74], [76, 84], [76, 77]], loss = 0.5001784563064575\n",
            "Epoch = 19, pairs = [[84, 86], [18, 20], [75, 77], [28, 30]], loss = 0.33544275164604187\n",
            "Epoch = 19, pairs = [[82, 84], [66, 67], [42, 43], [67, 68]], loss = 0.3108619749546051\n",
            "Epoch = 19, pairs = [[51, 53], [52, 54], [48, 64], [8, 24]], loss = 0.6577544808387756\n",
            "Epoch = 19, pairs = [[88, 90], [44, 48], [4, 6], [48, 50]], loss = 0.3538665771484375\n",
            "Epoch = 19, pairs = [[84, 85], [41, 42], [3, 4], [42, 44]], loss = 0.2535695731639862\n",
            "Epoch = 19, pairs = [[82, 83], [17, 19], [60, 68], [56, 57]], loss = 0.4550718367099762\n",
            "Epoch = 19, pairs = [[55, 56], [16, 32], [48, 80], [60, 62]], loss = 0.8426333665847778\n",
            "Epoch = 19, pairs = [[87, 89], [32, 34], [67, 69], [44, 46]], loss = 0.33577626943588257\n",
            "Epoch = 19, pairs = [[64, 65], [21, 22], [20, 21], [39, 40]], loss = 0.26640719175338745\n",
            "Epoch = 19, pairs = [[27, 28], [54, 55], [84, 88], [57, 59]], loss = 0.4134543836116791\n",
            "Epoch = 19, pairs = [[75, 76], [54, 58], [53, 55], [9, 11]], loss = 0.4115722179412842\n",
            "Epoch = 19, pairs = [[0, 1], [72, 73], [34, 35], [36, 44]], loss = 0.42298614978790283\n",
            "Epoch = 19, pairs = [[12, 20], [24, 25], [4, 8], [29, 31]], loss = 0.4311198592185974\n",
            "Epoch = 19, pairs = [[47, 49], [16, 17], [51, 52], [56, 64]], loss = 0.5275157690048218\n",
            "Epoch = 19, pairs = [[72, 76], [27, 29], [36, 40], [32, 40]], loss = 0.5551615953445435\n",
            "Epoch = 19, pairs = [[43, 45], [49, 50], [60, 64], [2, 6]], loss = 0.41154342889785767\n",
            "Epoch = 19, pairs = [[8, 10], [23, 25], [72, 80], [19, 20]], loss = 0.395823210477829\n",
            "Epoch = 19, pairs = [[62, 66], [26, 28], [16, 18], [30, 31]], loss = 0.3971759080886841\n",
            "Epoch = 19, pairs = [[70, 71], [6, 10], [6, 8], [55, 57]], loss = 0.3557462990283966\n",
            "Epoch = 19, pairs = [[86, 87], [32, 64], [32, 36], [46, 47]], loss = 0.7258350849151611\n",
            "Epoch = 19, pairs = [[74, 75], [66, 68], [83, 85], [65, 67]], loss = 0.43135496973991394\n",
            "Epoch = 19, pairs = [[57, 58], [3, 5], [56, 58], [49, 51]], loss = 0.40492892265319824\n",
            "Epoch = 19, pairs = [[72, 88], [11, 13], [60, 61], [50, 54]], loss = 0.5585029125213623\n",
            "Epoch = 19, pairs = [[21, 23], [13, 14], [2, 3], [87, 88]], loss = 0.23247569799423218\n",
            "Epoch = 19, pairs = [[0, 16], [59, 60], [80, 82], [64, 66]], loss = 0.48396530747413635\n",
            "Epoch = 19, pairs = [[44, 52], [24, 32], [30, 32], [44, 45]], loss = 0.5897778868675232\n",
            "Epoch = 19, pairs = [[1, 3], [73, 75], [65, 66], [70, 72]], loss = 0.3788259029388428\n",
            "Epoch = 19, pairs = [[13, 15], [46, 50], [14, 18], [56, 60]], loss = 0.5434040427207947\n",
            "Epoch = 19, pairs = [[16, 48], [12, 14], [86, 90], [72, 74]], loss = 0.9033148288726807\n",
            "Epoch = 19, pairs = [[4, 12], [14, 16], [24, 40], [15, 17]], loss = 0.6960055828094482\n",
            "Epoch = 19, pairs = [[43, 44], [10, 14], [79, 80], [18, 22]], loss = 0.3497678339481354\n",
            "Epoch = 19, pairs = [[68, 76], [69, 70], [11, 12], [20, 24]], loss = 0.40985721349716187\n",
            "Epoch = 19, pairs = [[58, 60], [32, 48], [40, 41], [25, 26]], loss = 0.557840883731842\n",
            "Epoch = 19, pairs = [[58, 62], [85, 87], [50, 51], [48, 49]], loss = 0.3602575659751892\n",
            "Epoch = 19, pairs = [[7, 8], [26, 27], [25, 27], [68, 70]], loss = 0.3022844195365906\n",
            "Epoch = 19, pairs = [[62, 63], [15, 16], [10, 11], [81, 82]], loss = 0.2801852822303772\n",
            "Epoch = 19, pairs = [[64, 68], [32, 33], [61, 63], [89, 90]], loss = 0.39989426732063293\n",
            "Epoch = 19, pairs = [[2, 4], [77, 79], [34, 36], [23, 24]], loss = 0.26941683888435364\n",
            "Epoch 19 took 84.91s.\n",
            "( 0,  1): reprojection: 0.232294, disparity: 0.047677\n",
            "( 0,  2): reprojection: 0.343086, disparity: 0.049996\n",
            "( 0,  4): reprojection: 0.384131, disparity: 0.059736\n",
            "( 0,  8): reprojection: 0.545582, disparity: 0.076344\n",
            "( 0, 16): reprojection: 0.866687, disparity: 0.083157\n",
            "( 0, 32): reprojection: 1.693811, disparity: 0.125399\n",
            "( 1,  2): reprojection: 0.194980, disparity: 0.044897\n",
            "( 1,  3): reprojection: 0.251667, disparity: 0.051247\n",
            "( 2,  3): reprojection: 0.109682, disparity: 0.046393\n",
            "( 2,  4): reprojection: 0.181434, disparity: 0.051949\n",
            "( 2,  6): reprojection: 0.364516, disparity: 0.067081\n",
            "( 3,  4): reprojection: 0.105681, disparity: 0.045962\n",
            "( 3,  5): reprojection: 0.210590, disparity: 0.051843\n",
            "( 4,  5): reprojection: 0.160158, disparity: 0.048311\n",
            "( 4,  6): reprojection: 0.231341, disparity: 0.052064\n",
            "( 4,  8): reprojection: 0.321089, disparity: 0.062975\n",
            "( 4, 12): reprojection: 0.526749, disparity: 0.075155\n",
            "( 5,  6): reprojection: 0.115880, disparity: 0.047279\n",
            "( 5,  7): reprojection: 0.167527, disparity: 0.053236\n",
            "( 6,  7): reprojection: 0.100929, disparity: 0.044959\n",
            "( 6,  8): reprojection: 0.171749, disparity: 0.050159\n",
            "( 6, 10): reprojection: 0.442517, disparity: 0.062884\n",
            "( 7,  8): reprojection: 0.090721, disparity: 0.044848\n",
            "( 7,  9): reprojection: 0.214813, disparity: 0.052455\n",
            "( 8,  9): reprojection: 0.167543, disparity: 0.048094\n",
            "( 8, 10): reprojection: 0.384943, disparity: 0.054583\n",
            "( 8, 12): reprojection: 0.565965, disparity: 0.065309\n",
            "( 8, 16): reprojection: 0.726484, disparity: 0.077275\n",
            "( 8, 24): reprojection: 1.043155, disparity: 0.095719\n",
            "( 9, 10): reprojection: 0.255762, disparity: 0.047248\n",
            "( 9, 11): reprojection: 0.393674, disparity: 0.054113\n",
            "(10, 11): reprojection: 0.171336, disparity: 0.043230\n",
            "(10, 12): reprojection: 0.231834, disparity: 0.050596\n",
            "(10, 14): reprojection: 0.400973, disparity: 0.065013\n",
            "(11, 12): reprojection: 0.146691, disparity: 0.042935\n",
            "(11, 13): reprojection: 0.240955, disparity: 0.054811\n",
            "(12, 13): reprojection: 0.129025, disparity: 0.045839\n",
            "(12, 14): reprojection: 0.275067, disparity: 0.053825\n",
            "(12, 16): reprojection: 0.404537, disparity: 0.073009\n",
            "(12, 20): reprojection: 0.733377, disparity: 0.079434\n",
            "(13, 14): reprojection: 0.194830, disparity: 0.046139\n",
            "(13, 15): reprojection: 0.214633, disparity: 0.055178\n",
            "(14, 15): reprojection: 0.179251, disparity: 0.047981\n",
            "(14, 16): reprojection: 0.411912, disparity: 0.057440\n",
            "(14, 18): reprojection: 0.369183, disparity: 0.066539\n",
            "(15, 16): reprojection: 0.292700, disparity: 0.049960\n",
            "(15, 17): reprojection: 0.379737, disparity: 0.058684\n",
            "(16, 17): reprojection: 0.141951, disparity: 0.048974\n",
            "(16, 18): reprojection: 0.349053, disparity: 0.058137\n",
            "(16, 20): reprojection: 0.678520, disparity: 0.069090\n",
            "(16, 24): reprojection: 0.980841, disparity: 0.082791\n",
            "(16, 32): reprojection: 1.583350, disparity: 0.125924\n",
            "(16, 48): reprojection: 2.672850, disparity: 0.191731\n",
            "(17, 18): reprojection: 0.317669, disparity: 0.049915\n",
            "(17, 19): reprojection: 0.520115, disparity: 0.059062\n",
            "(18, 19): reprojection: 0.236095, disparity: 0.048735\n",
            "(18, 20): reprojection: 0.354960, disparity: 0.057717\n",
            "(18, 22): reprojection: 0.585068, disparity: 0.068797\n",
            "(19, 20): reprojection: 0.139130, disparity: 0.048577\n",
            "(19, 21): reprojection: 0.307225, disparity: 0.057044\n",
            "(20, 21): reprojection: 0.194779, disparity: 0.049662\n",
            "(20, 22): reprojection: 0.296928, disparity: 0.060908\n",
            "(20, 24): reprojection: 0.400592, disparity: 0.066451\n",
            "(20, 28): reprojection: 0.688784, disparity: 0.095240\n",
            "(21, 22): reprojection: 0.147510, disparity: 0.050562\n",
            "(21, 23): reprojection: 0.210507, disparity: 0.061408\n",
            "(22, 23): reprojection: 0.128575, disparity: 0.051093\n",
            "(22, 24): reprojection: 0.208829, disparity: 0.061114\n",
            "(22, 26): reprojection: 0.421277, disparity: 0.075355\n",
            "(23, 24): reprojection: 0.126067, disparity: 0.050577\n",
            "(23, 25): reprojection: 0.221671, disparity: 0.059766\n",
            "(24, 25): reprojection: 0.128145, disparity: 0.053409\n",
            "(24, 26): reprojection: 0.218225, disparity: 0.061346\n",
            "(24, 28): reprojection: 0.437410, disparity: 0.076947\n",
            "(24, 32): reprojection: 0.854653, disparity: 0.098232\n",
            "(24, 40): reprojection: 1.259008, disparity: 0.131843\n",
            "(25, 26): reprojection: 0.152722, disparity: 0.055305\n",
            "(25, 27): reprojection: 0.261468, disparity: 0.062061\n",
            "(26, 27): reprojection: 0.151525, disparity: 0.054862\n",
            "(26, 28): reprojection: 0.229675, disparity: 0.063566\n",
            "(26, 30): reprojection: 0.409183, disparity: 0.075405\n",
            "(27, 28): reprojection: 0.137395, disparity: 0.055416\n",
            "(27, 29): reprojection: 0.277502, disparity: 0.062346\n",
            "(28, 29): reprojection: 0.176598, disparity: 0.056076\n",
            "(28, 30): reprojection: 0.273675, disparity: 0.063244\n",
            "(28, 32): reprojection: 0.425883, disparity: 0.075325\n",
            "(28, 36): reprojection: 0.737526, disparity: 0.095865\n",
            "(29, 30): reprojection: 0.139908, disparity: 0.053341\n",
            "(29, 31): reprojection: 0.271518, disparity: 0.062607\n",
            "(30, 31): reprojection: 0.201120, disparity: 0.052982\n",
            "(30, 32): reprojection: 0.270078, disparity: 0.062817\n",
            "(30, 34): reprojection: 0.451298, disparity: 0.074307\n",
            "(31, 32): reprojection: 0.128271, disparity: 0.053340\n",
            "(31, 33): reprojection: 0.301389, disparity: 0.063537\n",
            "(32, 33): reprojection: 0.229214, disparity: 0.054923\n",
            "(32, 34): reprojection: 0.253795, disparity: 0.062841\n",
            "(32, 36): reprojection: 0.401026, disparity: 0.076448\n",
            "(32, 40): reprojection: 0.575836, disparity: 0.095101\n",
            "(32, 48): reprojection: 1.196701, disparity: 0.143757\n",
            "(32, 64): reprojection: 2.079088, disparity: 0.293993\n",
            "(33, 34): reprojection: 0.161094, disparity: 0.052048\n",
            "(33, 35): reprojection: 0.286105, disparity: 0.058027\n",
            "(34, 35): reprojection: 0.190483, disparity: 0.050792\n",
            "(34, 36): reprojection: 0.276083, disparity: 0.061620\n",
            "(34, 38): reprojection: 0.406579, disparity: 0.070197\n",
            "(35, 36): reprojection: 0.221185, disparity: 0.055616\n",
            "(35, 37): reprojection: 0.283338, disparity: 0.067045\n",
            "(36, 37): reprojection: 0.251440, disparity: 0.058366\n",
            "(36, 38): reprojection: 0.424404, disparity: 0.064426\n",
            "(36, 40): reprojection: 0.379076, disparity: 0.072310\n",
            "(36, 44): reprojection: 0.798527, disparity: 0.108796\n",
            "(37, 38): reprojection: 0.235113, disparity: 0.054363\n",
            "(37, 39): reprojection: 0.270780, disparity: 0.060035\n",
            "(38, 39): reprojection: 0.190982, disparity: 0.052377\n",
            "(38, 40): reprojection: 0.366654, disparity: 0.064781\n",
            "(38, 42): reprojection: 0.695452, disparity: 0.080209\n",
            "(39, 40): reprojection: 0.273053, disparity: 0.056258\n",
            "(39, 41): reprojection: 0.495018, disparity: 0.069412\n",
            "(40, 41): reprojection: 0.296491, disparity: 0.063133\n",
            "(40, 42): reprojection: 0.445096, disparity: 0.075163\n",
            "(40, 44): reprojection: 0.616017, disparity: 0.098325\n",
            "(40, 48): reprojection: 0.929597, disparity: 0.125640\n",
            "(40, 56): reprojection: 1.049807, disparity: 0.193362\n",
            "(41, 42): reprojection: 0.196740, disparity: 0.058815\n",
            "(41, 43): reprojection: 0.328658, disparity: 0.069403\n",
            "(42, 43): reprojection: 0.195273, disparity: 0.060504\n",
            "(42, 44): reprojection: 0.361231, disparity: 0.072804\n",
            "(42, 46): reprojection: 0.609505, disparity: 0.086577\n",
            "(43, 44): reprojection: 0.203525, disparity: 0.060240\n",
            "(43, 45): reprojection: 0.331788, disparity: 0.069009\n",
            "(44, 45): reprojection: 0.155778, disparity: 0.058204\n",
            "(44, 46): reprojection: 0.316386, disparity: 0.069750\n",
            "(44, 48): reprojection: 0.514269, disparity: 0.088984\n",
            "(44, 52): reprojection: 0.836450, disparity: 0.138149\n",
            "(45, 46): reprojection: 0.212140, disparity: 0.062215\n",
            "(45, 47): reprojection: 0.343065, disparity: 0.073725\n",
            "(46, 47): reprojection: 0.190604, disparity: 0.061911\n",
            "(46, 48): reprojection: 0.316634, disparity: 0.074834\n",
            "(46, 50): reprojection: 0.462541, disparity: 0.102453\n",
            "(47, 48): reprojection: 0.214356, disparity: 0.060251\n",
            "(47, 49): reprojection: 0.281677, disparity: 0.071520\n",
            "(48, 49): reprojection: 0.164184, disparity: 0.067453\n",
            "(48, 50): reprojection: 0.261377, disparity: 0.088280\n",
            "(48, 52): reprojection: 0.463461, disparity: 0.122579\n",
            "(48, 56): reprojection: 0.897524, disparity: 0.159906\n",
            "(48, 64): reprojection: 1.249083, disparity: 0.203180\n",
            "(48, 80): reprojection: 2.692781, disparity: 0.281042\n",
            "(49, 50): reprojection: 0.144628, disparity: 0.067320\n",
            "(49, 51): reprojection: 0.268799, disparity: 0.076517\n",
            "(50, 51): reprojection: 0.167869, disparity: 0.061339\n",
            "(50, 52): reprojection: 0.290732, disparity: 0.075711\n",
            "(50, 54): reprojection: 0.486105, disparity: 0.098450\n",
            "(51, 52): reprojection: 0.191757, disparity: 0.067191\n",
            "(51, 53): reprojection: 0.345158, disparity: 0.078386\n",
            "(52, 53): reprojection: 0.188319, disparity: 0.063560\n",
            "(52, 54): reprojection: 0.290330, disparity: 0.075975\n",
            "(52, 56): reprojection: 0.591794, disparity: 0.085417\n",
            "(52, 60): reprojection: 0.761992, disparity: 0.119593\n",
            "(53, 54): reprojection: 0.138624, disparity: 0.065317\n",
            "(53, 55): reprojection: 0.354697, disparity: 0.070880\n",
            "(54, 55): reprojection: 0.312126, disparity: 0.061197\n",
            "(54, 56): reprojection: 0.568352, disparity: 0.066693\n",
            "(54, 58): reprojection: 0.482356, disparity: 0.091065\n",
            "(55, 56): reprojection: 0.349731, disparity: 0.064483\n",
            "(55, 57): reprojection: 0.391740, disparity: 0.073585\n",
            "(56, 57): reprojection: 0.300777, disparity: 0.067777\n",
            "(56, 58): reprojection: 0.561695, disparity: 0.087115\n",
            "(56, 60): reprojection: 0.727044, disparity: 0.096348\n",
            "(56, 64): reprojection: 1.020127, disparity: 0.114559\n",
            "(56, 72): reprojection: 1.162953, disparity: 0.136930\n",
            "(57, 58): reprojection: 0.361001, disparity: 0.075383\n",
            "(57, 59): reprojection: 0.501866, disparity: 0.079510\n",
            "(58, 59): reprojection: 0.257893, disparity: 0.069188\n",
            "(58, 60): reprojection: 0.405070, disparity: 0.077694\n",
            "(58, 62): reprojection: 0.534709, disparity: 0.090885\n",
            "(59, 60): reprojection: 0.203088, disparity: 0.072076\n",
            "(59, 61): reprojection: 0.359839, disparity: 0.081065\n",
            "(60, 61): reprojection: 0.230576, disparity: 0.074429\n",
            "(60, 62): reprojection: 0.352443, disparity: 0.079513\n",
            "(60, 64): reprojection: 0.593503, disparity: 0.092209\n",
            "(60, 68): reprojection: 0.707921, disparity: 0.103904\n",
            "(61, 62): reprojection: 0.233569, disparity: 0.072431\n",
            "(61, 63): reprojection: 0.330210, disparity: 0.078591\n",
            "(62, 63): reprojection: 0.226878, disparity: 0.068810\n",
            "(62, 64): reprojection: 0.346186, disparity: 0.078619\n",
            "(62, 66): reprojection: 0.476763, disparity: 0.098172\n",
            "(63, 64): reprojection: 0.216611, disparity: 0.070535\n",
            "(63, 65): reprojection: 0.316271, disparity: 0.073724\n",
            "(64, 65): reprojection: 0.192111, disparity: 0.071464\n",
            "(64, 66): reprojection: 0.345254, disparity: 0.073822\n",
            "(64, 68): reprojection: 0.503253, disparity: 0.092031\n",
            "(64, 72): reprojection: 0.898260, disparity: 0.108663\n",
            "(64, 80): reprojection: 1.491740, disparity: 0.122300\n",
            "(65, 66): reprojection: 0.278807, disparity: 0.065411\n",
            "(65, 67): reprojection: 0.423393, disparity: 0.073451\n",
            "(66, 67): reprojection: 0.255298, disparity: 0.069620\n",
            "(66, 68): reprojection: 0.387143, disparity: 0.071305\n",
            "(66, 70): reprojection: 0.513551, disparity: 0.081577\n",
            "(67, 68): reprojection: 0.344521, disparity: 0.066868\n",
            "(67, 69): reprojection: 0.364524, disparity: 0.069060\n",
            "(68, 69): reprojection: 0.215416, disparity: 0.065252\n",
            "(68, 70): reprojection: 0.476471, disparity: 0.071714\n",
            "(68, 72): reprojection: 0.559875, disparity: 0.079678\n",
            "(68, 76): reprojection: 0.805882, disparity: 0.091147\n",
            "(69, 70): reprojection: 0.314829, disparity: 0.062940\n",
            "(69, 71): reprojection: 0.475380, disparity: 0.066819\n",
            "(70, 71): reprojection: 0.217975, disparity: 0.062967\n",
            "(70, 72): reprojection: 0.310986, disparity: 0.066516\n",
            "(70, 74): reprojection: 0.437055, disparity: 0.074005\n",
            "(71, 72): reprojection: 0.239929, disparity: 0.064427\n",
            "(71, 73): reprojection: 0.379016, disparity: 0.070380\n",
            "(72, 73): reprojection: 0.237352, disparity: 0.064647\n",
            "(72, 74): reprojection: 0.305302, disparity: 0.068133\n",
            "(72, 76): reprojection: 0.460437, disparity: 0.077516\n",
            "(72, 80): reprojection: 0.709671, disparity: 0.098052\n",
            "(72, 88): reprojection: 1.151053, disparity: 0.099224\n",
            "(73, 74): reprojection: 0.316293, disparity: 0.063300\n",
            "(73, 75): reprojection: 0.425296, disparity: 0.068299\n",
            "(74, 75): reprojection: 0.228714, disparity: 0.061436\n",
            "(74, 76): reprojection: 0.257293, disparity: 0.064796\n",
            "(74, 78): reprojection: 0.401931, disparity: 0.076087\n",
            "(75, 76): reprojection: 0.224975, disparity: 0.060724\n",
            "(75, 77): reprojection: 0.344659, disparity: 0.066567\n",
            "(76, 77): reprojection: 0.218112, disparity: 0.060911\n",
            "(76, 78): reprojection: 0.332108, disparity: 0.064678\n",
            "(76, 80): reprojection: 0.471835, disparity: 0.081056\n",
            "(76, 84): reprojection: 0.803226, disparity: 0.095852\n",
            "(77, 78): reprojection: 0.220309, disparity: 0.061876\n",
            "(77, 79): reprojection: 0.274554, disparity: 0.066562\n",
            "(78, 79): reprojection: 0.205991, disparity: 0.062010\n",
            "(78, 80): reprojection: 0.280927, disparity: 0.070611\n",
            "(78, 82): reprojection: 0.465529, disparity: 0.074447\n",
            "(79, 80): reprojection: 0.154001, disparity: 0.061431\n",
            "(79, 81): reprojection: 0.258257, disparity: 0.065514\n",
            "(80, 81): reprojection: 0.150334, disparity: 0.060095\n",
            "(80, 82): reprojection: 0.296903, disparity: 0.063256\n",
            "(80, 84): reprojection: 0.519041, disparity: 0.067813\n",
            "(80, 88): reprojection: 0.789830, disparity: 0.083644\n",
            "(81, 82): reprojection: 0.196169, disparity: 0.057386\n",
            "(81, 83): reprojection: 0.313529, disparity: 0.063634\n",
            "(82, 83): reprojection: 0.156766, disparity: 0.057911\n",
            "(82, 84): reprojection: 0.268332, disparity: 0.063408\n",
            "(82, 86): reprojection: 0.501930, disparity: 0.072642\n",
            "(83, 84): reprojection: 0.151384, disparity: 0.057982\n",
            "(83, 85): reprojection: 0.319782, disparity: 0.065853\n",
            "(84, 85): reprojection: 0.181043, disparity: 0.056384\n",
            "(84, 86): reprojection: 0.280619, disparity: 0.062585\n",
            "(84, 88): reprojection: 0.519658, disparity: 0.066750\n",
            "(85, 86): reprojection: 0.171510, disparity: 0.056246\n",
            "(85, 87): reprojection: 0.295193, disparity: 0.059973\n",
            "(86, 87): reprojection: 0.239456, disparity: 0.057472\n",
            "(86, 88): reprojection: 0.333251, disparity: 0.064874\n",
            "(86, 90): reprojection: 0.497118, disparity: 0.067587\n",
            "(87, 88): reprojection: 0.175272, disparity: 0.056022\n",
            "(87, 89): reprojection: 0.277539, disparity: 0.061850\n",
            "(88, 89): reprojection: 0.141771, disparity: 0.053185\n",
            "(88, 90): reprojection: 0.310515, disparity: 0.059545\n",
            "(89, 90): reprojection: 0.214602, disparity: 0.051925\n",
            "(89, 91): reprojection: 0.412891, disparity: 0.062679\n",
            "(90, 91): reprojection: 0.332308, disparity: 0.056915\n",
            "Mean:     reprojection: 0.332308, disparity: 0.056915\n",
            "Done Validation for epoch 20 (5200 iterations)\n",
            "Finished Training\n",
            "\n",
            "*******************************\n",
            "****  Compute final depth  ****\n",
            "*******************************\n",
            "\n",
            "***************************************\n",
            "****  Export visualization videos  ****\n",
            "***************************************\n",
            "2021-06-21 09:54:54,309 - INFO - Make videos Namespace(align=16, batch_size=4, camera_model='SIMPLE_PINHOLE', camera_params='1671.770118, 540, 960', colmap_bin_path='colmap', color_dir='results/ayush/color_down_png', configure='default', dense_frame_ratio=0.95, dense_pixel_ratio=0.3, depth_dirs=['results/ayush/depth_mc', 'results/ayush/depth_colmap_dense', 'results/ayush/R_hierarchical2_mc/B0.1_R1.0_PL1-0_LR0.0004_BS4_Oadam/depth'], display_freq=100, ext='.mp4', ffmpeg='ffmpeg', flow_checkpoint='FlowNet2', flow_ops=['hierarchical2'], frame_fmt='frame_%06d.png', frame_range=NamedOptionalSet(name='', set=<utils.frame_range.OptionalSet object at 0x7faa24efd278>), initialize_pose=False, lambda_parameter=0, lambda_reprojection=1.0, lambda_view_baseline=0.1, learning_rate=0.0004, log_dir=None, make_video=True, matcher='exhaustive', model_type='mc', num_epochs=20, op='all', optimizer='Adam', out_dir='results/ayush/R_hierarchical2_mc/videos', overlap_ratio=0.2, path='results/ayush', print_freq=1, refine_intrinsics=False, save_epoch_freq=1, size=384, sparse=False, val_epoch_freq=1, video3d_dir=None, video_file='data/videos/ayush.mp4')\n",
            "ffmpeg version 3.4 Copyright (c) 2000-2017 the FFmpeg developers\n",
            "  built with gcc 7.2.0 (crosstool-NG fa8859cb)\n",
            "  configuration: --prefix=/usr/local --disable-doc --enable-shared --extra-cflags='-fPIC -I/usr/local/include' --extra-cxxflags='=-fPIC' --extra-libs='-L/usr/local/lib -lz' --enable-pic --disable-static --disable-gpl --disable-nonfree --disable-openssl --enable-libvpx --cc=/opt/conda/conda-bld/ffmpeg_1530807717919/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --cxx=/opt/conda/conda-bld/ffmpeg_1530807717919/_build_env/bin/x86_64-conda_cos6-linux-gnu-c++ --enable-libopus\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "Input #0, image2, from 'results/ayush/color_down_png/frame_%06d.png':\n",
            "  Duration: 00:00:03.68, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: png, rgb24(pc), 224x384, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
            "\u001b[4;31mUnknown encoder 'libx264'\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"main.py\", line 13, in <module>\n",
            "    dp.process(params)\n",
            "  File \"/content/consistent_depth/process.py\", line 117, in process\n",
            "    return self.pipeline(params)\n",
            "  File \"/content/consistent_depth/process.py\", line 97, in pipeline\n",
            "    self.make_videos(params, ft.out_dir)\n",
            "  File \"/content/consistent_depth/process.py\", line 141, in make_videos\n",
            "    mkvid.main(vid_params)\n",
            "  File \"/content/consistent_depth/tools/make_video.py\", line 249, in main\n",
            "    ext=args.ext,\n",
            "  File \"/content/consistent_depth/tools/make_video.py\", line 141, in make_video\n",
            "    print(subprocess.run(cmd, check=True))\n",
            "  File \"/usr/local/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['ffmpeg', '-r', '30', '-i', 'results/ayush/color_down_png/frame_%06d.png', '-vcodec', 'libx264', '-pix_fmt', 'yuv420p', '-crf', '1', '-vf', 'pad=ceil(iw/2)*2:ceil(ih/2)*2', 'results/ayush/R_hierarchical2_mc/videos/color.mp4']' returned non-zero exit status 1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsXXYUSEmSL8",
        "outputId": "c743c2f8-c112-40bd-979c-d07c23a65d50"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9675 sha256=2d0c6824accabf268c1df35b6778f136145d78fc7a4bb594fb878189bbfdd516\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7Uo1IOQm5FM",
        "outputId": "d2363c1f-685f-4632-8480-ee8ff7e20d9c"
      },
      "source": [
        "!pip install pypng"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pypng\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 7.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pypng\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-cp37-none-any.whl size=67179 sha256=b2616ae85b2e187632ab2729a50ea01bb38ccb1bed012e1f5fea94cba0193e59\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
            "Successfully built pypng\n",
            "Installing collected packages: pypng\n",
            "Successfully installed pypng-0.0.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r_j9XdPsA9P",
        "outputId": "4f261042-7529-4ba3-b0a2-d41544f14ef6"
      },
      "source": [
        "!which python # should return /usr/local/bin/python"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TC4jfkRsXQF",
        "outputId": "d4687380-0f0a-4c21-e0f4-bae83667c037"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdHoIayEsZxP",
        "outputId": "b6178bf2-cd00-4da2-b8ef-45d12d06036f"
      },
      "source": [
        "!echo $PYTHONPATH"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/env/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6Slaukksg-Z",
        "outputId": "72ee24e5-917f-4f40-c626-e087fa2d4b69"
      },
      "source": [
        "%env PYTHONPATH="
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9LzsXwWskHj",
        "outputId": "a4e86da7-2ead-4963-f2e7-8285ef7c7fbf"
      },
      "source": [
        "%%bash\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREFIX=/usr/local\n",
            "installing: python-3.6.5-hc3d631a_2 ...\n",
            "installing: ca-certificates-2018.03.07-0 ...\n",
            "installing: conda-env-2.6.0-h36134e3_1 ...\n",
            "installing: libgcc-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libstdcxx-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libffi-3.2.1-hd88cf55_4 ...\n",
            "installing: ncurses-6.1-hf484d3e_0 ...\n",
            "installing: openssl-1.0.2o-h20670df_0 ...\n",
            "installing: tk-8.6.7-hc745277_3 ...\n",
            "installing: xz-5.2.4-h14c3975_4 ...\n",
            "installing: yaml-0.1.7-had09818_2 ...\n",
            "installing: zlib-1.2.11-ha838bed_2 ...\n",
            "installing: libedit-3.1.20170329-h6b74fdf_2 ...\n",
            "installing: readline-7.0-ha6073c6_4 ...\n",
            "installing: sqlite-3.23.1-he433501_0 ...\n",
            "installing: asn1crypto-0.24.0-py36_0 ...\n",
            "installing: certifi-2018.4.16-py36_0 ...\n",
            "installing: chardet-3.0.4-py36h0f667ec_1 ...\n",
            "installing: idna-2.6-py36h82fb2a8_1 ...\n",
            "installing: pycosat-0.6.3-py36h0a5515d_0 ...\n",
            "installing: pycparser-2.18-py36hf9f622e_1 ...\n",
            "installing: pysocks-1.6.8-py36_0 ...\n",
            "installing: ruamel_yaml-0.15.37-py36h14c3975_2 ...\n",
            "installing: six-1.11.0-py36h372c433_1 ...\n",
            "installing: cffi-1.11.5-py36h9745a5d_0 ...\n",
            "installing: setuptools-39.2.0-py36_0 ...\n",
            "installing: cryptography-2.2.2-py36h14c3975_0 ...\n",
            "installing: wheel-0.31.1-py36_0 ...\n",
            "installing: pip-10.0.1-py36_0 ...\n",
            "installing: pyopenssl-18.0.0-py36_0 ...\n",
            "installing: urllib3-1.22-py36hbe7ace6_0 ...\n",
            "installing: requests-2.18.4-py36he2e5f8d_1 ...\n",
            "installing: conda-4.5.4-py36_0 ...\n",
            "installation finished.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2021-06-21 08:41:44--  https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.201.79, 104.18.200.79, 2606:4700::6812:c94f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.201.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh [following]\n",
            "--2021-06-21 08:41:45--  https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58468498 (56M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-4.5.4-Linux-x86_64.sh’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0% 7.08M 8s\n",
            "    50K .......... .......... .......... .......... ..........  0% 18.7M 5s\n",
            "   100K .......... .......... .......... .......... ..........  0% 7.52M 6s\n",
            "   150K .......... .......... .......... .......... ..........  0% 17.7M 5s\n",
            "   200K .......... .......... .......... .......... ..........  0% 22.9M 5s\n",
            "   250K .......... .......... .......... .......... ..........  0% 27.3M 4s\n",
            "   300K .......... .......... .......... .......... ..........  0% 20.8M 4s\n",
            "   350K .......... .......... .......... .......... ..........  0% 22.1M 4s\n",
            "   400K .......... .......... .......... .......... ..........  0% 48.6M 4s\n",
            "   450K .......... .......... .......... .......... ..........  0% 28.0M 3s\n",
            "   500K .......... .......... .......... .......... ..........  0%  205M 3s\n",
            "   550K .......... .......... .......... .......... ..........  1% 51.1M 3s\n",
            "   600K .......... .......... .......... .......... ..........  1% 64.6M 3s\n",
            "   650K .......... .......... .......... .......... ..........  1% 51.6M 3s\n",
            "   700K .......... .......... .......... .......... ..........  1%  215M 2s\n",
            "   750K .......... .......... .......... .......... ..........  1% 37.4M 2s\n",
            "   800K .......... .......... .......... .......... ..........  1% 66.1M 2s\n",
            "   850K .......... .......... .......... .......... ..........  1%  208M 2s\n",
            "   900K .......... .......... .......... .......... ..........  1% 65.7M 2s\n",
            "   950K .......... .......... .......... .......... ..........  1% 56.4M 2s\n",
            "  1000K .......... .......... .......... .......... ..........  1% 85.2M 2s\n",
            "  1050K .......... .......... .......... .......... ..........  1%  214M 2s\n",
            "  1100K .......... .......... .......... .......... ..........  2% 65.1M 2s\n",
            "  1150K .......... .......... .......... .......... ..........  2% 84.0M 2s\n",
            "  1200K .......... .......... .......... .......... ..........  2% 81.7M 2s\n",
            "  1250K .......... .......... .......... .......... ..........  2%  102M 2s\n",
            "  1300K .......... .......... .......... .......... ..........  2%  214M 2s\n",
            "  1350K .......... .......... .......... .......... ..........  2% 92.3M 2s\n",
            "  1400K .......... .......... .......... .......... ..........  2%  114M 2s\n",
            "  1450K .......... .......... .......... .......... ..........  2% 65.5M 2s\n",
            "  1500K .......... .......... .......... .......... ..........  2%  217M 2s\n",
            "  1550K .......... .......... .......... .......... ..........  2% 65.2M 1s\n",
            "  1600K .......... .......... .......... .......... ..........  2%  105M 1s\n",
            "  1650K .......... .......... .......... .......... ..........  2%  111M 1s\n",
            "  1700K .......... .......... .......... .......... ..........  3% 90.2M 1s\n",
            "  1750K .......... .......... .......... .......... ..........  3%  108M 1s\n",
            "  1800K .......... .......... .......... .......... ..........  3% 46.3M 1s\n",
            "  1850K .......... .......... .......... .......... ..........  3% 74.3M 1s\n",
            "  1900K .......... .......... .......... .......... ..........  3% 90.3M 1s\n",
            "  1950K .......... .......... .......... .......... ..........  3% 97.0M 1s\n",
            "  2000K .......... .......... .......... .......... ..........  3% 67.5M 1s\n",
            "  2050K .......... .......... .......... .......... ..........  3% 66.7M 1s\n",
            "  2100K .......... .......... .......... .......... ..........  3% 69.6M 1s\n",
            "  2150K .......... .......... .......... .......... ..........  3% 46.1M 1s\n",
            "  2200K .......... .......... .......... .......... ..........  3% 71.5M 1s\n",
            "  2250K .......... .......... .......... .......... ..........  4% 37.7M 1s\n",
            "  2300K .......... .......... .......... .......... ..........  4% 41.9M 1s\n",
            "  2350K .......... .......... .......... .......... ..........  4% 27.6M 1s\n",
            "  2400K .......... .......... .......... .......... ..........  4%  174M 1s\n",
            "  2450K .......... .......... .......... .......... ..........  4%  106M 1s\n",
            "  2500K .......... .......... .......... .......... ..........  4%  230M 1s\n",
            "  2550K .......... .......... .......... .......... ..........  4%  135M 1s\n",
            "  2600K .......... .......... .......... .......... ..........  4%  286M 1s\n",
            "  2650K .......... .......... .......... .......... ..........  4%  150M 1s\n",
            "  2700K .......... .......... .......... .......... ..........  4%  329M 1s\n",
            "  2750K .......... .......... .......... .......... ..........  4%  278M 1s\n",
            "  2800K .......... .......... .......... .......... ..........  4%  191M 1s\n",
            "  2850K .......... .......... .......... .......... ..........  5%  324M 1s\n",
            "  2900K .......... .......... .......... .......... ..........  5%  284M 1s\n",
            "  2950K .......... .......... .......... .......... ..........  5%  295M 1s\n",
            "  3000K .......... .......... .......... .......... ..........  5%  338M 1s\n",
            "  3050K .......... .......... .......... .......... ..........  5%  338M 1s\n",
            "  3100K .......... .......... .......... .......... ..........  5%  329M 1s\n",
            "  3150K .......... .......... .......... .......... ..........  5%  251M 1s\n",
            "  3200K .......... .......... .......... .......... ..........  5%  333M 1s\n",
            "  3250K .......... .......... .......... .......... ..........  5% 93.1M 1s\n",
            "  3300K .......... .......... .......... .......... ..........  5%  286M 1s\n",
            "  3350K .......... .......... .......... .......... ..........  5%  204M 1s\n",
            "  3400K .......... .......... .......... .......... ..........  6%  215M 1s\n",
            "  3450K .......... .......... .......... .......... ..........  6%  198M 1s\n",
            "  3500K .......... .......... .......... .......... ..........  6%  199M 1s\n",
            "  3550K .......... .......... .......... .......... ..........  6% 63.1M 1s\n",
            "  3600K .......... .......... .......... .......... ..........  6%  207M 1s\n",
            "  3650K .......... .......... .......... .......... ..........  6%  183M 1s\n",
            "  3700K .......... .......... .......... .......... ..........  6%  122M 1s\n",
            "  3750K .......... .......... .......... .......... ..........  6%  172M 1s\n",
            "  3800K .......... .......... .......... .......... ..........  6%  202M 1s\n",
            "  3850K .......... .......... .......... .......... ..........  6%  101M 1s\n",
            "  3900K .......... .......... .......... .......... ..........  6% 98.5M 1s\n",
            "  3950K .......... .......... .......... .......... ..........  7% 98.9M 1s\n",
            "  4000K .......... .......... .......... .......... ..........  7%  199M 1s\n",
            "  4050K .......... .......... .......... .......... ..........  7% 74.4M 1s\n",
            "  4100K .......... .......... .......... .......... ..........  7%  179M 1s\n",
            "  4150K .......... .......... .......... .......... ..........  7%  157M 1s\n",
            "  4200K .......... .......... .......... .......... ..........  7%  159M 1s\n",
            "  4250K .......... .......... .......... .......... ..........  7% 45.4M 1s\n",
            "  4300K .......... .......... .......... .......... ..........  7%  273M 1s\n",
            "  4350K .......... .......... .......... .......... ..........  7%  267M 1s\n",
            "  4400K .......... .......... .......... .......... ..........  7%  314M 1s\n",
            "  4450K .......... .......... .......... .......... ..........  7% 90.1M 1s\n",
            "  4500K .......... .......... .......... .......... ..........  7%  291M 1s\n",
            "  4550K .......... .......... .......... .......... ..........  8%  283M 1s\n",
            "  4600K .......... .......... .......... .......... ..........  8%  306M 1s\n",
            "  4650K .......... .......... .......... .......... ..........  8%  300M 1s\n",
            "  4700K .......... .......... .......... .......... ..........  8%  256M 1s\n",
            "  4750K .......... .......... .......... .......... ..........  8%  224M 1s\n",
            "  4800K .......... .......... .......... .......... ..........  8%  125M 1s\n",
            "  4850K .......... .......... .......... .......... ..........  8%  255M 1s\n",
            "  4900K .......... .......... .......... .......... ..........  8%  262M 1s\n",
            "  4950K .......... .......... .......... .......... ..........  8% 95.9M 1s\n",
            "  5000K .......... .......... .......... .......... ..........  8%  270M 1s\n",
            "  5050K .......... .......... .......... .......... ..........  8%  130M 1s\n",
            "  5100K .......... .......... .......... .......... ..........  9%  194M 1s\n",
            "  5150K .......... .......... .......... .......... ..........  9%  153M 1s\n",
            "  5200K .......... .......... .......... .......... ..........  9% 84.5M 1s\n",
            "  5250K .......... .......... .......... .......... ..........  9%  231M 1s\n",
            "  5300K .......... .......... .......... .......... ..........  9%  191M 1s\n",
            "  5350K .......... .......... .......... .......... ..........  9%  184M 1s\n",
            "  5400K .......... .......... .......... .......... ..........  9%  214M 1s\n",
            "  5450K .......... .......... .......... .......... ..........  9% 98.1M 1s\n",
            "  5500K .......... .......... .......... .......... ..........  9% 93.8M 1s\n",
            "  5550K .......... .......... .......... .......... ..........  9%  170M 1s\n",
            "  5600K .......... .......... .......... .......... ..........  9%  205M 1s\n",
            "  5650K .......... .......... .......... .......... ..........  9% 77.6M 1s\n",
            "  5700K .......... .......... .......... .......... .......... 10%  183M 1s\n",
            "  5750K .......... .......... .......... .......... .......... 10%  177M 1s\n",
            "  5800K .......... .......... .......... .......... .......... 10%  191M 1s\n",
            "  5850K .......... .......... .......... .......... .......... 10%  132M 1s\n",
            "  5900K .......... .......... .......... .......... .......... 10% 43.4M 1s\n",
            "  5950K .......... .......... .......... .......... .......... 10%  114M 1s\n",
            "  6000K .......... .......... .......... .......... .......... 10%  208M 1s\n",
            "  6050K .......... .......... .......... .......... .......... 10%  205M 1s\n",
            "  6100K .......... .......... .......... .......... .......... 10% 79.1M 1s\n",
            "  6150K .......... .......... .......... .......... .......... 10%  188M 1s\n",
            "  6200K .......... .......... .......... .......... .......... 10%  204M 1s\n",
            "  6250K .......... .......... .......... .......... .......... 11%  213M 1s\n",
            "  6300K .......... .......... .......... .......... .......... 11%  197M 1s\n",
            "  6350K .......... .......... .......... .......... .......... 11% 73.6M 1s\n",
            "  6400K .......... .......... .......... .......... .......... 11% 98.3M 1s\n",
            "  6450K .......... .......... .......... .......... .......... 11%  196M 1s\n",
            "  6500K .......... .......... .......... .......... .......... 11%  181M 1s\n",
            "  6550K .......... .......... .......... .......... .......... 11% 87.5M 1s\n",
            "  6600K .......... .......... .......... .......... .......... 11%  199M 1s\n",
            "  6650K .......... .......... .......... .......... .......... 11%  232M 1s\n",
            "  6700K .......... .......... .......... .......... .......... 11%  219M 1s\n",
            "  6750K .......... .......... .......... .......... .......... 11%  165M 1s\n",
            "  6800K .......... .......... .......... .......... .......... 11% 81.1M 1s\n",
            "  6850K .......... .......... .......... .......... .......... 12%  131M 1s\n",
            "  6900K .......... .......... .......... .......... .......... 12%  194M 1s\n",
            "  6950K .......... .......... .......... .......... .......... 12%  116M 1s\n",
            "  7000K .......... .......... .......... .......... .......... 12%  191M 1s\n",
            "  7050K .......... .......... .......... .......... .......... 12%  199M 1s\n",
            "  7100K .......... .......... .......... .......... .......... 12%  174M 1s\n",
            "  7150K .......... .......... .......... .......... .......... 12% 73.3M 1s\n",
            "  7200K .......... .......... .......... .......... .......... 12%  202M 1s\n",
            "  7250K .......... .......... .......... .......... .......... 12%  217M 1s\n",
            "  7300K .......... .......... .......... .......... .......... 12%  203M 1s\n",
            "  7350K .......... .......... .......... .......... .......... 12% 81.7M 1s\n",
            "  7400K .......... .......... .......... .......... .......... 13%  217M 1s\n",
            "  7450K .......... .......... .......... .......... .......... 13%  189M 1s\n",
            "  7500K .......... .......... .......... .......... .......... 13%  124M 1s\n",
            "  7550K .......... .......... .......... .......... .......... 13% 53.4M 1s\n",
            "  7600K .......... .......... .......... .......... .......... 13%  182M 1s\n",
            "  7650K .......... .......... .......... .......... .......... 13%  178M 1s\n",
            "  7700K .......... .......... .......... .......... .......... 13%  157M 1s\n",
            "  7750K .......... .......... .......... .......... .......... 13% 42.9M 1s\n",
            "  7800K .......... .......... .......... .......... .......... 13%  197M 1s\n",
            "  7850K .......... .......... .......... .......... .......... 13% 91.7M 1s\n",
            "  7900K .......... .......... .......... .......... .......... 13%  198M 1s\n",
            "  7950K .......... .......... .......... .......... .......... 14%  167M 1s\n",
            "  8000K .......... .......... .......... .......... .......... 14% 96.1M 1s\n",
            "  8050K .......... .......... .......... .......... .......... 14%  210M 1s\n",
            "  8100K .......... .......... .......... .......... .......... 14%  204M 1s\n",
            "  8150K .......... .......... .......... .......... .......... 14%  176M 1s\n",
            "  8200K .......... .......... .......... .......... .......... 14%  219M 1s\n",
            "  8250K .......... .......... .......... .......... .......... 14%  233M 1s\n",
            "  8300K .......... .......... .......... .......... .......... 14% 56.6M 1s\n",
            "  8350K .......... .......... .......... .......... .......... 14%  152M 1s\n",
            "  8400K .......... .......... .......... .......... .......... 14%  188M 1s\n",
            "  8450K .......... .......... .......... .......... .......... 14%  199M 1s\n",
            "  8500K .......... .......... .......... .......... .......... 14%  212M 1s\n",
            "  8550K .......... .......... .......... .......... .......... 15%  173M 1s\n",
            "  8600K .......... .......... .......... .......... .......... 15%  198M 1s\n",
            "  8650K .......... .......... .......... .......... .......... 15%  212M 1s\n",
            "  8700K .......... .......... .......... .......... .......... 15%  206M 1s\n",
            "  8750K .......... .......... .......... .......... .......... 15%  167M 1s\n",
            "  8800K .......... .......... .......... .......... .......... 15%  202M 1s\n",
            "  8850K .......... .......... .......... .......... .......... 15%  214M 1s\n",
            "  8900K .......... .......... .......... .......... .......... 15%  211M 1s\n",
            "  8950K .......... .......... .......... .......... .......... 15%  166M 1s\n",
            "  9000K .......... .......... .......... .......... .......... 15%  159M 1s\n",
            "  9050K .......... .......... .......... .......... .......... 15% 74.4M 1s\n",
            "  9100K .......... .......... .......... .......... .......... 16% 43.1M 1s\n",
            "  9150K .......... .......... .......... .......... .......... 16%  169M 1s\n",
            "  9200K .......... .......... .......... .......... .......... 16%  204M 1s\n",
            "  9250K .......... .......... .......... .......... .......... 16%  212M 1s\n",
            "  9300K .......... .......... .......... .......... .......... 16%  198M 1s\n",
            "  9350K .......... .......... .......... .......... .......... 16%  152M 1s\n",
            "  9400K .......... .......... .......... .......... .......... 16% 58.4M 1s\n",
            "  9450K .......... .......... .......... .......... .......... 16%  210M 1s\n",
            "  9500K .......... .......... .......... .......... .......... 16%  197M 1s\n",
            "  9550K .......... .......... .......... .......... .......... 16%  170M 1s\n",
            "  9600K .......... .......... .......... .......... .......... 16%  201M 1s\n",
            "  9650K .......... .......... .......... .......... .......... 16%  207M 1s\n",
            "  9700K .......... .......... .......... .......... .......... 17%  207M 1s\n",
            "  9750K .......... .......... .......... .......... .......... 17%  170M 1s\n",
            "  9800K .......... .......... .......... .......... .......... 17%  171M 1s\n",
            "  9850K .......... .......... .......... .......... .......... 17%  187M 1s\n",
            "  9900K .......... .......... .......... .......... .......... 17%  185M 1s\n",
            "  9950K .......... .......... .......... .......... .......... 17%  142M 1s\n",
            " 10000K .......... .......... .......... .......... .......... 17%  189M 1s\n",
            " 10050K .......... .......... .......... .......... .......... 17% 39.6M 1s\n",
            " 10100K .......... .......... .......... .......... .......... 17%  205M 1s\n",
            " 10150K .......... .......... .......... .......... .......... 17%  117M 0s\n",
            " 10200K .......... .......... .......... .......... .......... 17%  203M 0s\n",
            " 10250K .......... .......... .......... .......... .......... 18%  216M 0s\n",
            " 10300K .......... .......... .......... .......... .......... 18%  202M 0s\n",
            " 10350K .......... .......... .......... .......... .......... 18%  170M 0s\n",
            " 10400K .......... .......... .......... .......... .......... 18% 74.3M 0s\n",
            " 10450K .......... .......... .......... .......... .......... 18%  118M 0s\n",
            " 10500K .......... .......... .......... .......... .......... 18%  189M 0s\n",
            " 10550K .......... .......... .......... .......... .......... 18%  170M 0s\n",
            " 10600K .......... .......... .......... .......... .......... 18%  174M 0s\n",
            " 10650K .......... .......... .......... .......... .......... 18%  206M 0s\n",
            " 10700K .......... .......... .......... .......... .......... 18%  199M 0s\n",
            " 10750K .......... .......... .......... .......... .......... 18%  165M 0s\n",
            " 10800K .......... .......... .......... .......... .......... 19%  202M 0s\n",
            " 10850K .......... .......... .......... .......... .......... 19%  140M 0s\n",
            " 10900K .......... .......... .......... .......... .......... 19%  188M 0s\n",
            " 10950K .......... .......... .......... .......... .......... 19%  183M 0s\n",
            " 11000K .......... .......... .......... .......... .......... 19%  200M 0s\n",
            " 11050K .......... .......... .......... .......... .......... 19%  220M 0s\n",
            " 11100K .......... .......... .......... .......... .......... 19% 43.5M 0s\n",
            " 11150K .......... .......... .......... .......... .......... 19%  168M 0s\n",
            " 11200K .......... .......... .......... .......... .......... 19% 98.3M 0s\n",
            " 11250K .......... .......... .......... .......... .......... 19%  181M 0s\n",
            " 11300K .......... .......... .......... .......... .......... 19%  175M 0s\n",
            " 11350K .......... .......... .......... .......... .......... 19%  159M 0s\n",
            " 11400K .......... .......... .......... .......... .......... 20%  193M 0s\n",
            " 11450K .......... .......... .......... .......... .......... 20% 55.5M 0s\n",
            " 11500K .......... .......... .......... .......... .......... 20%  110M 0s\n",
            " 11550K .......... .......... .......... .......... .......... 20%  136M 0s\n",
            " 11600K .......... .......... .......... .......... .......... 20%  199M 0s\n",
            " 11650K .......... .......... .......... .......... .......... 20%  187M 0s\n",
            " 11700K .......... .......... .......... .......... .......... 20%  206M 0s\n",
            " 11750K .......... .......... .......... .......... .......... 20%  180M 0s\n",
            " 11800K .......... .......... .......... .......... .......... 20% 58.7M 0s\n",
            " 11850K .......... .......... .......... .......... .......... 20%  187M 0s\n",
            " 11900K .......... .......... .......... .......... .......... 20%  192M 0s\n",
            " 11950K .......... .......... .......... .......... .......... 21% 67.2M 0s\n",
            " 12000K .......... .......... .......... .......... .......... 21%  112M 0s\n",
            " 12050K .......... .......... .......... .......... .......... 21%  203M 0s\n",
            " 12100K .......... .......... .......... .......... .......... 21% 90.1M 0s\n",
            " 12150K .......... .......... .......... .......... .......... 21%  171M 0s\n",
            " 12200K .......... .......... .......... .......... .......... 21%  193M 0s\n",
            " 12250K .......... .......... .......... .......... .......... 21%  190M 0s\n",
            " 12300K .......... .......... .......... .......... .......... 21%  199M 0s\n",
            " 12350K .......... .......... .......... .......... .......... 21% 70.2M 0s\n",
            " 12400K .......... .......... .......... .......... .......... 21%  197M 0s\n",
            " 12450K .......... .......... .......... .......... .......... 21%  113M 0s\n",
            " 12500K .......... .......... .......... .......... .......... 21%  167M 0s\n",
            " 12550K .......... .......... .......... .......... .......... 22%  166M 0s\n",
            " 12600K .......... .......... .......... .......... .......... 22%  185M 0s\n",
            " 12650K .......... .......... .......... .......... .......... 22%  181M 0s\n",
            " 12700K .......... .......... .......... .......... .......... 22%  182M 0s\n",
            " 12750K .......... .......... .......... .......... .......... 22%  163M 0s\n",
            " 12800K .......... .......... .......... .......... .......... 22% 58.7M 0s\n",
            " 12850K .......... .......... .......... .......... .......... 22%  156M 0s\n",
            " 12900K .......... .......... .......... .......... .......... 22% 93.0M 0s\n",
            " 12950K .......... .......... .......... .......... .......... 22%  154M 0s\n",
            " 13000K .......... .......... .......... .......... .......... 22%  134M 0s\n",
            " 13050K .......... .......... .......... .......... .......... 22% 89.7M 0s\n",
            " 13100K .......... .......... .......... .......... .......... 23%  171M 0s\n",
            " 13150K .......... .......... .......... .......... .......... 23%  159M 0s\n",
            " 13200K .......... .......... .......... .......... .......... 23%  214M 0s\n",
            " 13250K .......... .......... .......... .......... .......... 23%  209M 0s\n",
            " 13300K .......... .......... .......... .......... .......... 23%  188M 0s\n",
            " 13350K .......... .......... .......... .......... .......... 23% 75.0M 0s\n",
            " 13400K .......... .......... .......... .......... .......... 23% 98.4M 0s\n",
            " 13450K .......... .......... .......... .......... .......... 23%  210M 0s\n",
            " 13500K .......... .......... .......... .......... .......... 23%  187M 0s\n",
            " 13550K .......... .......... .......... .......... .......... 23%  151M 0s\n",
            " 13600K .......... .......... .......... .......... .......... 23%  176M 0s\n",
            " 13650K .......... .......... .......... .......... .......... 23%  191M 0s\n",
            " 13700K .......... .......... .......... .......... .......... 24%  213M 0s\n",
            " 13750K .......... .......... .......... .......... .......... 24% 69.1M 0s\n",
            " 13800K .......... .......... .......... .......... .......... 24%  209M 0s\n",
            " 13850K .......... .......... .......... .......... .......... 24%  134M 0s\n",
            " 13900K .......... .......... .......... .......... .......... 24%  110M 0s\n",
            " 13950K .......... .......... .......... .......... .......... 24%  162M 0s\n",
            " 14000K .......... .......... .......... .......... .......... 24% 88.6M 0s\n",
            " 14050K .......... .......... .......... .......... .......... 24%  112M 0s\n",
            " 14100K .......... .......... .......... .......... .......... 24%  187M 0s\n",
            " 14150K .......... .......... .......... .......... .......... 24%  173M 0s\n",
            " 14200K .......... .......... .......... .......... .......... 24%  198M 0s\n",
            " 14250K .......... .......... .......... .......... .......... 25%  190M 0s\n",
            " 14300K .......... .......... .......... .......... .......... 25%  192M 0s\n",
            " 14350K .......... .......... .......... .......... .......... 25% 59.4M 0s\n",
            " 14400K .......... .......... .......... .......... .......... 25%  120M 0s\n",
            " 14450K .......... .......... .......... .......... .......... 25%  208M 0s\n",
            " 14500K .......... .......... .......... .......... .......... 25%  169M 0s\n",
            " 14550K .......... .......... .......... .......... .......... 25%  165M 0s\n",
            " 14600K .......... .......... .......... .......... .......... 25%  181M 0s\n",
            " 14650K .......... .......... .......... .......... .......... 25%  198M 0s\n",
            " 14700K .......... .......... .......... .......... .......... 25%  197M 0s\n",
            " 14750K .......... .......... .......... .......... .......... 25% 56.0M 0s\n",
            " 14800K .......... .......... .......... .......... .......... 26%  112M 0s\n",
            " 14850K .......... .......... .......... .......... .......... 26%  124M 0s\n",
            " 14900K .......... .......... .......... .......... .......... 26%  212M 0s\n",
            " 14950K .......... .......... .......... .......... .......... 26% 84.8M 0s\n",
            " 15000K .......... .......... .......... .......... .......... 26%  132M 0s\n",
            " 15050K .......... .......... .......... .......... .......... 26%  208M 0s\n",
            " 15100K .......... .......... .......... .......... .......... 26%  211M 0s\n",
            " 15150K .......... .......... .......... .......... .......... 26%  169M 0s\n",
            " 15200K .......... .......... .......... .......... .......... 26%  192M 0s\n",
            " 15250K .......... .......... .......... .......... .......... 26%  188M 0s\n",
            " 15300K .......... .......... .......... .......... .......... 26%  156M 0s\n",
            " 15350K .......... .......... .......... .......... .......... 26% 50.7M 0s\n",
            " 15400K .......... .......... .......... .......... .......... 27%  193M 0s\n",
            " 15450K .......... .......... .......... .......... .......... 27%  178M 0s\n",
            " 15500K .......... .......... .......... .......... .......... 27%  185M 0s\n",
            " 15550K .......... .......... .......... .......... .......... 27%  157M 0s\n",
            " 15600K .......... .......... .......... .......... .......... 27%  204M 0s\n",
            " 15650K .......... .......... .......... .......... .......... 27%  214M 0s\n",
            " 15700K .......... .......... .......... .......... .......... 27% 65.9M 0s\n",
            " 15750K .......... .......... .......... .......... .......... 27%  187M 0s\n",
            " 15800K .......... .......... .......... .......... .......... 27%  131M 0s\n",
            " 15850K .......... .......... .......... .......... .......... 27%  129M 0s\n",
            " 15900K .......... .......... .......... .......... .......... 27%  182M 0s\n",
            " 15950K .......... .......... .......... .......... .......... 28% 81.5M 0s\n",
            " 16000K .......... .......... .......... .......... .......... 28%  207M 0s\n",
            " 16050K .......... .......... .......... .......... .......... 28%  213M 0s\n",
            " 16100K .......... .......... .......... .......... .......... 28%  207M 0s\n",
            " 16150K .......... .......... .......... .......... .......... 28%  183M 0s\n",
            " 16200K .......... .......... .......... .......... .......... 28%  198M 0s\n",
            " 16250K .......... .......... .......... .......... .......... 28%  192M 0s\n",
            " 16300K .......... .......... .......... .......... .......... 28%  206M 0s\n",
            " 16350K .......... .......... .......... .......... .......... 28% 69.7M 0s\n",
            " 16400K .......... .......... .......... .......... .......... 28%  124M 0s\n",
            " 16450K .......... .......... .......... .......... .......... 28%  122M 0s\n",
            " 16500K .......... .......... .......... .......... .......... 28%  117M 0s\n",
            " 16550K .......... .......... .......... .......... .......... 29%  174M 0s\n",
            " 16600K .......... .......... .......... .......... .......... 29%  216M 0s\n",
            " 16650K .......... .......... .......... .......... .......... 29%  217M 0s\n",
            " 16700K .......... .......... .......... .......... .......... 29% 87.4M 0s\n",
            " 16750K .......... .......... .......... .......... .......... 29% 65.3M 0s\n",
            " 16800K .......... .......... .......... .......... .......... 29% 81.7M 0s\n",
            " 16850K .......... .......... .......... .......... .......... 29% 85.1M 0s\n",
            " 16900K .......... .......... .......... .......... .......... 29%  108M 0s\n",
            " 16950K .......... .......... .......... .......... .......... 29%  177M 0s\n",
            " 17000K .......... .......... .......... .......... .......... 29%  199M 0s\n",
            " 17050K .......... .......... .......... .......... .......... 29%  211M 0s\n",
            " 17100K .......... .......... .......... .......... .......... 30%  214M 0s\n",
            " 17150K .......... .......... .......... .......... .......... 30%  167M 0s\n",
            " 17200K .......... .......... .......... .......... .......... 30%  167M 0s\n",
            " 17250K .......... .......... .......... .......... .......... 30% 78.8M 0s\n",
            " 17300K .......... .......... .......... .......... .......... 30%  103M 0s\n",
            " 17350K .......... .......... .......... .......... .......... 30% 69.0M 0s\n",
            " 17400K .......... .......... .......... .......... .......... 30%  126M 0s\n",
            " 17450K .......... .......... .......... .......... .......... 30%  212M 0s\n",
            " 17500K .......... .......... .......... .......... .......... 30%  207M 0s\n",
            " 17550K .......... .......... .......... .......... .......... 30% 67.8M 0s\n",
            " 17600K .......... .......... .......... .......... .......... 30%  121M 0s\n",
            " 17650K .......... .......... .......... .......... .......... 30%  184M 0s\n",
            " 17700K .......... .......... .......... .......... .......... 31%  111M 0s\n",
            " 17750K .......... .......... .......... .......... .......... 31%  115M 0s\n",
            " 17800K .......... .......... .......... .......... .......... 31%  182M 0s\n",
            " 17850K .......... .......... .......... .......... .......... 31%  130M 0s\n",
            " 17900K .......... .......... .......... .......... .......... 31%  199M 0s\n",
            " 17950K .......... .......... .......... .......... .......... 31%  156M 0s\n",
            " 18000K .......... .......... .......... .......... .......... 31%  198M 0s\n",
            " 18050K .......... .......... .......... .......... .......... 31%  196M 0s\n",
            " 18100K .......... .......... .......... .......... .......... 31%  187M 0s\n",
            " 18150K .......... .......... .......... .......... .......... 31% 68.6M 0s\n",
            " 18200K .......... .......... .......... .......... .......... 31% 99.6M 0s\n",
            " 18250K .......... .......... .......... .......... .......... 32%  138M 0s\n",
            " 18300K .......... .......... .......... .......... .......... 32%  223M 0s\n",
            " 18350K .......... .......... .......... .......... .......... 32%  193M 0s\n",
            " 18400K .......... .......... .......... .......... .......... 32%  191M 0s\n",
            " 18450K .......... .......... .......... .......... .......... 32% 78.3M 0s\n",
            " 18500K .......... .......... .......... .......... .......... 32% 93.5M 0s\n",
            " 18550K .......... .......... .......... .......... .......... 32% 75.6M 0s\n",
            " 18600K .......... .......... .......... .......... .......... 32%  111M 0s\n",
            " 18650K .......... .......... .......... .......... .......... 32%  174M 0s\n",
            " 18700K .......... .......... .......... .......... .......... 32% 89.5M 0s\n",
            " 18750K .......... .......... .......... .......... .......... 32%  115M 0s\n",
            " 18800K .......... .......... .......... .......... .......... 33%  204M 0s\n",
            " 18850K .......... .......... .......... .......... .......... 33%  204M 0s\n",
            " 18900K .......... .......... .......... .......... .......... 33%  187M 0s\n",
            " 18950K .......... .......... .......... .......... .......... 33%  168M 0s\n",
            " 19000K .......... .......... .......... .......... .......... 33%  213M 0s\n",
            " 19050K .......... .......... .......... .......... .......... 33% 88.7M 0s\n",
            " 19100K .......... .......... .......... .......... .......... 33% 90.9M 0s\n",
            " 19150K .......... .......... .......... .......... .......... 33%  100M 0s\n",
            " 19200K .......... .......... .......... .......... .......... 33%  202M 0s\n",
            " 19250K .......... .......... .......... .......... .......... 33%  190M 0s\n",
            " 19300K .......... .......... .......... .......... .......... 33%  170M 0s\n",
            " 19350K .......... .......... .......... .......... .......... 33% 61.6M 0s\n",
            " 19400K .......... .......... .......... .......... .......... 34% 67.7M 0s\n",
            " 19450K .......... .......... .......... .......... .......... 34%  188M 0s\n",
            " 19500K .......... .......... .......... .......... .......... 34%  186M 0s\n",
            " 19550K .......... .......... .......... .......... .......... 34%  106M 0s\n",
            " 19600K .......... .......... .......... .......... .......... 34%  202M 0s\n",
            " 19650K .......... .......... .......... .......... .......... 34% 94.9M 0s\n",
            " 19700K .......... .......... .......... .......... .......... 34%  201M 0s\n",
            " 19750K .......... .......... .......... .......... .......... 34%  170M 0s\n",
            " 19800K .......... .......... .......... .......... .......... 34%  194M 0s\n",
            " 19850K .......... .......... .......... .......... .......... 34%  191M 0s\n",
            " 19900K .......... .......... .......... .......... .......... 34%  207M 0s\n",
            " 19950K .......... .......... .......... .......... .......... 35%  168M 0s\n",
            " 20000K .......... .......... .......... .......... .......... 35% 77.1M 0s\n",
            " 20050K .......... .......... .......... .......... .......... 35% 95.2M 0s\n",
            " 20100K .......... .......... .......... .......... .......... 35%  101M 0s\n",
            " 20150K .......... .......... .......... .......... .......... 35%  198M 0s\n",
            " 20200K .......... .......... .......... .......... .......... 35%  230M 0s\n",
            " 20250K .......... .......... .......... .......... .......... 35%  187M 0s\n",
            " 20300K .......... .......... .......... .......... .......... 35% 76.6M 0s\n",
            " 20350K .......... .......... .......... .......... .......... 35%  162M 0s\n",
            " 20400K .......... .......... .......... .......... .......... 35%  122M 0s\n",
            " 20450K .......... .......... .......... .......... .......... 35%  194M 0s\n",
            " 20500K .......... .......... .......... .......... .......... 35%  129M 0s\n",
            " 20550K .......... .......... .......... .......... .......... 36%  117M 0s\n",
            " 20600K .......... .......... .......... .......... .......... 36%  181M 0s\n",
            " 20650K .......... .......... .......... .......... .......... 36% 85.0M 0s\n",
            " 20700K .......... .......... .......... .......... .......... 36%  209M 0s\n",
            " 20750K .......... .......... .......... .......... .......... 36%  171M 0s\n",
            " 20800K .......... .......... .......... .......... .......... 36%  231M 0s\n",
            " 20850K .......... .......... .......... .......... .......... 36%  232M 0s\n",
            " 20900K .......... .......... .......... .......... .......... 36%  221M 0s\n",
            " 20950K .......... .......... .......... .......... .......... 36%  203M 0s\n",
            " 21000K .......... .......... .......... .......... .......... 36%  114M 0s\n",
            " 21050K .......... .......... .......... .......... .......... 36% 85.2M 0s\n",
            " 21100K .......... .......... .......... .......... .......... 37%  107M 0s\n",
            " 21150K .......... .......... .......... .......... .......... 37%  182M 0s\n",
            " 21200K .......... .......... .......... .......... .......... 37%  196M 0s\n",
            " 21250K .......... .......... .......... .......... .......... 37%  210M 0s\n",
            " 21300K .......... .......... .......... .......... .......... 37% 77.2M 0s\n",
            " 21350K .......... .......... .......... .......... .......... 37%  115M 0s\n",
            " 21400K .......... .......... .......... .......... .......... 37%  133M 0s\n",
            " 21450K .......... .......... .......... .......... .......... 37%  211M 0s\n",
            " 21500K .......... .......... .......... .......... .......... 37%  200M 0s\n",
            " 21550K .......... .......... .......... .......... .......... 37% 78.0M 0s\n",
            " 21600K .......... .......... .......... .......... .......... 37%  217M 0s\n",
            " 21650K .......... .......... .......... .......... .......... 38%  149M 0s\n",
            " 21700K .......... .......... .......... .......... .......... 38%  119M 0s\n",
            " 21750K .......... .......... .......... .......... .......... 38%  199M 0s\n",
            " 21800K .......... .......... .......... .......... .......... 38%  156M 0s\n",
            " 21850K .......... .......... .......... .......... .......... 38%  200M 0s\n",
            " 21900K .......... .......... .......... .......... .......... 38%  224M 0s\n",
            " 21950K .......... .......... .......... .......... .......... 38%  186M 0s\n",
            " 22000K .......... .......... .......... .......... .......... 38%  232M 0s\n",
            " 22050K .......... .......... .......... .......... .......... 38% 82.0M 0s\n",
            " 22100K .......... .......... .......... .......... .......... 38% 75.9M 0s\n",
            " 22150K .......... .......... .......... .......... .......... 38% 92.1M 0s\n",
            " 22200K .......... .......... .......... .......... .......... 38%  210M 0s\n",
            " 22250K .......... .......... .......... .......... .......... 39% 99.8M 0s\n",
            " 22300K .......... .......... .......... .......... .......... 39%  130M 0s\n",
            " 22350K .......... .......... .......... .......... .......... 39% 83.4M 0s\n",
            " 22400K .......... .......... .......... .......... .......... 39%  208M 0s\n",
            " 22450K .......... .......... .......... .......... .......... 39%  204M 0s\n",
            " 22500K .......... .......... .......... .......... .......... 39% 96.1M 0s\n",
            " 22550K .......... .......... .......... .......... .......... 39%  180M 0s\n",
            " 22600K .......... .......... .......... .......... .......... 39%  192M 0s\n",
            " 22650K .......... .......... .......... .......... .......... 39% 72.7M 0s\n",
            " 22700K .......... .......... .......... .......... .......... 39%  118M 0s\n",
            " 22750K .......... .......... .......... .......... .......... 39%  170M 0s\n",
            " 22800K .......... .......... .......... .......... .......... 40%  191M 0s\n",
            " 22850K .......... .......... .......... .......... .......... 40%  206M 0s\n",
            " 22900K .......... .......... .......... .......... .......... 40%  120M 0s\n",
            " 22950K .......... .......... .......... .......... .......... 40% 57.7M 0s\n",
            " 23000K .......... .......... .......... .......... .......... 40%  115M 0s\n",
            " 23050K .......... .......... .......... .......... .......... 40%  186M 0s\n",
            " 23100K .......... .......... .......... .......... .......... 40%  210M 0s\n",
            " 23150K .......... .......... .......... .......... .......... 40%  176M 0s\n",
            " 23200K .......... .......... .......... .......... .......... 40%  193M 0s\n",
            " 23250K .......... .......... .......... .......... .......... 40% 64.4M 0s\n",
            " 23300K .......... .......... .......... .......... .......... 40%  118M 0s\n",
            " 23350K .......... .......... .......... .......... .......... 40%  159M 0s\n",
            " 23400K .......... .......... .......... .......... .......... 41% 79.2M 0s\n",
            " 23450K .......... .......... .......... .......... .......... 41%  185M 0s\n",
            " 23500K .......... .......... .......... .......... .......... 41%  108M 0s\n",
            " 23550K .......... .......... .......... .......... .......... 41%  181M 0s\n",
            " 23600K .......... .......... .......... .......... .......... 41%  225M 0s\n",
            " 23650K .......... .......... .......... .......... .......... 41%  233M 0s\n",
            " 23700K .......... .......... .......... .......... .......... 41%  230M 0s\n",
            " 23750K .......... .......... .......... .......... .......... 41%  204M 0s\n",
            " 23800K .......... .......... .......... .......... .......... 41%  225M 0s\n",
            " 23850K .......... .......... .......... .......... .......... 41%  229M 0s\n",
            " 23900K .......... .......... .......... .......... .......... 41%  230M 0s\n",
            " 23950K .......... .......... .......... .......... .......... 42%  192M 0s\n",
            " 24000K .......... .......... .......... .......... .......... 42% 91.3M 0s\n",
            " 24050K .......... .......... .......... .......... .......... 42%  165M 0s\n",
            " 24100K .......... .......... .......... .......... .......... 42%  189M 0s\n",
            " 24150K .......... .......... .......... .......... .......... 42%  183M 0s\n",
            " 24200K .......... .......... .......... .......... .......... 42%  208M 0s\n",
            " 24250K .......... .......... .......... .......... .......... 42%  114M 0s\n",
            " 24300K .......... .......... .......... .......... .......... 42% 77.5M 0s\n",
            " 24350K .......... .......... .......... .......... .......... 42% 79.6M 0s\n",
            " 24400K .......... .......... .......... .......... .......... 42% 91.5M 0s\n",
            " 24450K .......... .......... .......... .......... .......... 42%  115M 0s\n",
            " 24500K .......... .......... .......... .......... .......... 42%  129M 0s\n",
            " 24550K .......... .......... .......... .......... .......... 43% 89.1M 0s\n",
            " 24600K .......... .......... .......... .......... .......... 43%  197M 0s\n",
            " 24650K .......... .......... .......... .......... .......... 43%  209M 0s\n",
            " 24700K .......... .......... .......... .......... .......... 43%  212M 0s\n",
            " 24750K .......... .......... .......... .......... .......... 43%  171M 0s\n",
            " 24800K .......... .......... .......... .......... .......... 43%  177M 0s\n",
            " 24850K .......... .......... .......... .......... .......... 43%  180M 0s\n",
            " 24900K .......... .......... .......... .......... .......... 43%  181M 0s\n",
            " 24950K .......... .......... .......... .......... .......... 43% 65.6M 0s\n",
            " 25000K .......... .......... .......... .......... .......... 43%  106M 0s\n",
            " 25050K .......... .......... .......... .......... .......... 43%  117M 0s\n",
            " 25100K .......... .......... .......... .......... .......... 44%  194M 0s\n",
            " 25150K .......... .......... .......... .......... .......... 44%  125M 0s\n",
            " 25200K .......... .......... .......... .......... .......... 44%  109M 0s\n",
            " 25250K .......... .......... .......... .......... .......... 44%  127M 0s\n",
            " 25300K .......... .......... .......... .......... .......... 44% 97.8M 0s\n",
            " 25350K .......... .......... .......... .......... .......... 44% 73.6M 0s\n",
            " 25400K .......... .......... .......... .......... .......... 44% 95.4M 0s\n",
            " 25450K .......... .......... .......... .......... .......... 44%  140M 0s\n",
            " 25500K .......... .......... .......... .......... .......... 44%  194M 0s\n",
            " 25550K .......... .......... .......... .......... .......... 44%  175M 0s\n",
            " 25600K .......... .......... .......... .......... .......... 44%  209M 0s\n",
            " 25650K .......... .......... .......... .......... .......... 45%  203M 0s\n",
            " 25700K .......... .......... .......... .......... .......... 45%  205M 0s\n",
            " 25750K .......... .......... .......... .......... .......... 45%  193M 0s\n",
            " 25800K .......... .......... .......... .......... .......... 45%  217M 0s\n",
            " 25850K .......... .......... .......... .......... .......... 45%  203M 0s\n",
            " 25900K .......... .......... .......... .......... .......... 45%  103M 0s\n",
            " 25950K .......... .......... .......... .......... .......... 45%  112M 0s\n",
            " 26000K .......... .......... .......... .......... .......... 45% 93.0M 0s\n",
            " 26050K .......... .......... .......... .......... .......... 45% 89.7M 0s\n",
            " 26100K .......... .......... .......... .......... .......... 45% 70.2M 0s\n",
            " 26150K .......... .......... .......... .......... .......... 45% 77.0M 0s\n",
            " 26200K .......... .......... .......... .......... .......... 45%  116M 0s\n",
            " 26250K .......... .......... .......... .......... .......... 46%  202M 0s\n",
            " 26300K .......... .......... .......... .......... .......... 46%  126M 0s\n",
            " 26350K .......... .......... .......... .......... .......... 46%  114M 0s\n",
            " 26400K .......... .......... .......... .......... .......... 46%  201M 0s\n",
            " 26450K .......... .......... .......... .......... .......... 46%  211M 0s\n",
            " 26500K .......... .......... .......... .......... .......... 46%  202M 0s\n",
            " 26550K .......... .......... .......... .......... .......... 46%  193M 0s\n",
            " 26600K .......... .......... .......... .......... .......... 46%  198M 0s\n",
            " 26650K .......... .......... .......... .......... .......... 46%  204M 0s\n",
            " 26700K .......... .......... .......... .......... .......... 46%  201M 0s\n",
            " 26750K .......... .......... .......... .......... .......... 46%  176M 0s\n",
            " 26800K .......... .......... .......... .......... .......... 47%  207M 0s\n",
            " 26850K .......... .......... .......... .......... .......... 47%  206M 0s\n",
            " 26900K .......... .......... .......... .......... .......... 47% 96.2M 0s\n",
            " 26950K .......... .......... .......... .......... .......... 47% 77.3M 0s\n",
            " 27000K .......... .......... .......... .......... .......... 47% 91.0M 0s\n",
            " 27050K .......... .......... .......... .......... .......... 47%  109M 0s\n",
            " 27100K .......... .......... .......... .......... .......... 47%  102M 0s\n",
            " 27150K .......... .......... .......... .......... .......... 47% 69.0M 0s\n",
            " 27200K .......... .......... .......... .......... .......... 47%  128M 0s\n",
            " 27250K .......... .......... .......... .......... .......... 47%  125M 0s\n",
            " 27300K .......... .......... .......... .......... .......... 47%  185M 0s\n",
            " 27350K .......... .......... .......... .......... .......... 47%  133M 0s\n",
            " 27400K .......... .......... .......... .......... .......... 48%  225M 0s\n",
            " 27450K .......... .......... .......... .......... .......... 48%  170M 0s\n",
            " 27500K .......... .......... .......... .......... .......... 48%  203M 0s\n",
            " 27550K .......... .......... .......... .......... .......... 48%  157M 0s\n",
            " 27600K .......... .......... .......... .......... .......... 48%  194M 0s\n",
            " 27650K .......... .......... .......... .......... .......... 48%  191M 0s\n",
            " 27700K .......... .......... .......... .......... .......... 48%  202M 0s\n",
            " 27750K .......... .......... .......... .......... .......... 48%  162M 0s\n",
            " 27800K .......... .......... .......... .......... .......... 48%  189M 0s\n",
            " 27850K .......... .......... .......... .......... .......... 48% 85.6M 0s\n",
            " 27900K .......... .......... .......... .......... .......... 48%  116M 0s\n",
            " 27950K .......... .......... .......... .......... .......... 49% 79.7M 0s\n",
            " 28000K .......... .......... .......... .......... .......... 49% 90.0M 0s\n",
            " 28050K .......... .......... .......... .......... .......... 49%  136M 0s\n",
            " 28100K .......... .......... .......... .......... .......... 49% 86.5M 0s\n",
            " 28150K .......... .......... .......... .......... .......... 49%  135M 0s\n",
            " 28200K .......... .......... .......... .......... .......... 49%  124M 0s\n",
            " 28250K .......... .......... .......... .......... .......... 49%  101M 0s\n",
            " 28300K .......... .......... .......... .......... .......... 49%  124M 0s\n",
            " 28350K .......... .......... .......... .......... .......... 49%  105M 0s\n",
            " 28400K .......... .......... .......... .......... .......... 49%  213M 0s\n",
            " 28450K .......... .......... .......... .......... .......... 49%  218M 0s\n",
            " 28500K .......... .......... .......... .......... .......... 50%  210M 0s\n",
            " 28550K .......... .......... .......... .......... .......... 50%  181M 0s\n",
            " 28600K .......... .......... .......... .......... .......... 50%  232M 0s\n",
            " 28650K .......... .......... .......... .......... .......... 50%  228M 0s\n",
            " 28700K .......... .......... .......... .......... .......... 50%  224M 0s\n",
            " 28750K .......... .......... .......... .......... .......... 50%  163M 0s\n",
            " 28800K .......... .......... .......... .......... .......... 50%  212M 0s\n",
            " 28850K .......... .......... .......... .......... .......... 50% 93.6M 0s\n",
            " 28900K .......... .......... .......... .......... .......... 50% 78.4M 0s\n",
            " 28950K .......... .......... .......... .......... .......... 50% 61.1M 0s\n",
            " 29000K .......... .......... .......... .......... .......... 50% 70.9M 0s\n",
            " 29050K .......... .......... .......... .......... .......... 50% 85.2M 0s\n",
            " 29100K .......... .......... .......... .......... .......... 51%  127M 0s\n",
            " 29150K .......... .......... .......... .......... .......... 51% 84.8M 0s\n",
            " 29200K .......... .......... .......... .......... .......... 51%  134M 0s\n",
            " 29250K .......... .......... .......... .......... .......... 51%  120M 0s\n",
            " 29300K .......... .......... .......... .......... .......... 51%  212M 0s\n",
            " 29350K .......... .......... .......... .......... .......... 51%  175M 0s\n",
            " 29400K .......... .......... .......... .......... .......... 51%  205M 0s\n",
            " 29450K .......... .......... .......... .......... .......... 51%  215M 0s\n",
            " 29500K .......... .......... .......... .......... .......... 51%  217M 0s\n",
            " 29550K .......... .......... .......... .......... .......... 51%  174M 0s\n",
            " 29600K .......... .......... .......... .......... .......... 51%  194M 0s\n",
            " 29650K .......... .......... .......... .......... .......... 52%  187M 0s\n",
            " 29700K .......... .......... .......... .......... .......... 52% 88.2M 0s\n",
            " 29750K .......... .......... .......... .......... .......... 52% 92.3M 0s\n",
            " 29800K .......... .......... .......... .......... .......... 52%  128M 0s\n",
            " 29850K .......... .......... .......... .......... .......... 52% 89.2M 0s\n",
            " 29900K .......... .......... .......... .......... .......... 52%  126M 0s\n",
            " 29950K .......... .......... .......... .......... .......... 52% 84.9M 0s\n",
            " 30000K .......... .......... .......... .......... .......... 52%  129M 0s\n",
            " 30050K .......... .......... .......... .......... .......... 52%  212M 0s\n",
            " 30100K .......... .......... .......... .......... .......... 52% 82.3M 0s\n",
            " 30150K .......... .......... .......... .......... .......... 52%  118M 0s\n",
            " 30200K .......... .......... .......... .......... .......... 52%  105M 0s\n",
            " 30250K .......... .......... .......... .......... .......... 53%  206M 0s\n",
            " 30300K .......... .......... .......... .......... .......... 53%  194M 0s\n",
            " 30350K .......... .......... .......... .......... .......... 53%  164M 0s\n",
            " 30400K .......... .......... .......... .......... .......... 53%  210M 0s\n",
            " 30450K .......... .......... .......... .......... .......... 53%  186M 0s\n",
            " 30500K .......... .......... .......... .......... .......... 53%  178M 0s\n",
            " 30550K .......... .......... .......... .......... .......... 53%  151M 0s\n",
            " 30600K .......... .......... .......... .......... .......... 53% 86.4M 0s\n",
            " 30650K .......... .......... .......... .......... .......... 53% 83.3M 0s\n",
            " 30700K .......... .......... .......... .......... .......... 53% 90.5M 0s\n",
            " 30750K .......... .......... .......... .......... .......... 53% 91.2M 0s\n",
            " 30800K .......... .......... .......... .......... .......... 54% 97.8M 0s\n",
            " 30850K .......... .......... .......... .......... .......... 54%  129M 0s\n",
            " 30900K .......... .......... .......... .......... .......... 54%  119M 0s\n",
            " 30950K .......... .......... .......... .......... .......... 54%  191M 0s\n",
            " 31000K .......... .......... .......... .......... .......... 54%  122M 0s\n",
            " 31050K .......... .......... .......... .......... .......... 54%  116M 0s\n",
            " 31100K .......... .......... .......... .......... .......... 54%  114M 0s\n",
            " 31150K .......... .......... .......... .......... .......... 54%  173M 0s\n",
            " 31200K .......... .......... .......... .......... .......... 54%  208M 0s\n",
            " 31250K .......... .......... .......... .......... .......... 54%  220M 0s\n",
            " 31300K .......... .......... .......... .......... .......... 54%  208M 0s\n",
            " 31350K .......... .......... .......... .......... .......... 54%  188M 0s\n",
            " 31400K .......... .......... .......... .......... .......... 55%  211M 0s\n",
            " 31450K .......... .......... .......... .......... .......... 55%  212M 0s\n",
            " 31500K .......... .......... .......... .......... .......... 55%  175M 0s\n",
            " 31550K .......... .......... .......... .......... .......... 55% 81.0M 0s\n",
            " 31600K .......... .......... .......... .......... .......... 55%  111M 0s\n",
            " 31650K .......... .......... .......... .......... .......... 55%  103M 0s\n",
            " 31700K .......... .......... .......... .......... .......... 55%  102M 0s\n",
            " 31750K .......... .......... .......... .......... .......... 55% 99.9M 0s\n",
            " 31800K .......... .......... .......... .......... .......... 55% 84.2M 0s\n",
            " 31850K .......... .......... .......... .......... .......... 55%  136M 0s\n",
            " 31900K .......... .......... .......... .......... .......... 55%  134M 0s\n",
            " 31950K .......... .......... .......... .......... .......... 56%  105M 0s\n",
            " 32000K .......... .......... .......... .......... .......... 56%  134M 0s\n",
            " 32050K .......... .......... .......... .......... .......... 56%  114M 0s\n",
            " 32100K .......... .......... .......... .......... .......... 56%  194M 0s\n",
            " 32150K .......... .......... .......... .......... .......... 56%  171M 0s\n",
            " 32200K .......... .......... .......... .......... .......... 56%  191M 0s\n",
            " 32250K .......... .......... .......... .......... .......... 56%  192M 0s\n",
            " 32300K .......... .......... .......... .......... .......... 56%  181M 0s\n",
            " 32350K .......... .......... .......... .......... .......... 56%  174M 0s\n",
            " 32400K .......... .......... .......... .......... .......... 56%  201M 0s\n",
            " 32450K .......... .......... .......... .......... .......... 56%  217M 0s\n",
            " 32500K .......... .......... .......... .......... .......... 57%  215M 0s\n",
            " 32550K .......... .......... .......... .......... .......... 57% 64.8M 0s\n",
            " 32600K .......... .......... .......... .......... .......... 57%  105M 0s\n",
            " 32650K .......... .......... .......... .......... .......... 57% 89.3M 0s\n",
            " 32700K .......... .......... .......... .......... .......... 57% 94.0M 0s\n",
            " 32750K .......... .......... .......... .......... .......... 57% 90.4M 0s\n",
            " 32800K .......... .......... .......... .......... .......... 57% 62.0M 0s\n",
            " 32850K .......... .......... .......... .......... .......... 57%  106M 0s\n",
            " 32900K .......... .......... .......... .......... .......... 57% 82.4M 0s\n",
            " 32950K .......... .......... .......... .......... .......... 57%  188M 0s\n",
            " 33000K .......... .......... .......... .......... .......... 57%  209M 0s\n",
            " 33050K .......... .......... .......... .......... .......... 57%  181M 0s\n",
            " 33100K .......... .......... .......... .......... .......... 58%  186M 0s\n",
            " 33150K .......... .......... .......... .......... .......... 58%  159M 0s\n",
            " 33200K .......... .......... .......... .......... .......... 58%  197M 0s\n",
            " 33250K .......... .......... .......... .......... .......... 58%  192M 0s\n",
            " 33300K .......... .......... .......... .......... .......... 58%  196M 0s\n",
            " 33350K .......... .......... .......... .......... .......... 58%  164M 0s\n",
            " 33400K .......... .......... .......... .......... .......... 58% 69.7M 0s\n",
            " 33450K .......... .......... .......... .......... .......... 58%  112M 0s\n",
            " 33500K .......... .......... .......... .......... .......... 58% 65.7M 0s\n",
            " 33550K .......... .......... .......... .......... .......... 58% 80.3M 0s\n",
            " 33600K .......... .......... .......... .......... .......... 58% 83.9M 0s\n",
            " 33650K .......... .......... .......... .......... .......... 59%  122M 0s\n",
            " 33700K .......... .......... .......... .......... .......... 59%  133M 0s\n",
            " 33750K .......... .......... .......... .......... .......... 59%  189M 0s\n",
            " 33800K .......... .......... .......... .......... .......... 59%  126M 0s\n",
            " 33850K .......... .......... .......... .......... .......... 59%  213M 0s\n",
            " 33900K .......... .......... .......... .......... .......... 59%  209M 0s\n",
            " 33950K .......... .......... .......... .......... .......... 59%  174M 0s\n",
            " 34000K .......... .......... .......... .......... .......... 59%  191M 0s\n",
            " 34050K .......... .......... .......... .......... .......... 59%  175M 0s\n",
            " 34100K .......... .......... .......... .......... .......... 59%  187M 0s\n",
            " 34150K .......... .......... .......... .......... .......... 59%  173M 0s\n",
            " 34200K .......... .......... .......... .......... .......... 59%  168M 0s\n",
            " 34250K .......... .......... .......... .......... .......... 60%  199M 0s\n",
            " 34300K .......... .......... .......... .......... .......... 60%  213M 0s\n",
            " 34350K .......... .......... .......... .......... .......... 60% 93.4M 0s\n",
            " 34400K .......... .......... .......... .......... .......... 60%  122M 0s\n",
            " 34450K .......... .......... .......... .......... .......... 60% 85.6M 0s\n",
            " 34500K .......... .......... .......... .......... .......... 60%  107M 0s\n",
            " 34550K .......... .......... .......... .......... .......... 60% 74.5M 0s\n",
            " 34600K .......... .......... .......... .......... .......... 60%  119M 0s\n",
            " 34650K .......... .......... .......... .......... .......... 60% 91.9M 0s\n",
            " 34700K .......... .......... .......... .......... .......... 60%  123M 0s\n",
            " 34750K .......... .......... .......... .......... .......... 60% 88.5M 0s\n",
            " 34800K .......... .......... .......... .......... .......... 61%  150M 0s\n",
            " 34850K .......... .......... .......... .......... .......... 61%  210M 0s\n",
            " 34900K .......... .......... .......... .......... .......... 61%  207M 0s\n",
            " 34950K .......... .......... .......... .......... .......... 61%  180M 0s\n",
            " 35000K .......... .......... .......... .......... .......... 61%  190M 0s\n",
            " 35050K .......... .......... .......... .......... .......... 61%  191M 0s\n",
            " 35100K .......... .......... .......... .......... .......... 61%  202M 0s\n",
            " 35150K .......... .......... .......... .......... .......... 61%  154M 0s\n",
            " 35200K .......... .......... .......... .......... .......... 61%  198M 0s\n",
            " 35250K .......... .......... .......... .......... .......... 61%  207M 0s\n",
            " 35300K .......... .......... .......... .......... .......... 61% 93.1M 0s\n",
            " 35350K .......... .......... .......... .......... .......... 61%  122M 0s\n",
            " 35400K .......... .......... .......... .......... .......... 62% 86.1M 0s\n",
            " 35450K .......... .......... .......... .......... .......... 62% 77.4M 0s\n",
            " 35500K .......... .......... .......... .......... .......... 62% 71.1M 0s\n",
            " 35550K .......... .......... .......... .......... .......... 62% 52.5M 0s\n",
            " 35600K .......... .......... .......... .......... .......... 62% 68.5M 0s\n",
            " 35650K .......... .......... .......... .......... .......... 62%  113M 0s\n",
            " 35700K .......... .......... .......... .......... .......... 62%  211M 0s\n",
            " 35750K .......... .......... .......... .......... .......... 62%  181M 0s\n",
            " 35800K .......... .......... .......... .......... .......... 62%  193M 0s\n",
            " 35850K .......... .......... .......... .......... .......... 62%  198M 0s\n",
            " 35900K .......... .......... .......... .......... .......... 62%  186M 0s\n",
            " 35950K .......... .......... .......... .......... .......... 63%  141M 0s\n",
            " 36000K .......... .......... .......... .......... .......... 63%  187M 0s\n",
            " 36050K .......... .......... .......... .......... .......... 63%  186M 0s\n",
            " 36100K .......... .......... .......... .......... .......... 63% 59.2M 0s\n",
            " 36150K .......... .......... .......... .......... .......... 63% 53.1M 0s\n",
            " 36200K .......... .......... .......... .......... .......... 63%  196M 0s\n",
            " 36250K .......... .......... .......... .......... .......... 63%  190M 0s\n",
            " 36300K .......... .......... .......... .......... .......... 63%  215M 0s\n",
            " 36350K .......... .......... .......... .......... .......... 63% 77.9M 0s\n",
            " 36400K .......... .......... .......... .......... .......... 63% 82.6M 0s\n",
            " 36450K .......... .......... .......... .......... .......... 63% 64.9M 0s\n",
            " 36500K .......... .......... .......... .......... .......... 64%  102M 0s\n",
            " 36550K .......... .......... .......... .......... .......... 64%  112M 0s\n",
            " 36600K .......... .......... .......... .......... .......... 64%  227M 0s\n",
            " 36650K .......... .......... .......... .......... .......... 64%  234M 0s\n",
            " 36700K .......... .......... .......... .......... .......... 64%  224M 0s\n",
            " 36750K .......... .......... .......... .......... .......... 64%  184M 0s\n",
            " 36800K .......... .......... .......... .......... .......... 64%  207M 0s\n",
            " 36850K .......... .......... .......... .......... .......... 64%  211M 0s\n",
            " 36900K .......... .......... .......... .......... .......... 64%  209M 0s\n",
            " 36950K .......... .......... .......... .......... .......... 64% 86.9M 0s\n",
            " 37000K .......... .......... .......... .......... .......... 64%  125M 0s\n",
            " 37050K .......... .......... .......... .......... .......... 64% 96.7M 0s\n",
            " 37100K .......... .......... .......... .......... .......... 65%  127M 0s\n",
            " 37150K .......... .......... .......... .......... .......... 65% 92.6M 0s\n",
            " 37200K .......... .......... .......... .......... .......... 65% 58.8M 0s\n",
            " 37250K .......... .......... .......... .......... .......... 65%  111M 0s\n",
            " 37300K .......... .......... .......... .......... .......... 65%  227M 0s\n",
            " 37350K .......... .......... .......... .......... .......... 65% 97.5M 0s\n",
            " 37400K .......... .......... .......... .......... .......... 65%  155M 0s\n",
            " 37450K .......... .......... .......... .......... .......... 65%  133M 0s\n",
            " 37500K .......... .......... .......... .......... .......... 65% 79.9M 0s\n",
            " 37550K .......... .......... .......... .......... .......... 65%  195M 0s\n",
            " 37600K .......... .......... .......... .......... .......... 65%  230M 0s\n",
            " 37650K .......... .......... .......... .......... .......... 66%  224M 0s\n",
            " 37700K .......... .......... .......... .......... .......... 66%  235M 0s\n",
            " 37750K .......... .......... .......... .......... .......... 66%  179M 0s\n",
            " 37800K .......... .......... .......... .......... .......... 66%  229M 0s\n",
            " 37850K .......... .......... .......... .......... .......... 66%  220M 0s\n",
            " 37900K .......... .......... .......... .......... .......... 66% 44.9M 0s\n",
            " 37950K .......... .......... .......... .......... .......... 66% 73.8M 0s\n",
            " 38000K .......... .......... .......... .......... .......... 66%  117M 0s\n",
            " 38050K .......... .......... .......... .......... .......... 66% 65.9M 0s\n",
            " 38100K .......... .......... .......... .......... .......... 66%  106M 0s\n",
            " 38150K .......... .......... .......... .......... .......... 66% 39.3M 0s\n",
            " 38200K .......... .......... .......... .......... .......... 66%  172M 0s\n",
            " 38250K .......... .......... .......... .......... .......... 67%  102M 0s\n",
            " 38300K .......... .......... .......... .......... .......... 67%  209M 0s\n",
            " 38350K .......... .......... .......... .......... .......... 67%  171M 0s\n",
            " 38400K .......... .......... .......... .......... .......... 67%  211M 0s\n",
            " 38450K .......... .......... .......... .......... .......... 67%  218M 0s\n",
            " 38500K .......... .......... .......... .......... .......... 67%  228M 0s\n",
            " 38550K .......... .......... .......... .......... .......... 67%  201M 0s\n",
            " 38600K .......... .......... .......... .......... .......... 67%  206M 0s\n",
            " 38650K .......... .......... .......... .......... .......... 67% 80.9M 0s\n",
            " 38700K .......... .......... .......... .......... .......... 67% 64.7M 0s\n",
            " 38750K .......... .......... .......... .......... .......... 67% 57.9M 0s\n",
            " 38800K .......... .......... .......... .......... .......... 68%  102M 0s\n",
            " 38850K .......... .......... .......... .......... .......... 68% 66.7M 0s\n",
            " 38900K .......... .......... .......... .......... .......... 68%  102M 0s\n",
            " 38950K .......... .......... .......... .......... .......... 68%  180M 0s\n",
            " 39000K .......... .......... .......... .......... .......... 68%  229M 0s\n",
            " 39050K .......... .......... .......... .......... .......... 68%  190M 0s\n",
            " 39100K .......... .......... .......... .......... .......... 68%  175M 0s\n",
            " 39150K .......... .......... .......... .......... .......... 68% 64.5M 0s\n",
            " 39200K .......... .......... .......... .......... .......... 68%  213M 0s\n",
            " 39250K .......... .......... .......... .......... .......... 68%  212M 0s\n",
            " 39300K .......... .......... .......... .......... .......... 68%  232M 0s\n",
            " 39350K .......... .......... .......... .......... .......... 69%  213M 0s\n",
            " 39400K .......... .......... .......... .......... .......... 69%  220M 0s\n",
            " 39450K .......... .......... .......... .......... .......... 69% 83.7M 0s\n",
            " 39500K .......... .......... .......... .......... .......... 69% 95.8M 0s\n",
            " 39550K .......... .......... .......... .......... .......... 69% 83.6M 0s\n",
            " 39600K .......... .......... .......... .......... .......... 69% 69.1M 0s\n",
            " 39650K .......... .......... .......... .......... .......... 69%  128M 0s\n",
            " 39700K .......... .......... .......... .......... .......... 69%  127M 0s\n",
            " 39750K .......... .......... .......... .......... .......... 69%  188M 0s\n",
            " 39800K .......... .......... .......... .......... .......... 69%  225M 0s\n",
            " 39850K .......... .......... .......... .......... .......... 69%  232M 0s\n",
            " 39900K .......... .......... .......... .......... .......... 69%  103M 0s\n",
            " 39950K .......... .......... .......... .......... .......... 70%  171M 0s\n",
            " 40000K .......... .......... .......... .......... .......... 70%  212M 0s\n",
            " 40050K .......... .......... .......... .......... .......... 70%  182M 0s\n",
            " 40100K .......... .......... .......... .......... .......... 70% 83.4M 0s\n",
            " 40150K .......... .......... .......... .......... .......... 70% 71.8M 0s\n",
            " 40200K .......... .......... .......... .......... .......... 70%  184M 0s\n",
            " 40250K .......... .......... .......... .......... .......... 70%  192M 0s\n",
            " 40300K .......... .......... .......... .......... .......... 70%  199M 0s\n",
            " 40350K .......... .......... .......... .......... .......... 70% 55.1M 0s\n",
            " 40400K .......... .......... .......... .......... .......... 70% 82.2M 0s\n",
            " 40450K .......... .......... .......... .......... .......... 70% 83.5M 0s\n",
            " 40500K .......... .......... .......... .......... .......... 71%  111M 0s\n",
            " 40550K .......... .......... .......... .......... .......... 71%  164M 0s\n",
            " 40600K .......... .......... .......... .......... .......... 71% 84.9M 0s\n",
            " 40650K .......... .......... .......... .......... .......... 71%  203M 0s\n",
            " 40700K .......... .......... .......... .......... .......... 71%  113M 0s\n",
            " 40750K .......... .......... .......... .......... .......... 71%  121M 0s\n",
            " 40800K .......... .......... .......... .......... .......... 71%  215M 0s\n",
            " 40850K .......... .......... .......... .......... .......... 71%  206M 0s\n",
            " 40900K .......... .......... .......... .......... .......... 71%  197M 0s\n",
            " 40950K .......... .......... .......... .......... .......... 71% 79.0M 0s\n",
            " 41000K .......... .......... .......... .......... .......... 71% 95.5M 0s\n",
            " 41050K .......... .......... .......... .......... .......... 71%  131M 0s\n",
            " 41100K .......... .......... .......... .......... .......... 72%  173M 0s\n",
            " 41150K .......... .......... .......... .......... .......... 72%  168M 0s\n",
            " 41200K .......... .......... .......... .......... .......... 72%  132M 0s\n",
            " 41250K .......... .......... .......... .......... .......... 72% 87.9M 0s\n",
            " 41300K .......... .......... .......... .......... .......... 72%  222M 0s\n",
            " 41350K .......... .......... .......... .......... .......... 72%  203M 0s\n",
            " 41400K .......... .......... .......... .......... .......... 72%  126M 0s\n",
            " 41450K .......... .......... .......... .......... .......... 72%  106M 0s\n",
            " 41500K .......... .......... .......... .......... .......... 72%  198M 0s\n",
            " 41550K .......... .......... .......... .......... .......... 72% 97.1M 0s\n",
            " 41600K .......... .......... .......... .......... .......... 72%  130M 0s\n",
            " 41650K .......... .......... .......... .......... .......... 73%  131M 0s\n",
            " 41700K .......... .......... .......... .......... .......... 73%  120M 0s\n",
            " 41750K .......... .......... .......... .......... .......... 73%  187M 0s\n",
            " 41800K .......... .......... .......... .......... .......... 73%  190M 0s\n",
            " 41850K .......... .......... .......... .......... .......... 73%  198M 0s\n",
            " 41900K .......... .......... .......... .......... .......... 73%  145M 0s\n",
            " 41950K .......... .......... .......... .......... .......... 73%  166M 0s\n",
            " 42000K .......... .......... .......... .......... .......... 73%  210M 0s\n",
            " 42050K .......... .......... .......... .......... .......... 73%  118M 0s\n",
            " 42100K .......... .......... .......... .......... .......... 73% 97.3M 0s\n",
            " 42150K .......... .......... .......... .......... .......... 73% 85.2M 0s\n",
            " 42200K .......... .......... .......... .......... .......... 73%  106M 0s\n",
            " 42250K .......... .......... .......... .......... .......... 74%  112M 0s\n",
            " 42300K .......... .......... .......... .......... .......... 74%  130M 0s\n",
            " 42350K .......... .......... .......... .......... .......... 74% 62.7M 0s\n",
            " 42400K .......... .......... .......... .......... .......... 74%  182M 0s\n",
            " 42450K .......... .......... .......... .......... .......... 74%  127M 0s\n",
            " 42500K .......... .......... .......... .......... .......... 74%  126M 0s\n",
            " 42550K .......... .......... .......... .......... .......... 74%  160M 0s\n",
            " 42600K .......... .......... .......... .......... .......... 74% 87.4M 0s\n",
            " 42650K .......... .......... .......... .......... .......... 74%  208M 0s\n",
            " 42700K .......... .......... .......... .......... .......... 74%  204M 0s\n",
            " 42750K .......... .......... .......... .......... .......... 74%  149M 0s\n",
            " 42800K .......... .......... .......... .......... .......... 75% 68.2M 0s\n",
            " 42850K .......... .......... .......... .......... .......... 75%  111M 0s\n",
            " 42900K .......... .......... .......... .......... .......... 75%  154M 0s\n",
            " 42950K .......... .......... .......... .......... .......... 75%  108M 0s\n",
            " 43000K .......... .......... .......... .......... .......... 75%  117M 0s\n",
            " 43050K .......... .......... .......... .......... .......... 75%  176M 0s\n",
            " 43100K .......... .......... .......... .......... .......... 75%  131M 0s\n",
            " 43150K .......... .......... .......... .......... .......... 75%  106M 0s\n",
            " 43200K .......... .......... .......... .......... .......... 75%  198M 0s\n",
            " 43250K .......... .......... .......... .......... .......... 75%  117M 0s\n",
            " 43300K .......... .......... .......... .......... .......... 75%  200M 0s\n",
            " 43350K .......... .......... .......... .......... .......... 76% 82.4M 0s\n",
            " 43400K .......... .......... .......... .......... .......... 76%  128M 0s\n",
            " 43450K .......... .......... .......... .......... .......... 76%  201M 0s\n",
            " 43500K .......... .......... .......... .......... .......... 76%  132M 0s\n",
            " 43550K .......... .......... .......... .......... .......... 76%  176M 0s\n",
            " 43600K .......... .......... .......... .......... .......... 76%  207M 0s\n",
            " 43650K .......... .......... .......... .......... .......... 76%  205M 0s\n",
            " 43700K .......... .......... .......... .......... .......... 76%  201M 0s\n",
            " 43750K .......... .......... .......... .......... .......... 76% 86.9M 0s\n",
            " 43800K .......... .......... .......... .......... .......... 76%  117M 0s\n",
            " 43850K .......... .......... .......... .......... .......... 76% 91.9M 0s\n",
            " 43900K .......... .......... .......... .......... .......... 76%  191M 0s\n",
            " 43950K .......... .......... .......... .......... .......... 77% 80.5M 0s\n",
            " 44000K .......... .......... .......... .......... .......... 77%  195M 0s\n",
            " 44050K .......... .......... .......... .......... .......... 77%  116M 0s\n",
            " 44100K .......... .......... .......... .......... .......... 77%  198M 0s\n",
            " 44150K .......... .......... .......... .......... .......... 77%  112M 0s\n",
            " 44200K .......... .......... .......... .......... .......... 77%  164M 0s\n",
            " 44250K .......... .......... .......... .......... .......... 77% 60.4M 0s\n",
            " 44300K .......... .......... .......... .......... .......... 77%  148M 0s\n",
            " 44350K .......... .......... .......... .......... .......... 77% 99.5M 0s\n",
            " 44400K .......... .......... .......... .......... .......... 77%  107M 0s\n",
            " 44450K .......... .......... .......... .......... .......... 77%  233M 0s\n",
            " 44500K .......... .......... .......... .......... .......... 78%  200M 0s\n",
            " 44550K .......... .......... .......... .......... .......... 78%  189M 0s\n",
            " 44600K .......... .......... .......... .......... .......... 78%  196M 0s\n",
            " 44650K .......... .......... .......... .......... .......... 78% 75.0M 0s\n",
            " 44700K .......... .......... .......... .......... .......... 78%  163M 0s\n",
            " 44750K .......... .......... .......... .......... .......... 78% 66.7M 0s\n",
            " 44800K .......... .......... .......... .......... .......... 78%  103M 0s\n",
            " 44850K .......... .......... .......... .......... .......... 78%  193M 0s\n",
            " 44900K .......... .......... .......... .......... .......... 78%  203M 0s\n",
            " 44950K .......... .......... .......... .......... .......... 78% 65.8M 0s\n",
            " 45000K .......... .......... .......... .......... .......... 78% 88.6M 0s\n",
            " 45050K .......... .......... .......... .......... .......... 78% 78.5M 0s\n",
            " 45100K .......... .......... .......... .......... .......... 79%  107M 0s\n",
            " 45150K .......... .......... .......... .......... .......... 79%  173M 0s\n",
            " 45200K .......... .......... .......... .......... .......... 79% 72.0M 0s\n",
            " 45250K .......... .......... .......... .......... .......... 79%  178M 0s\n",
            " 45300K .......... .......... .......... .......... .......... 79%  190M 0s\n",
            " 45350K .......... .......... .......... .......... .......... 79%  187M 0s\n",
            " 45400K .......... .......... .......... .......... .......... 79%  186M 0s\n",
            " 45450K .......... .......... .......... .......... .......... 79%  221M 0s\n",
            " 45500K .......... .......... .......... .......... .......... 79% 39.5M 0s\n",
            " 45550K .......... .......... .......... .......... .......... 79% 92.0M 0s\n",
            " 45600K .......... .......... .......... .......... .......... 79%  196M 0s\n",
            " 45650K .......... .......... .......... .......... .......... 80%  203M 0s\n",
            " 45700K .......... .......... .......... .......... .......... 80%  112M 0s\n",
            " 45750K .......... .......... .......... .......... .......... 80% 70.4M 0s\n",
            " 45800K .......... .......... .......... .......... .......... 80%  210M 0s\n",
            " 45850K .......... .......... .......... .......... .......... 80%  115M 0s\n",
            " 45900K .......... .......... .......... .......... .......... 80%  219M 0s\n",
            " 45950K .......... .......... .......... .......... .......... 80%  103M 0s\n",
            " 46000K .......... .......... .......... .......... .......... 80%  234M 0s\n",
            " 46050K .......... .......... .......... .......... .......... 80%  233M 0s\n",
            " 46100K .......... .......... .......... .......... .......... 80% 81.4M 0s\n",
            " 46150K .......... .......... .......... .......... .......... 80%  119M 0s\n",
            " 46200K .......... .......... .......... .......... .......... 81%  228M 0s\n",
            " 46250K .......... .......... .......... .......... .......... 81%  227M 0s\n",
            " 46300K .......... .......... .......... .......... .......... 81%  238M 0s\n",
            " 46350K .......... .......... .......... .......... .......... 81%  188M 0s\n",
            " 46400K .......... .......... .......... .......... .......... 81%  256M 0s\n",
            " 46450K .......... .......... .......... .......... .......... 81% 97.7M 0s\n",
            " 46500K .......... .......... .......... .......... .......... 81%  104M 0s\n",
            " 46550K .......... .......... .......... .......... .......... 81% 75.8M 0s\n",
            " 46600K .......... .......... .......... .......... .......... 81%  201M 0s\n",
            " 46650K .......... .......... .......... .......... .......... 81%  164M 0s\n",
            " 46700K .......... .......... .......... .......... .......... 81% 89.4M 0s\n",
            " 46750K .......... .......... .......... .......... .......... 81%  170M 0s\n",
            " 46800K .......... .......... .......... .......... .......... 82%  132M 0s\n",
            " 46850K .......... .......... .......... .......... .......... 82%  123M 0s\n",
            " 46900K .......... .......... .......... .......... .......... 82%  204M 0s\n",
            " 46950K .......... .......... .......... .......... .......... 82%  116M 0s\n",
            " 47000K .......... .......... .......... .......... .......... 82%  202M 0s\n",
            " 47050K .......... .......... .......... .......... .......... 82%  207M 0s\n",
            " 47100K .......... .......... .......... .......... .......... 82%  137M 0s\n",
            " 47150K .......... .......... .......... .......... .......... 82% 64.7M 0s\n",
            " 47200K .......... .......... .......... .......... .......... 82%  202M 0s\n",
            " 47250K .......... .......... .......... .......... .......... 82%  182M 0s\n",
            " 47300K .......... .......... .......... .......... .......... 82%  215M 0s\n",
            " 47350K .......... .......... .......... .......... .......... 83%  190M 0s\n",
            " 47400K .......... .......... .......... .......... .......... 83%  202M 0s\n",
            " 47450K .......... .......... .......... .......... .......... 83%  147M 0s\n",
            " 47500K .......... .......... .......... .......... .......... 83%  214M 0s\n",
            " 47550K .......... .......... .......... .......... .......... 83% 78.0M 0s\n",
            " 47600K .......... .......... .......... .......... .......... 83% 97.5M 0s\n",
            " 47650K .......... .......... .......... .......... .......... 83% 65.2M 0s\n",
            " 47700K .......... .......... .......... .......... .......... 83%  114M 0s\n",
            " 47750K .......... .......... .......... .......... .......... 83% 83.1M 0s\n",
            " 47800K .......... .......... .......... .......... .......... 83%  122M 0s\n",
            " 47850K .......... .......... .......... .......... .......... 83%  161M 0s\n",
            " 47900K .......... .......... .......... .......... .......... 83% 87.9M 0s\n",
            " 47950K .......... .......... .......... .......... .......... 84% 78.2M 0s\n",
            " 48000K .......... .......... .......... .......... .......... 84%  133M 0s\n",
            " 48050K .......... .......... .......... .......... .......... 84%  214M 0s\n",
            " 48100K .......... .......... .......... .......... .......... 84%  228M 0s\n",
            " 48150K .......... .......... .......... .......... .......... 84%  202M 0s\n",
            " 48200K .......... .......... .......... .......... .......... 84%  222M 0s\n",
            " 48250K .......... .......... .......... .......... .......... 84%  208M 0s\n",
            " 48300K .......... .......... .......... .......... .......... 84%  231M 0s\n",
            " 48350K .......... .......... .......... .......... .......... 84% 49.1M 0s\n",
            " 48400K .......... .......... .......... .......... .......... 84% 94.9M 0s\n",
            " 48450K .......... .......... .......... .......... .......... 84%  124M 0s\n",
            " 48500K .......... .......... .......... .......... .......... 85%  120M 0s\n",
            " 48550K .......... .......... .......... .......... .......... 85%  174M 0s\n",
            " 48600K .......... .......... .......... .......... .......... 85%  114M 0s\n",
            " 48650K .......... .......... .......... .......... .......... 85%  213M 0s\n",
            " 48700K .......... .......... .......... .......... .......... 85%  135M 0s\n",
            " 48750K .......... .......... .......... .......... .......... 85%  169M 0s\n",
            " 48800K .......... .......... .......... .......... .......... 85%  112M 0s\n",
            " 48850K .......... .......... .......... .......... .......... 85%  126M 0s\n",
            " 48900K .......... .......... .......... .......... .......... 85%  211M 0s\n",
            " 48950K .......... .......... .......... .......... .......... 85%  120M 0s\n",
            " 49000K .......... .......... .......... .......... .......... 85%  113M 0s\n",
            " 49050K .......... .......... .......... .......... .......... 85%  203M 0s\n",
            " 49100K .......... .......... .......... .......... .......... 86%  214M 0s\n",
            " 49150K .......... .......... .......... .......... .......... 86%  173M 0s\n",
            " 49200K .......... .......... .......... .......... .......... 86%  205M 0s\n",
            " 49250K .......... .......... .......... .......... .......... 86%  192M 0s\n",
            " 49300K .......... .......... .......... .......... .......... 86% 99.2M 0s\n",
            " 49350K .......... .......... .......... .......... .......... 86% 84.3M 0s\n",
            " 49400K .......... .......... .......... .......... .......... 86% 86.0M 0s\n",
            " 49450K .......... .......... .......... .......... .......... 86% 92.5M 0s\n",
            " 49500K .......... .......... .......... .......... .......... 86%  204M 0s\n",
            " 49550K .......... .......... .......... .......... .......... 86%  115M 0s\n",
            " 49600K .......... .......... .......... .......... .......... 86%  128M 0s\n",
            " 49650K .......... .......... .......... .......... .......... 87%  130M 0s\n",
            " 49700K .......... .......... .......... .......... .......... 87%  203M 0s\n",
            " 49750K .......... .......... .......... .......... .......... 87%  115M 0s\n",
            " 49800K .......... .......... .......... .......... .......... 87%  114M 0s\n",
            " 49850K .......... .......... .......... .......... .......... 87%  212M 0s\n",
            " 49900K .......... .......... .......... .......... .......... 87%  132M 0s\n",
            " 49950K .......... .......... .......... .......... .......... 87% 82.4M 0s\n",
            " 50000K .......... .......... .......... .......... .......... 87%  200M 0s\n",
            " 50050K .......... .......... .......... .......... .......... 87%  213M 0s\n",
            " 50100K .......... .......... .......... .......... .......... 87%  214M 0s\n",
            " 50150K .......... .......... .......... .......... .......... 87%  190M 0s\n",
            " 50200K .......... .......... .......... .......... .......... 88% 50.5M 0s\n",
            " 50250K .......... .......... .......... .......... .......... 88% 56.3M 0s\n",
            " 50300K .......... .......... .......... .......... .......... 88% 79.5M 0s\n",
            " 50350K .......... .......... .......... .......... .......... 88%  166M 0s\n",
            " 50400K .......... .......... .......... .......... .......... 88% 35.0M 0s\n",
            " 50450K .......... .......... .......... .......... .......... 88% 71.2M 0s\n",
            " 50500K .......... .......... .......... .......... .......... 88%  220M 0s\n",
            " 50550K .......... .......... .......... .......... .......... 88%  195M 0s\n",
            " 50600K .......... .......... .......... .......... .......... 88% 59.9M 0s\n",
            " 50650K .......... .......... .......... .......... .......... 88%  182M 0s\n",
            " 50700K .......... .......... .......... .......... .......... 88%  219M 0s\n",
            " 50750K .......... .......... .......... .......... .......... 88%  171M 0s\n",
            " 50800K .......... .......... .......... .......... .......... 89%  208M 0s\n",
            " 50850K .......... .......... .......... .......... .......... 89%  214M 0s\n",
            " 50900K .......... .......... .......... .......... .......... 89% 39.7M 0s\n",
            " 50950K .......... .......... .......... .......... .......... 89%  180M 0s\n",
            " 51000K .......... .......... .......... .......... .......... 89%  207M 0s\n",
            " 51050K .......... .......... .......... .......... .......... 89%  122M 0s\n",
            " 51100K .......... .......... .......... .......... .......... 89%  214M 0s\n",
            " 51150K .......... .......... .......... .......... .......... 89%  106M 0s\n",
            " 51200K .......... .......... .......... .......... .......... 89%  210M 0s\n",
            " 51250K .......... .......... .......... .......... .......... 89%  209M 0s\n",
            " 51300K .......... .......... .......... .......... .......... 89%  170M 0s\n",
            " 51350K .......... .......... .......... .......... .......... 90% 89.9M 0s\n",
            " 51400K .......... .......... .......... .......... .......... 90%  192M 0s\n",
            " 51450K .......... .......... .......... .......... .......... 90%  110M 0s\n",
            " 51500K .......... .......... .......... .......... .......... 90%  105M 0s\n",
            " 51550K .......... .......... .......... .......... .......... 90% 97.8M 0s\n",
            " 51600K .......... .......... .......... .......... .......... 90%  175M 0s\n",
            " 51650K .......... .......... .......... .......... .......... 90%  179M 0s\n",
            " 51700K .......... .......... .......... .......... .......... 90%  182M 0s\n",
            " 51750K .......... .......... .......... .......... .......... 90%  141M 0s\n",
            " 51800K .......... .......... .......... .......... .......... 90% 93.3M 0s\n",
            " 51850K .......... .......... .......... .......... .......... 90%  113M 0s\n",
            " 51900K .......... .......... .......... .......... .......... 90% 88.6M 0s\n",
            " 51950K .......... .......... .......... .......... .......... 91%  131M 0s\n",
            " 52000K .......... .......... .......... .......... .......... 91% 89.9M 0s\n",
            " 52050K .......... .......... .......... .......... .......... 91%  184M 0s\n",
            " 52100K .......... .......... .......... .......... .......... 91%  120M 0s\n",
            " 52150K .......... .......... .......... .......... .......... 91%  210M 0s\n",
            " 52200K .......... .......... .......... .......... .......... 91%  131M 0s\n",
            " 52250K .......... .......... .......... .......... .......... 91% 90.8M 0s\n",
            " 52300K .......... .......... .......... .......... .......... 91%  126M 0s\n",
            " 52350K .......... .......... .......... .......... .......... 91%  167M 0s\n",
            " 52400K .......... .......... .......... .......... .......... 91%  123M 0s\n",
            " 52450K .......... .......... .......... .......... .......... 91%  108M 0s\n",
            " 52500K .......... .......... .......... .......... .......... 92%  153M 0s\n",
            " 52550K .......... .......... .......... .......... .......... 92%  185M 0s\n",
            " 52600K .......... .......... .......... .......... .......... 92%  210M 0s\n",
            " 52650K .......... .......... .......... .......... .......... 92%  189M 0s\n",
            " 52700K .......... .......... .......... .......... .......... 92%  186M 0s\n",
            " 52750K .......... .......... .......... .......... .......... 92% 87.2M 0s\n",
            " 52800K .......... .......... .......... .......... .......... 92% 96.8M 0s\n",
            " 52850K .......... .......... .......... .......... .......... 92%  132M 0s\n",
            " 52900K .......... .......... .......... .......... .......... 92%  117M 0s\n",
            " 52950K .......... .......... .......... .......... .......... 92%  178M 0s\n",
            " 53000K .......... .......... .......... .......... .......... 92%  125M 0s\n",
            " 53050K .......... .......... .......... .......... .......... 92%  112M 0s\n",
            " 53100K .......... .......... .......... .......... .......... 93%  197M 0s\n",
            " 53150K .......... .......... .......... .......... .......... 93% 76.9M 0s\n",
            " 53200K .......... .......... .......... .......... .......... 93%  134M 0s\n",
            " 53250K .......... .......... .......... .......... .......... 93%  198M 0s\n",
            " 53300K .......... .......... .......... .......... .......... 93%  232M 0s\n",
            " 53350K .......... .......... .......... .......... .......... 93%  182M 0s\n",
            " 53400K .......... .......... .......... .......... .......... 93% 93.5M 0s\n",
            " 53450K .......... .......... .......... .......... .......... 93% 98.5M 0s\n",
            " 53500K .......... .......... .......... .......... .......... 93%  207M 0s\n",
            " 53550K .......... .......... .......... .......... .......... 93%  156M 0s\n",
            " 53600K .......... .......... .......... .......... .......... 93%  195M 0s\n",
            " 53650K .......... .......... .......... .......... .......... 94%  207M 0s\n",
            " 53700K .......... .......... .......... .......... .......... 94%  173M 0s\n",
            " 53750K .......... .......... .......... .......... .......... 94%  114M 0s\n",
            " 53800K .......... .......... .......... .......... .......... 94%  177M 0s\n",
            " 53850K .......... .......... .......... .......... .......... 94%  180M 0s\n",
            " 53900K .......... .......... .......... .......... .......... 94%  210M 0s\n",
            " 53950K .......... .......... .......... .......... .......... 94%  169M 0s\n",
            " 54000K .......... .......... .......... .......... .......... 94%  232M 0s\n",
            " 54050K .......... .......... .......... .......... .......... 94%  233M 0s\n",
            " 54100K .......... .......... .......... .......... .......... 94%  212M 0s\n",
            " 54150K .......... .......... .......... .......... .......... 94%  189M 0s\n",
            " 54200K .......... .......... .......... .......... .......... 95%  195M 0s\n",
            " 54250K .......... .......... .......... .......... .......... 95%  193M 0s\n",
            " 54300K .......... .......... .......... .......... .......... 95%  207M 0s\n",
            " 54350K .......... .......... .......... .......... .......... 95%  177M 0s\n",
            " 54400K .......... .......... .......... .......... .......... 95%  204M 0s\n",
            " 54450K .......... .......... .......... .......... .......... 95%  214M 0s\n",
            " 54500K .......... .......... .......... .......... .......... 95%  205M 0s\n",
            " 54550K .......... .......... .......... .......... .......... 95%  195M 0s\n",
            " 54600K .......... .......... .......... .......... .......... 95%  176M 0s\n",
            " 54650K .......... .......... .......... .......... .......... 95%  200M 0s\n",
            " 54700K .......... .......... .......... .......... .......... 95%  199M 0s\n",
            " 54750K .......... .......... .......... .......... .......... 95%  154M 0s\n",
            " 54800K .......... .......... .......... .......... .......... 96%  214M 0s\n",
            " 54850K .......... .......... .......... .......... .......... 96%  235M 0s\n",
            " 54900K .......... .......... .......... .......... .......... 96%  233M 0s\n",
            " 54950K .......... .......... .......... .......... .......... 96%  189M 0s\n",
            " 55000K .......... .......... .......... .......... .......... 96%  208M 0s\n",
            " 55050K .......... .......... .......... .......... .......... 96%  210M 0s\n",
            " 55100K .......... .......... .......... .......... .......... 96%  218M 0s\n",
            " 55150K .......... .......... .......... .......... .......... 96%  173M 0s\n",
            " 55200K .......... .......... .......... .......... .......... 96%  189M 0s\n",
            " 55250K .......... .......... .......... .......... .......... 96%  181M 0s\n",
            " 55300K .......... .......... .......... .......... .......... 96%  202M 0s\n",
            " 55350K .......... .......... .......... .......... .......... 97%  171M 0s\n",
            " 55400K .......... .......... .......... .......... .......... 97%  186M 0s\n",
            " 55450K .......... .......... .......... .......... .......... 97%  213M 0s\n",
            " 55500K .......... .......... .......... .......... .......... 97%  207M 0s\n",
            " 55550K .......... .......... .......... .......... .......... 97%  175M 0s\n",
            " 55600K .......... .......... .......... .......... .......... 97%  203M 0s\n",
            " 55650K .......... .......... .......... .......... .......... 97%  177M 0s\n",
            " 55700K .......... .......... .......... .......... .......... 97%  181M 0s\n",
            " 55750K .......... .......... .......... .......... .......... 97%  168M 0s\n",
            " 55800K .......... .......... .......... .......... .......... 97%  200M 0s\n",
            " 55850K .......... .......... .......... .......... .......... 97%  182M 0s\n",
            " 55900K .......... .......... .......... .......... .......... 97%  199M 0s\n",
            " 55950K .......... .......... .......... .......... .......... 98%  162M 0s\n",
            " 56000K .......... .......... .......... .......... .......... 98%  195M 0s\n",
            " 56050K .......... .......... .......... .......... .......... 98%  193M 0s\n",
            " 56100K .......... .......... .......... .......... .......... 98% 4.39M 0s\n",
            " 56150K .......... .......... .......... .......... .......... 98%  108M 0s\n",
            " 56200K .......... .......... .......... .......... .......... 98%  125M 0s\n",
            " 56250K .......... .......... .......... .......... .......... 98% 92.0M 0s\n",
            " 56300K .......... .......... .......... .......... .......... 98%  127M 0s\n",
            " 56350K .......... .......... .......... .......... .......... 98%  106M 0s\n",
            " 56400K .......... .......... .......... .......... .......... 98% 91.6M 0s\n",
            " 56450K .......... .......... .......... .......... .......... 98%  197M 0s\n",
            " 56500K .......... .......... .......... .......... .......... 99%  235M 0s\n",
            " 56550K .......... .......... .......... .......... .......... 99%  178M 0s\n",
            " 56600K .......... .......... .......... .......... .......... 99%  216M 0s\n",
            " 56650K .......... .......... .......... .......... .......... 99%  212M 0s\n",
            " 56700K .......... .......... .......... .......... .......... 99% 16.1M 0s\n",
            " 56750K .......... .......... .......... .......... .......... 99% 65.0M 0s\n",
            " 56800K .......... .......... .......... .......... .......... 99% 79.7M 0s\n",
            " 56850K .......... .......... .......... .......... .......... 99%  183M 0s\n",
            " 56900K .......... .......... .......... .......... .......... 99%  215M 0s\n",
            " 56950K .......... .......... .......... .......... .......... 99%  187M 0s\n",
            " 57000K .......... .......... .......... .......... .......... 99%  205M 0s\n",
            " 57050K .......... .......... .......... .......... ........  100%  208M=0.5s\n",
            "\n",
            "2021-06-21 08:41:45 (118 MB/s) - ‘Miniconda3-4.5.4-Linux-x86_64.sh’ saved [58468498/58468498]\n",
            "\n",
            "Python 3.6.5 :: Anaconda, Inc.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqnstUezsm6i",
        "outputId": "38ec6d5a-6249-4c89-ee5a-4bcc80341bce"
      },
      "source": [
        "!which conda # should return /usr/local/bin/conda"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin/conda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F5XOnOist9g",
        "outputId": "9731937c-2166-454e-d85e-494268ce5657"
      },
      "source": [
        "!conda --version # should return 4.5.4"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conda 4.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFVhM1GZswWD",
        "outputId": "6e6117ed-8576-45dc-9875-ed98f2ef90e4"
      },
      "source": [
        "!which python # still returns /usr/local/bin/python"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "367nafTisye5",
        "outputId": "b406fde5-5164-4b23-d5ba-3f6e554edac2"
      },
      "source": [
        "!python --version # now returns Python 3.6.5 :: Anaconda, Inc."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.5 :: Anaconda, Inc.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-BFOZszs1nY",
        "outputId": "aef10c4b-7ec9-4b9e-d6db-353f05ed970d"
      },
      "source": [
        "%%bash\n",
        "conda install --channel defaults conda python=3.6 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - conda\n",
            "    - python=3.6\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    cryptography-3.4.7         |   py36hd23ed53_0         1.0 MB\n",
            "    ca-certificates-2021.5.25  |       h06a4308_1         118 KB\n",
            "    zlib-1.2.11                |       h7b6447c_3         120 KB\n",
            "    cffi-1.14.5                |   py36h261ae71_0         224 KB\n",
            "    chardet-4.0.0              |py36h06a4308_1003         213 KB\n",
            "    openssl-1.1.1k             |       h27cfd23_0         3.8 MB\n",
            "    urllib3-1.26.4             |     pyhd3eb1b0_0          99 KB\n",
            "    libstdcxx-ng-9.3.0         |      hd4cf53a_17         4.0 MB\n",
            "    python-3.6.13              |       h12debd9_1        32.5 MB\n",
            "    pip-21.1.2                 |   py36h06a4308_0         2.1 MB\n",
            "    ld_impl_linux-64-2.35.1    |       h7274673_9         637 KB\n",
            "    pycparser-2.20             |             py_2          94 KB\n",
            "    _openmp_mutex-4.5          |            1_gnu          22 KB\n",
            "    brotlipy-0.7.0             |py36h27cfd23_1003         349 KB\n",
            "    idna-2.10                  |     pyhd3eb1b0_0          52 KB\n",
            "    requests-2.25.1            |     pyhd3eb1b0_0          51 KB\n",
            "    tk-8.6.10                  |       hbc83047_0         3.2 MB\n",
            "    xz-5.2.5                   |       h7b6447c_0         438 KB\n",
            "    six-1.16.0                 |     pyhd3eb1b0_0          18 KB\n",
            "    certifi-2021.5.30          |   py36h06a4308_0         141 KB\n",
            "    libgcc-ng-9.3.0            |      h5101ec6_17         7.8 MB\n",
            "    conda-package-handling-1.7.3|   py36h27cfd23_1         946 KB\n",
            "    readline-8.1               |       h27cfd23_0         464 KB\n",
            "    libgomp-9.3.0              |      h5101ec6_17         378 KB\n",
            "    tqdm-4.59.0                |     pyhd3eb1b0_1          90 KB\n",
            "    pyopenssl-20.0.1           |     pyhd3eb1b0_1          48 KB\n",
            "    conda-4.10.1               |   py36h06a4308_1         3.1 MB\n",
            "    _libgcc_mutex-0.1          |             main           3 KB\n",
            "    ncurses-6.2                |       he6710b0_1         1.1 MB\n",
            "    pycosat-0.6.3              |   py36h27cfd23_0         107 KB\n",
            "    libffi-3.3                 |       he6710b0_2          54 KB\n",
            "    wheel-0.36.2               |     pyhd3eb1b0_0          31 KB\n",
            "    yaml-0.2.5                 |       h7b6447c_0          87 KB\n",
            "    ruamel_yaml-0.15.100       |   py36h27cfd23_0         268 KB\n",
            "    sqlite-3.35.4              |       hdfb4753_0         1.4 MB\n",
            "    setuptools-52.0.0          |   py36h06a4308_0         933 KB\n",
            "    pysocks-1.7.1              |   py36h06a4308_0          30 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        65.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    _libgcc_mutex:          0.1-main               \n",
            "    _openmp_mutex:          4.5-1_gnu              \n",
            "    brotlipy:               0.7.0-py36h27cfd23_1003\n",
            "    conda-package-handling: 1.7.3-py36h27cfd23_1   \n",
            "    ld_impl_linux-64:       2.35.1-h7274673_9      \n",
            "    libgomp:                9.3.0-h5101ec6_17      \n",
            "    tqdm:                   4.59.0-pyhd3eb1b0_1    \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    ca-certificates:        2018.03.07-0            --> 2021.5.25-h06a4308_1    \n",
            "    certifi:                2018.4.16-py36_0        --> 2021.5.30-py36h06a4308_0\n",
            "    cffi:                   1.11.5-py36h9745a5d_0   --> 1.14.5-py36h261ae71_0   \n",
            "    chardet:                3.0.4-py36h0f667ec_1    --> 4.0.0-py36h06a4308_1003 \n",
            "    conda:                  4.5.4-py36_0            --> 4.10.1-py36h06a4308_1   \n",
            "    cryptography:           2.2.2-py36h14c3975_0    --> 3.4.7-py36hd23ed53_0    \n",
            "    idna:                   2.6-py36h82fb2a8_1      --> 2.10-pyhd3eb1b0_0       \n",
            "    libffi:                 3.2.1-hd88cf55_4        --> 3.3-he6710b0_2          \n",
            "    libgcc-ng:              7.2.0-hdf63c60_3        --> 9.3.0-h5101ec6_17       \n",
            "    libstdcxx-ng:           7.2.0-hdf63c60_3        --> 9.3.0-hd4cf53a_17       \n",
            "    ncurses:                6.1-hf484d3e_0          --> 6.2-he6710b0_1          \n",
            "    openssl:                1.0.2o-h20670df_0       --> 1.1.1k-h27cfd23_0       \n",
            "    pip:                    10.0.1-py36_0           --> 21.1.2-py36h06a4308_0   \n",
            "    pycosat:                0.6.3-py36h0a5515d_0    --> 0.6.3-py36h27cfd23_0    \n",
            "    pycparser:              2.18-py36hf9f622e_1     --> 2.20-py_2               \n",
            "    pyopenssl:              18.0.0-py36_0           --> 20.0.1-pyhd3eb1b0_1     \n",
            "    pysocks:                1.6.8-py36_0            --> 1.7.1-py36h06a4308_0    \n",
            "    python:                 3.6.5-hc3d631a_2        --> 3.6.13-h12debd9_1       \n",
            "    readline:               7.0-ha6073c6_4          --> 8.1-h27cfd23_0          \n",
            "    requests:               2.18.4-py36he2e5f8d_1   --> 2.25.1-pyhd3eb1b0_0     \n",
            "    ruamel_yaml:            0.15.37-py36h14c3975_2  --> 0.15.100-py36h27cfd23_0 \n",
            "    setuptools:             39.2.0-py36_0           --> 52.0.0-py36h06a4308_0   \n",
            "    six:                    1.11.0-py36h372c433_1   --> 1.16.0-pyhd3eb1b0_0     \n",
            "    sqlite:                 3.23.1-he433501_0       --> 3.35.4-hdfb4753_0       \n",
            "    tk:                     8.6.7-hc745277_3        --> 8.6.10-hbc83047_0       \n",
            "    urllib3:                1.22-py36hbe7ace6_0     --> 1.26.4-pyhd3eb1b0_0     \n",
            "    wheel:                  0.31.1-py36_0           --> 0.36.2-pyhd3eb1b0_0     \n",
            "    xz:                     5.2.4-h14c3975_4        --> 5.2.5-h7b6447c_0        \n",
            "    yaml:                   0.1.7-had09818_2        --> 0.2.5-h7b6447c_0        \n",
            "    zlib:                   1.2.11-ha838bed_2       --> 1.2.11-h7b6447c_3       \n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  asn1crypto-0.24.0-py36_0\n",
            "  conda-env-2.6.0-h36134e3_1\n",
            "  libedit-3.1.20170329-h6b74fdf_2\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rcryptography-3.4.7   |  1.0 MB |            |   0% \rcryptography-3.4.7   |  1.0 MB | #######9   |  80% \rcryptography-3.4.7   |  1.0 MB | ########## | 100% \n",
            "\rca-certificates-2021 |  118 KB |            |   0% \rca-certificates-2021 |  118 KB | ########## | 100% \n",
            "\rzlib-1.2.11          |  120 KB |            |   0% \rzlib-1.2.11          |  120 KB | ########## | 100% \n",
            "\rcffi-1.14.5          |  224 KB |            |   0% \rcffi-1.14.5          |  224 KB | ########## | 100% \n",
            "\rchardet-4.0.0        |  213 KB |            |   0% \rchardet-4.0.0        |  213 KB | ########## | 100% \n",
            "\ropenssl-1.1.1k       |  3.8 MB |            |   0% \ropenssl-1.1.1k       |  3.8 MB | #######7   |  78% \ropenssl-1.1.1k       |  3.8 MB | ########## | 100% \n",
            "\rurllib3-1.26.4       |   99 KB |            |   0% \rurllib3-1.26.4       |   99 KB | ########## | 100% \n",
            "\rlibstdcxx-ng-9.3.0   |  4.0 MB |            |   0% \rlibstdcxx-ng-9.3.0   |  4.0 MB | #######7   |  78% \rlibstdcxx-ng-9.3.0   |  4.0 MB | ########## | 100% \n",
            "\rpython-3.6.13        | 32.5 MB |            |   0% \rpython-3.6.13        | 32.5 MB | ##6        |  26% \rpython-3.6.13        | 32.5 MB | #####9     |  60% \rpython-3.6.13        | 32.5 MB | #######5   |  75% \rpython-3.6.13        | 32.5 MB | #########1 |  91% \rpython-3.6.13        | 32.5 MB | ########## | 100% \n",
            "\rpip-21.1.2           |  2.1 MB |            |   0% \rpip-21.1.2           |  2.1 MB | #######7   |  78% \rpip-21.1.2           |  2.1 MB | #########4 |  95% \rpip-21.1.2           |  2.1 MB | ########## | 100% \n",
            "\rld_impl_linux-64-2.3 |  637 KB |            |   0% \rld_impl_linux-64-2.3 |  637 KB | #########5 |  95% \rld_impl_linux-64-2.3 |  637 KB | ########## | 100% \n",
            "\rpycparser-2.20       |   94 KB |            |   0% \rpycparser-2.20       |   94 KB | ########## | 100% \n",
            "\r_openmp_mutex-4.5    |   22 KB |            |   0% \r_openmp_mutex-4.5    |   22 KB | ########## | 100% \n",
            "\rbrotlipy-0.7.0       |  349 KB |            |   0% \rbrotlipy-0.7.0       |  349 KB | ########## | 100% \n",
            "\ridna-2.10            |   52 KB |            |   0% \ridna-2.10            |   52 KB | ########## | 100% \n",
            "\rrequests-2.25.1      |   51 KB |            |   0% \rrequests-2.25.1      |   51 KB | ########## | 100% \n",
            "\rtk-8.6.10            |  3.2 MB |            |   0% \rtk-8.6.10            |  3.2 MB | #######9   |  79% \rtk-8.6.10            |  3.2 MB | ########## | 100% \n",
            "\rxz-5.2.5             |  438 KB |            |   0% \rxz-5.2.5             |  438 KB | ########## | 100% \n",
            "\rsix-1.16.0           |   18 KB |            |   0% \rsix-1.16.0           |   18 KB | ########## | 100% \n",
            "\rcertifi-2021.5.30    |  141 KB |            |   0% \rcertifi-2021.5.30    |  141 KB | ########## | 100% \n",
            "\rlibgcc-ng-9.3.0      |  7.8 MB |            |   0% \rlibgcc-ng-9.3.0      |  7.8 MB | #######5   |  76% \rlibgcc-ng-9.3.0      |  7.8 MB | #########6 |  97% \rlibgcc-ng-9.3.0      |  7.8 MB | ########## | 100% \n",
            "\rconda-package-handli |  946 KB |            |   0% \rconda-package-handli |  946 KB | #########2 |  93% \rconda-package-handli |  946 KB | ########## | 100% \n",
            "\rreadline-8.1         |  464 KB |            |   0% \rreadline-8.1         |  464 KB | ########## | 100% \n",
            "\rlibgomp-9.3.0        |  378 KB |            |   0% \rlibgomp-9.3.0        |  378 KB | ########## | 100% \n",
            "\rtqdm-4.59.0          |   90 KB |            |   0% \rtqdm-4.59.0          |   90 KB | ########## | 100% \n",
            "\rpyopenssl-20.0.1     |   48 KB |            |   0% \rpyopenssl-20.0.1     |   48 KB | ########## | 100% \n",
            "\rconda-4.10.1         |  3.1 MB |            |   0% \rconda-4.10.1         |  3.1 MB | #######6   |  77% \rconda-4.10.1         |  3.1 MB | #########4 |  95% \rconda-4.10.1         |  3.1 MB | ########## | 100% \n",
            "\r_libgcc_mutex-0.1    |    3 KB |            |   0% \r_libgcc_mutex-0.1    |    3 KB | ########## | 100% \n",
            "\rncurses-6.2          |  1.1 MB |            |   0% \rncurses-6.2          |  1.1 MB | ########1  |  82% \rncurses-6.2          |  1.1 MB | ########## | 100% \n",
            "\rpycosat-0.6.3        |  107 KB |            |   0% \rpycosat-0.6.3        |  107 KB | ########## | 100% \n",
            "\rlibffi-3.3           |   54 KB |            |   0% \rlibffi-3.3           |   54 KB | ########## | 100% \n",
            "\rwheel-0.36.2         |   31 KB |            |   0% \rwheel-0.36.2         |   31 KB | ########## | 100% \n",
            "\ryaml-0.2.5           |   87 KB |            |   0% \ryaml-0.2.5           |   87 KB | ########## | 100% \n",
            "\rruamel_yaml-0.15.100 |  268 KB |            |   0% \rruamel_yaml-0.15.100 |  268 KB | ########## | 100% \n",
            "\rsqlite-3.35.4        |  1.4 MB |            |   0% \rsqlite-3.35.4        |  1.4 MB | ########4  |  84% \rsqlite-3.35.4        |  1.4 MB | ########## | 100% \n",
            "\rsetuptools-52.0.0    |  933 KB |            |   0% \rsetuptools-52.0.0    |  933 KB | ########2  |  82% \rsetuptools-52.0.0    |  933 KB | ########## | 100% \n",
            "\rpysocks-1.7.1        |   30 KB |            |   0% \rpysocks-1.7.1        |   30 KB | ########## | 100% \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_8u9kFYs5E_",
        "outputId": "008747a2-13ac-4720-8b2b-e9fe21696116"
      },
      "source": [
        "!conda --version # now returns 4.8.3"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conda 4.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39TB_M6vtKD_",
        "outputId": "7a579e05-6103-4909-d2e3-b43957e5724f"
      },
      "source": [
        "!python --version # now returns Python 3.6.10 :: Anaconda, Inc."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.13 :: Anaconda, Inc.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldnv2cBptMzB",
        "outputId": "11dc88c5-c2b5-4f16-d972-1fd7d5884625"
      },
      "source": [
        "import sys\n",
        "sys.path"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python37.zip',\n",
              " '/usr/lib/python3.7',\n",
              " '/usr/lib/python3.7/lib-dynload',\n",
              " '/usr/local/lib/python3.7/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n",
              " '/root/.ipython']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eClEI69QtQz_",
        "outputId": "d01dd7a5-7cc7-4b68-f245-12ead2eb9dd0"
      },
      "source": [
        "['',  \n",
        " '/env/python',\n",
        " '/usr/lib/python36.zip',\n",
        " '/usr/lib/python3.6',\n",
        " '/usr/lib/python3.6/lib-dynload',\n",
        " '/usr/local/lib/python3.6/dist-packages', # pre-installed packages\n",
        " '/usr/lib/python3/dist-packages',\n",
        " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
        " '/root/.ipython']"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/env/python',\n",
              " '/usr/lib/python36.zip',\n",
              " '/usr/lib/python3.6',\n",
              " '/usr/lib/python3.6/lib-dynload',\n",
              " '/usr/local/lib/python3.6/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
              " '/root/.ipython']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJX_5o9btToZ"
      },
      "source": [
        "!ls /usr/local/lib/python3.6/dist-packages"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4aUxWs3tVoz"
      },
      "source": [
        "import sys\n",
        "_ = (sys.path\n",
        "        .append(\"/usr/local/lib/python3.6/site-packages\"))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rwu0D_cntXuK",
        "outputId": "81af35a4-7b63-4392-9c3f-26fcd322db01"
      },
      "source": [
        "!conda install --channel conda-forge featuretools --yes"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - featuretools\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    bokeh-2.3.2                |   py36h5fab9bb_0         8.3 MB  conda-forge\n",
            "    ca-certificates-2021.5.30  |       ha878542_0         136 KB  conda-forge\n",
            "    certifi-2021.5.30          |   py36h5fab9bb_0         141 KB  conda-forge\n",
            "    click-7.1.2                |     pyh9f0ad1d_0          64 KB  conda-forge\n",
            "    cloudpickle-1.6.0          |             py_0          22 KB  conda-forge\n",
            "    conda-4.10.1               |   py36h5fab9bb_0         3.1 MB  conda-forge\n",
            "    contextvars-2.4            |             py_0          11 KB  conda-forge\n",
            "    cytoolz-0.11.0             |   py36h8f6f2f9_3         393 KB  conda-forge\n",
            "    dask-2021.3.0              |     pyhd8ed1ab_0           4 KB  conda-forge\n",
            "    dask-core-2021.3.0         |     pyhd8ed1ab_0         702 KB  conda-forge\n",
            "    distributed-2021.3.0       |   py36h5fab9bb_0         1.1 MB  conda-forge\n",
            "    featuretools-0.23.3        |     pyhd8ed1ab_0         267 KB  conda-forge\n",
            "    freetype-2.10.4            |       h0708190_1         890 KB  conda-forge\n",
            "    fsspec-2021.6.0            |     pyhd8ed1ab_0          79 KB  conda-forge\n",
            "    heapdict-1.0.1             |             py_0           7 KB  conda-forge\n",
            "    immutables-0.15            |   py36h8f6f2f9_0          70 KB  conda-forge\n",
            "    jbig-2.1                   |    h7f98852_2003          43 KB  conda-forge\n",
            "    jinja2-3.0.1               |     pyhd8ed1ab_0          99 KB  conda-forge\n",
            "    jpeg-9d                    |       h36c2ea0_0         264 KB  conda-forge\n",
            "    lcms2-2.12                 |       hddcbb42_0         443 KB  conda-forge\n",
            "    lerc-2.2.1                 |       h9c3ff4c_0         213 KB  conda-forge\n",
            "    libblas-3.9.0              |       9_openblas          11 KB  conda-forge\n",
            "    libcblas-3.9.0             |       9_openblas          11 KB  conda-forge\n",
            "    libdeflate-1.7             |       h7f98852_5          67 KB  conda-forge\n",
            "    libgfortran-ng-9.3.0       |      hff62375_19          22 KB  conda-forge\n",
            "    libgfortran5-9.3.0         |      hff62375_19         2.0 MB  conda-forge\n",
            "    liblapack-3.9.0            |       9_openblas          11 KB  conda-forge\n",
            "    libopenblas-0.3.15         |pthreads_h8fe5266_1         9.2 MB  conda-forge\n",
            "    libpng-1.6.37              |       h21135ba_2         306 KB  conda-forge\n",
            "    libtiff-4.3.0              |       hf544144_1         668 KB  conda-forge\n",
            "    libwebp-base-1.2.0         |       h7f98852_2         815 KB  conda-forge\n",
            "    locket-0.2.0               |             py_2           6 KB  conda-forge\n",
            "    lz4-c-1.9.3                |       h9c3ff4c_0         179 KB  conda-forge\n",
            "    markupsafe-2.0.1           |   py36h8f6f2f9_0          22 KB  conda-forge\n",
            "    msgpack-python-1.0.2       |   py36h605e78d_1          91 KB  conda-forge\n",
            "    numpy-1.19.5               |   py36h2aa4a07_1         5.3 MB  conda-forge\n",
            "    olefile-0.46               |     pyh9f0ad1d_1          32 KB  conda-forge\n",
            "    openjpeg-2.4.0             |       hb52868f_1         444 KB  conda-forge\n",
            "    openssl-1.1.1k             |       h7f98852_0         2.1 MB  conda-forge\n",
            "    packaging-20.9             |     pyh44b312d_0          35 KB  conda-forge\n",
            "    pandas-1.1.5               |   py36h284efc9_0        11.3 MB  conda-forge\n",
            "    partd-1.2.0                |     pyhd8ed1ab_0          18 KB  conda-forge\n",
            "    pillow-8.2.0               |   py36ha6010c0_1         688 KB  conda-forge\n",
            "    psutil-5.8.0               |   py36h8f6f2f9_1         342 KB  conda-forge\n",
            "    pyparsing-2.4.7            |     pyh9f0ad1d_0          60 KB  conda-forge\n",
            "    python-dateutil-2.8.1      |             py_0         220 KB  conda-forge\n",
            "    python_abi-3.6             |          1_cp36m           4 KB  conda-forge\n",
            "    pytz-2021.1                |     pyhd8ed1ab_0         239 KB  conda-forge\n",
            "    pyyaml-5.4.1               |   py36h8f6f2f9_0         190 KB  conda-forge\n",
            "    scipy-1.5.3                |   py36h9e8f40b_0        19.1 MB  conda-forge\n",
            "    sortedcontainers-2.4.0     |     pyhd8ed1ab_0          26 KB  conda-forge\n",
            "    tblib-1.7.0                |     pyhd8ed1ab_0          15 KB  conda-forge\n",
            "    toolz-0.11.1               |             py_0          46 KB  conda-forge\n",
            "    tornado-6.1                |   py36h8f6f2f9_1         643 KB  conda-forge\n",
            "    typing_extensions-3.10.0.0 |     pyha770c72_0          28 KB  conda-forge\n",
            "    zict-2.0.0                 |             py_0          10 KB  conda-forge\n",
            "    zstd-1.5.0                 |       ha95c52a_0         490 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        70.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  bokeh              conda-forge/linux-64::bokeh-2.3.2-py36h5fab9bb_0\n",
            "  click              conda-forge/noarch::click-7.1.2-pyh9f0ad1d_0\n",
            "  cloudpickle        conda-forge/noarch::cloudpickle-1.6.0-py_0\n",
            "  contextvars        conda-forge/noarch::contextvars-2.4-py_0\n",
            "  cytoolz            conda-forge/linux-64::cytoolz-0.11.0-py36h8f6f2f9_3\n",
            "  dask               conda-forge/noarch::dask-2021.3.0-pyhd8ed1ab_0\n",
            "  dask-core          conda-forge/noarch::dask-core-2021.3.0-pyhd8ed1ab_0\n",
            "  distributed        conda-forge/linux-64::distributed-2021.3.0-py36h5fab9bb_0\n",
            "  featuretools       conda-forge/noarch::featuretools-0.23.3-pyhd8ed1ab_0\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.4-h0708190_1\n",
            "  fsspec             conda-forge/noarch::fsspec-2021.6.0-pyhd8ed1ab_0\n",
            "  heapdict           conda-forge/noarch::heapdict-1.0.1-py_0\n",
            "  immutables         conda-forge/linux-64::immutables-0.15-py36h8f6f2f9_0\n",
            "  jbig               conda-forge/linux-64::jbig-2.1-h7f98852_2003\n",
            "  jinja2             conda-forge/noarch::jinja2-3.0.1-pyhd8ed1ab_0\n",
            "  jpeg               conda-forge/linux-64::jpeg-9d-h36c2ea0_0\n",
            "  lcms2              conda-forge/linux-64::lcms2-2.12-hddcbb42_0\n",
            "  lerc               conda-forge/linux-64::lerc-2.2.1-h9c3ff4c_0\n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-9_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-9_openblas\n",
            "  libdeflate         conda-forge/linux-64::libdeflate-1.7-h7f98852_5\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-9.3.0-hff62375_19\n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-9.3.0-hff62375_19\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-9_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.15-pthreads_h8fe5266_1\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-h21135ba_2\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.3.0-hf544144_1\n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.2.0-h7f98852_2\n",
            "  locket             conda-forge/noarch::locket-0.2.0-py_2\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.9.3-h9c3ff4c_0\n",
            "  markupsafe         conda-forge/linux-64::markupsafe-2.0.1-py36h8f6f2f9_0\n",
            "  msgpack-python     conda-forge/linux-64::msgpack-python-1.0.2-py36h605e78d_1\n",
            "  numpy              conda-forge/linux-64::numpy-1.19.5-py36h2aa4a07_1\n",
            "  olefile            conda-forge/noarch::olefile-0.46-pyh9f0ad1d_1\n",
            "  openjpeg           conda-forge/linux-64::openjpeg-2.4.0-hb52868f_1\n",
            "  packaging          conda-forge/noarch::packaging-20.9-pyh44b312d_0\n",
            "  pandas             conda-forge/linux-64::pandas-1.1.5-py36h284efc9_0\n",
            "  partd              conda-forge/noarch::partd-1.2.0-pyhd8ed1ab_0\n",
            "  pillow             conda-forge/linux-64::pillow-8.2.0-py36ha6010c0_1\n",
            "  psutil             conda-forge/linux-64::psutil-5.8.0-py36h8f6f2f9_1\n",
            "  pyparsing          conda-forge/noarch::pyparsing-2.4.7-pyh9f0ad1d_0\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.1-py_0\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.6-1_cp36m\n",
            "  pytz               conda-forge/noarch::pytz-2021.1-pyhd8ed1ab_0\n",
            "  pyyaml             conda-forge/linux-64::pyyaml-5.4.1-py36h8f6f2f9_0\n",
            "  scipy              conda-forge/linux-64::scipy-1.5.3-py36h9e8f40b_0\n",
            "  sortedcontainers   conda-forge/noarch::sortedcontainers-2.4.0-pyhd8ed1ab_0\n",
            "  tblib              conda-forge/noarch::tblib-1.7.0-pyhd8ed1ab_0\n",
            "  toolz              conda-forge/noarch::toolz-0.11.1-py_0\n",
            "  tornado            conda-forge/linux-64::tornado-6.1-py36h8f6f2f9_1\n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-3.10.0.0-pyha770c72_0\n",
            "  zict               conda-forge/noarch::zict-2.0.0-py_0\n",
            "  zstd               conda-forge/linux-64::zstd-1.5.0-ha95c52a_0\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2021.5.25-~ --> conda-forge::ca-certificates-2021.5.30-ha878542_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            pkgs/main::certifi-2021.5.30-py36h06a~ --> conda-forge::certifi-2021.5.30-py36h5fab9bb_0\n",
            "  conda              pkgs/main::conda-4.10.1-py36h06a4308_1 --> conda-forge::conda-4.10.1-py36h5fab9bb_0\n",
            "  openssl              pkgs/main::openssl-1.1.1k-h27cfd23_0 --> conda-forge::openssl-1.1.1k-h7f98852_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "libpng-1.6.37        | 306 KB    | : 100% 1.0/1 [00:00<00:00,  5.55it/s]                 \n",
            "libopenblas-0.3.15   | 9.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.43s/it]\n",
            "toolz-0.11.1         | 46 KB     | : 100% 1.0/1 [00:00<00:00, 23.96it/s]\n",
            "tblib-1.7.0          | 15 KB     | : 100% 1.0/1 [00:00<00:00, 30.20it/s]\n",
            "tornado-6.1          | 643 KB    | : 100% 1.0/1 [00:00<00:00,  6.05it/s]\n",
            "certifi-2021.5.30    | 141 KB    | : 100% 1.0/1 [00:00<00:00, 23.15it/s]\n",
            "cytoolz-0.11.0       | 393 KB    | : 100% 1.0/1 [00:00<00:00,  9.12it/s]\n",
            "dask-core-2021.3.0   | 702 KB    | : 100% 1.0/1 [00:00<00:00,  5.36it/s]\n",
            "featuretools-0.23.3  | 267 KB    | : 100% 1.0/1 [00:00<00:00,  7.38it/s]\n",
            "scipy-1.5.3          | 19.1 MB   | : 100% 1.0/1 [00:02<00:00,  2.90s/it]\n",
            "pandas-1.1.5         | 11.3 MB   | : 100% 1.0/1 [00:02<00:00,  2.06s/it]\n",
            "bokeh-2.3.2          | 8.3 MB    | : 100% 1.0/1 [00:01<00:00,  1.87s/it]\n",
            "conda-4.10.1         | 3.1 MB    | : 100% 1.0/1 [00:00<00:00,  1.78it/s]\n",
            "jpeg-9d              | 264 KB    | : 100% 1.0/1 [00:00<00:00, 12.89it/s]\n",
            "ca-certificates-2021 | 136 KB    | : 100% 1.0/1 [00:00<00:00, 17.36it/s]\n",
            "lz4-c-1.9.3          | 179 KB    | : 100% 1.0/1 [00:00<00:00, 15.51it/s]\n",
            "lerc-2.2.1           | 213 KB    | : 100% 1.0/1 [00:00<00:00, 13.25it/s]\n",
            "immutables-0.15      | 70 KB     | : 100% 1.0/1 [00:00<00:00, 17.17it/s]\n",
            "jbig-2.1             | 43 KB     | : 100% 1.0/1 [00:00<00:00, 23.70it/s]\n",
            "fsspec-2021.6.0      | 79 KB     | : 100% 1.0/1 [00:00<00:00, 21.80it/s]\n",
            "psutil-5.8.0         | 342 KB    | : 100% 1.0/1 [00:00<00:00, 10.55it/s]\n",
            "locket-0.2.0         | 6 KB      | : 100% 1.0/1 [00:00<00:00, 34.76it/s]\n",
            "typing_extensions-3. | 28 KB     | : 100% 1.0/1 [00:00<00:00, 33.14it/s]\n",
            "libgfortran-ng-9.3.0 | 22 KB     | : 100% 1.0/1 [00:00<00:00, 31.27it/s]\n",
            "markupsafe-2.0.1     | 22 KB     | : 100% 1.0/1 [00:00<00:00, 29.51it/s]\n",
            "zstd-1.5.0           | 490 KB    | : 100% 1.0/1 [00:00<00:00, 10.00it/s]\n",
            "pyyaml-5.4.1         | 190 KB    | : 100% 1.0/1 [00:00<00:00, 17.49it/s]\n",
            "libdeflate-1.7       | 67 KB     | : 100% 1.0/1 [00:00<00:00, 13.83it/s]\n",
            "python_abi-3.6       | 4 KB      | : 100% 1.0/1 [00:00<00:00, 32.42it/s]\n",
            "partd-1.2.0          | 18 KB     | : 100% 1.0/1 [00:00<00:00, 30.88it/s]\n",
            "libblas-3.9.0        | 11 KB     | : 100% 1.0/1 [00:00<00:00, 27.12it/s]\n",
            "openjpeg-2.4.0       | 444 KB    | : 100% 1.0/1 [00:00<00:00, 10.45it/s]\n",
            "libgfortran5-9.3.0   | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  2.89it/s]\n",
            "freetype-2.10.4      | 890 KB    | : 100% 1.0/1 [00:00<00:00,  6.55it/s]\n",
            "msgpack-python-1.0.2 | 91 KB     | : 100% 1.0/1 [00:00<00:00, 25.62it/s]\n",
            "pytz-2021.1          | 239 KB    | : 100% 1.0/1 [00:00<00:00,  9.06it/s]\n",
            "click-7.1.2          | 64 KB     | : 100% 1.0/1 [00:00<00:00, 23.25it/s]\n",
            "numpy-1.19.5         | 5.3 MB    | : 100% 1.0/1 [00:00<00:00,  1.06it/s]\n",
            "zict-2.0.0           | 10 KB     | : 100% 1.0/1 [00:00<00:00, 34.69it/s]\n",
            "jinja2-3.0.1         | 99 KB     | : 100% 1.0/1 [00:00<00:00, 21.49it/s]\n",
            "liblapack-3.9.0      | 11 KB     | : 100% 1.0/1 [00:00<00:00, 36.52it/s]\n",
            "cloudpickle-1.6.0    | 22 KB     | : 100% 1.0/1 [00:00<00:00, 30.51it/s]\n",
            "olefile-0.46         | 32 KB     | : 100% 1.0/1 [00:00<00:00, 27.30it/s]\n",
            "packaging-20.9       | 35 KB     | : 100% 1.0/1 [00:00<00:00, 28.85it/s]\n",
            "openssl-1.1.1k       | 2.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.18it/s]\n",
            "dask-2021.3.0        | 4 KB      | : 100% 1.0/1 [00:00<00:00, 40.63it/s]\n",
            "sortedcontainers-2.4 | 26 KB     | : 100% 1.0/1 [00:00<00:00, 29.65it/s]\n",
            "libcblas-3.9.0       | 11 KB     | : 100% 1.0/1 [00:00<00:00, 33.68it/s]\n",
            "contextvars-2.4      | 11 KB     | : 100% 1.0/1 [00:00<00:00, 33.78it/s]\n",
            "libtiff-4.3.0        | 668 KB    | : 100% 1.0/1 [00:00<00:00,  8.09it/s]\n",
            "distributed-2021.3.0 | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.66it/s]\n",
            "libwebp-base-1.2.0   | 815 KB    | : 100% 1.0/1 [00:00<00:00,  6.80it/s]\n",
            "pillow-8.2.0         | 688 KB    | : 100% 1.0/1 [00:00<00:00,  6.61it/s]\n",
            "heapdict-1.0.1       | 7 KB      | : 100% 1.0/1 [00:00<00:00, 30.03it/s]\n",
            "pyparsing-2.4.7      | 60 KB     | : 100% 1.0/1 [00:00<00:00, 26.27it/s]\n",
            "python-dateutil-2.8. | 220 KB    | : 100% 1.0/1 [00:00<00:00, 18.30it/s]\n",
            "lcms2-2.12           | 443 KB    | : 100% 1.0/1 [00:00<00:00, 11.13it/s]\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpaqlERWtZwA",
        "outputId": "8935279b-8d8d-44bf-9f08-18a2472d6e9b"
      },
      "source": [
        "!conda install pytorch=0.4.1 torchvision=0.2.1 -c pytorch\n",
        "!pip install tensorboardX==1.4\n",
        "!conda install opencv=3.3.1   # just needed for evaluation"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pytorch=0.4.1\n",
            "    - torchvision=0.2.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    certifi-2021.5.30          |   py36h06a4308_0         139 KB\n",
            "    conda-4.10.1               |   py36h06a4308_1         2.9 MB\n",
            "    intel-openmp-2021.2.0      |     h06a4308_610         1.3 MB\n",
            "    mkl-2021.2.0               |     h06a4308_296       144.3 MB\n",
            "    ninja-1.10.2               |       hff7bd54_1         1.4 MB\n",
            "    pytorch-0.4.1              |py36_py35_py27__9.0.176_7.1.2_2       471.7 MB  pytorch\n",
            "    torchvision-0.2.1          |             py_2          37 KB  pytorch\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       621.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  intel-openmp       pkgs/main/linux-64::intel-openmp-2021.2.0-h06a4308_610\n",
            "  mkl                pkgs/main/linux-64::mkl-2021.2.0-h06a4308_296\n",
            "  ninja              pkgs/main/linux-64::ninja-1.10.2-hff7bd54_1\n",
            "  pytorch            pytorch/linux-64::pytorch-0.4.1-py36_py35_py27__9.0.176_7.1.2_2\n",
            "  torchvision        pytorch/noarch::torchvision-0.2.1-py_2\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  conda              conda-forge::conda-4.10.1-py36h5fab9b~ --> pkgs/main::conda-4.10.1-py36h06a4308_1\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge::certifi-2021.5.30-py36h5~ --> pkgs/main::certifi-2021.5.30-py36h06a4308_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "certifi-2021.5.30    | 139 KB    | : 100% 1.0/1 [00:00<00:00,  9.73it/s]\n",
            "ninja-1.10.2         | 1.4 MB    | : 100% 1.0/1 [00:00<00:00, 10.54it/s]\n",
            "conda-4.10.1         | 2.9 MB    | : 100% 1.0/1 [00:00<00:00,  5.95it/s]\n",
            "torchvision-0.2.1    | 37 KB     | : 100% 1.0/1 [00:00<00:00,  1.20it/s]               \n",
            "intel-openmp-2021.2. | 1.3 MB    | : 100% 1.0/1 [00:00<00:00, 15.19it/s]\n",
            "mkl-2021.2.0         | 144.3 MB  | : 100% 1.0/1 [00:04<00:00,  4.99s/it]               \n",
            "pytorch-0.4.1        | 471.7 MB  | : 100% 1.0/1 [01:04<00:00, 64.48s/it]  \n",
            "Preparing transaction: | \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Collecting tensorboardX==1.4\n",
            "  Downloading tensorboardX-1.4-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from tensorboardX==1.4) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from tensorboardX==1.4) (1.16.0)\n",
            "Collecting protobuf>=3.2.0\n",
            "  Downloading protobuf-3.17.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 14.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: protobuf, tensorboardX\n",
            "Successfully installed protobuf-3.17.3 tensorboardX-1.4\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - opencv=3.3.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    blas-1.0                   |         openblas          46 KB\n",
            "    bzip2-1.0.8                |       h7b6447c_0          78 KB\n",
            "    cairo-1.16.0               |       hf32fb01_1         1.0 MB\n",
            "    ffmpeg-3.4                 |       h7985aa0_0         7.1 MB\n",
            "    fontconfig-2.13.1          |       h6c09931_0         250 KB\n",
            "    glib-2.68.2                |       h36276a3_0         3.0 MB\n",
            "    graphite2-1.3.14           |       h23475e2_0          99 KB\n",
            "    harfbuzz-1.8.8             |       hffaf4a1_0         507 KB\n",
            "    hdf5-1.10.1                |       h9caa474_1         3.8 MB\n",
            "    icu-58.2                   |       he6710b0_3        10.5 MB\n",
            "    jasper-1.900.1             |       hd497a04_4         198 KB\n",
            "    libgfortran-ng-7.5.0       |      ha8ba4b0_17          22 KB\n",
            "    libgfortran4-7.5.0         |      ha8ba4b0_17         995 KB\n",
            "    libopenblas-0.3.13         |       h4367d64_0         4.8 MB\n",
            "    libopus-1.3.1              |       h7b6447c_0         491 KB\n",
            "    libprotobuf-3.4.1          |       h5b8497f_0         2.4 MB\n",
            "    libuuid-1.0.3              |       h1bed415_2          15 KB\n",
            "    libvpx-1.7.0               |       h439df22_0         1.2 MB\n",
            "    libxcb-1.14                |       h7b6447c_0         505 KB\n",
            "    libxml2-2.9.10             |       hb55368b_3         1.2 MB\n",
            "    numpy-1.19.2               |   py36h6163131_0          22 KB\n",
            "    numpy-base-1.19.2          |   py36h75fe3a5_0         4.1 MB\n",
            "    opencv-3.3.1               |   py36h6cbbc71_1        21.2 MB\n",
            "    pcre-8.44                  |       he6710b0_0         212 KB\n",
            "    pixman-0.40.0              |       h7b6447c_0         370 KB\n",
            "    scipy-1.5.2                |   py36habc2bb6_0        14.3 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        78.5 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               pkgs/main/linux-64::blas-1.0-openblas\n",
            "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h7b6447c_0\n",
            "  cairo              pkgs/main/linux-64::cairo-1.16.0-hf32fb01_1\n",
            "  ffmpeg             pkgs/main/linux-64::ffmpeg-3.4-h7985aa0_0\n",
            "  fontconfig         pkgs/main/linux-64::fontconfig-2.13.1-h6c09931_0\n",
            "  glib               pkgs/main/linux-64::glib-2.68.2-h36276a3_0\n",
            "  graphite2          pkgs/main/linux-64::graphite2-1.3.14-h23475e2_0\n",
            "  harfbuzz           pkgs/main/linux-64::harfbuzz-1.8.8-hffaf4a1_0\n",
            "  hdf5               pkgs/main/linux-64::hdf5-1.10.1-h9caa474_1\n",
            "  icu                pkgs/main/linux-64::icu-58.2-he6710b0_3\n",
            "  jasper             pkgs/main/linux-64::jasper-1.900.1-hd497a04_4\n",
            "  libgfortran4       pkgs/main/linux-64::libgfortran4-7.5.0-ha8ba4b0_17\n",
            "  libopus            pkgs/main/linux-64::libopus-1.3.1-h7b6447c_0\n",
            "  libprotobuf        pkgs/main/linux-64::libprotobuf-3.4.1-h5b8497f_0\n",
            "  libuuid            pkgs/main/linux-64::libuuid-1.0.3-h1bed415_2\n",
            "  libvpx             pkgs/main/linux-64::libvpx-1.7.0-h439df22_0\n",
            "  libxcb             pkgs/main/linux-64::libxcb-1.14-h7b6447c_0\n",
            "  libxml2            pkgs/main/linux-64::libxml2-2.9.10-hb55368b_3\n",
            "  numpy-base         pkgs/main/linux-64::numpy-base-1.19.2-py36h75fe3a5_0\n",
            "  opencv             pkgs/main/linux-64::opencv-3.3.1-py36h6cbbc71_1\n",
            "  pcre               pkgs/main/linux-64::pcre-8.44-he6710b0_0\n",
            "  pixman             pkgs/main/linux-64::pixman-0.40.0-h7b6447c_0\n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  libblas-3.9.0-9_openblas\n",
            "  libcblas-3.9.0-9_openblas\n",
            "  libgfortran5-9.3.0-hff62375_19\n",
            "  liblapack-3.9.0-9_openblas\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2021.5.3~ --> pkgs/main::ca-certificates-2021.5.25-h06a4308_1\n",
            "  libgfortran-ng     conda-forge::libgfortran-ng-9.3.0-hff~ --> pkgs/main::libgfortran-ng-7.5.0-ha8ba4b0_17\n",
            "  libopenblas        conda-forge::libopenblas-0.3.15-pthre~ --> pkgs/main::libopenblas-0.3.13-h4367d64_0\n",
            "  numpy              conda-forge::numpy-1.19.5-py36h2aa4a0~ --> pkgs/main::numpy-1.19.2-py36h6163131_0\n",
            "  openssl            conda-forge::openssl-1.1.1k-h7f98852_0 --> pkgs/main::openssl-1.1.1k-h27cfd23_0\n",
            "  scipy              conda-forge::scipy-1.5.3-py36h9e8f40b~ --> pkgs/main::scipy-1.5.2-py36habc2bb6_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "libxml2-2.9.10       | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  6.08it/s]\n",
            "icu-58.2             | 10.5 MB   | : 100% 1.0/1 [00:00<00:00,  2.62it/s]\n",
            "libxcb-1.14          | 505 KB    | : 100% 1.0/1 [00:00<00:00, 10.98it/s]\n",
            "libvpx-1.7.0         | 1.2 MB    | : 100% 1.0/1 [00:00<00:00, 12.02it/s]\n",
            "cairo-1.16.0         | 1.0 MB    | : 100% 1.0/1 [00:00<00:00, 10.31it/s]\n",
            "pixman-0.40.0        | 370 KB    | : 100% 1.0/1 [00:00<00:00, 13.94it/s]\n",
            "bzip2-1.0.8          | 78 KB     | : 100% 1.0/1 [00:00<00:00, 20.84it/s]\n",
            "jasper-1.900.1       | 198 KB    | : 100% 1.0/1 [00:00<00:00, 19.89it/s]\n",
            "numpy-1.19.2         | 22 KB     | : 100% 1.0/1 [00:00<00:00, 22.51it/s]\n",
            "libprotobuf-3.4.1    | 2.4 MB    | : 100% 1.0/1 [00:00<00:00,  6.23it/s]\n",
            "libgfortran-ng-7.5.0 | 22 KB     | : 100% 1.0/1 [00:00<00:00, 20.82it/s]\n",
            "blas-1.0             | 46 KB     | : 100% 1.0/1 [00:00<00:00, 21.46it/s]\n",
            "scipy-1.5.2          | 14.3 MB   | : 100% 1.0/1 [00:00<00:00,  1.74it/s]\n",
            "hdf5-1.10.1          | 3.8 MB    | : 100% 1.0/1 [00:00<00:00,  5.22it/s]\n",
            "fontconfig-2.13.1    | 250 KB    | : 100% 1.0/1 [00:00<00:00,  9.91it/s]\n",
            "pcre-8.44            | 212 KB    | : 100% 1.0/1 [00:00<00:00, 14.66it/s]\n",
            "libuuid-1.0.3        | 15 KB     | : 100% 1.0/1 [00:00<00:00, 20.96it/s]\n",
            "ffmpeg-3.4           | 7.1 MB    | : 100% 1.0/1 [00:00<00:00,  1.62it/s]               \n",
            "glib-2.68.2          | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  5.01it/s]\n",
            "libopus-1.3.1        | 491 KB    | : 100% 1.0/1 [00:00<00:00, 13.99it/s]\n",
            "graphite2-1.3.14     | 99 KB     | : 100% 1.0/1 [00:00<00:00, 19.52it/s]\n",
            "harfbuzz-1.8.8       | 507 KB    | : 100% 1.0/1 [00:00<00:00, 13.67it/s]\n",
            "numpy-base-1.19.2    | 4.1 MB    | : 100% 1.0/1 [00:00<00:00,  4.06it/s]\n",
            "opencv-3.3.1         | 21.2 MB   | : 100% 1.0/1 [00:01<00:00,  1.61s/it]              \n",
            "libgfortran4-7.5.0   | 995 KB    | : 100% 1.0/1 [00:00<00:00, 12.48it/s]\n",
            "libopenblas-0.3.13   | 4.8 MB    | : 100% 1.0/1 [00:00<00:00,  4.76it/s]\n",
            "Preparing transaction: | \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNA23_5jub_4",
        "outputId": "ab722bb8-91bd-4240-c337-a3e6277cf74e"
      },
      "source": [
        "!conda install pytorch torchvision opencv\n",
        "!pip install timm"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n",
            "Collecting timm\n",
            "  Downloading timm-0.4.9-py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.6/site-packages (from timm) (0.2.1)\n",
            "Collecting torch>=1.4\n",
            "  Downloading torch-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 10 kB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/site-packages (from torch>=1.4->timm) (3.10.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from torchvision->timm) (1.19.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/site-packages (from torchvision->timm) (8.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from torchvision->timm) (1.16.0)\n",
            "Installing collected packages: dataclasses, torch, timm\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 0.4.1.post2\n",
            "    Uninstalling torch-0.4.1.post2:\n",
            "      Successfully uninstalled torch-0.4.1.post2\n",
            "Successfully installed dataclasses-0.8 timm-0.4.9 torch-1.9.0\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1smpoSnmv_lX",
        "outputId": "56202a44-e4d9-4997-aa64-430cc136ad13"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting h5py\n",
            "  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 9.2 MB/s \n",
            "\u001b[?25hCollecting cached-property\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/site-packages (from h5py) (1.19.2)\n",
            "Installing collected packages: cached-property, h5py\n",
            "Successfully installed cached-property-1.5.2 h5py-3.1.0\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "I7d3AAmGwyu3",
        "outputId": "01f0a58e-cea9-45b8-af78-e0ee75c77f17"
      },
      "source": [
        "!pip install scikit-image"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-image\n",
            "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 169 kB/s \n",
            "\u001b[?25hCollecting networkx>=2.0\n",
            "  Downloading networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 64.9 MB/s \n",
            "\u001b[?25hCollecting imageio>=2.3.0\n",
            "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 54.7 MB/s \n",
            "\u001b[?25hCollecting tifffile>=2019.7.26\n",
            "  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
            "\u001b[K     |████████████████████████████████| 148 kB 79.7 MB/s \n",
            "\u001b[?25hCollecting matplotlib!=3.0.0,>=2.0.0\n",
            "  Downloading matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5 MB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.6/site-packages (from scikit-image) (1.19.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/site-packages (from scikit-image) (1.5.2)\n",
            "Collecting PyWavelets>=1.1.1\n",
            "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 57.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.6/site-packages (from scikit-image) (8.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.7)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 36.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.1)\n",
            "Collecting cycler>=0.10\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.16.0)\n",
            "Collecting decorator<5,>=4.3\n",
            "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: kiwisolver, decorator, cycler, tifffile, PyWavelets, networkx, matplotlib, imageio, scikit-image\n",
            "Successfully installed PyWavelets-1.1.1 cycler-0.10.0 decorator-4.4.2 imageio-2.9.0 kiwisolver-1.3.1 matplotlib-3.3.4 networkx-2.5.1 scikit-image-0.17.2 tifffile-2020.9.3\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cycler",
                  "decorator",
                  "kiwisolver"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6bncvtLw9f2",
        "outputId": "60e229dc-95e2-407c-a27a-65cc352df583"
      },
      "source": [
        "!conda install ipykernel\n",
        "!python -m ipykernel install --user"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - ipykernel\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    backcall-0.2.0             |     pyhd3eb1b0_0          13 KB\n",
            "    decorator-5.0.9            |     pyhd3eb1b0_0          12 KB\n",
            "    ipykernel-5.3.4            |   py36h5ca1d4c_0         181 KB\n",
            "    ipython-7.16.1             |   py36h5ca1d4c_0         999 KB\n",
            "    ipython_genutils-0.2.0     |     pyhd3eb1b0_1          27 KB\n",
            "    jedi-0.17.0                |           py36_0         780 KB\n",
            "    jupyter_client-6.1.12      |     pyhd3eb1b0_0          88 KB\n",
            "    jupyter_core-4.7.1         |   py36h06a4308_0          68 KB\n",
            "    libsodium-1.0.18           |       h7b6447c_0         244 KB\n",
            "    parso-0.8.2                |     pyhd3eb1b0_0          69 KB\n",
            "    pexpect-4.8.0              |     pyhd3eb1b0_3          53 KB\n",
            "    pickleshare-0.7.5          |  pyhd3eb1b0_1003          13 KB\n",
            "    prompt-toolkit-3.0.17      |     pyh06a4308_0         256 KB\n",
            "    ptyprocess-0.7.0           |     pyhd3eb1b0_2          17 KB\n",
            "    pygments-2.9.0             |     pyhd3eb1b0_0         721 KB\n",
            "    pyzmq-20.0.0               |   py36h2531618_1         438 KB\n",
            "    traitlets-4.3.3            |           py36_0         140 KB\n",
            "    wcwidth-0.2.5              |             py_0          29 KB\n",
            "    zeromq-4.3.4               |       h2531618_0         331 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         4.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  backcall           pkgs/main/noarch::backcall-0.2.0-pyhd3eb1b0_0\n",
            "  decorator          pkgs/main/noarch::decorator-5.0.9-pyhd3eb1b0_0\n",
            "  ipykernel          pkgs/main/linux-64::ipykernel-5.3.4-py36h5ca1d4c_0\n",
            "  ipython            pkgs/main/linux-64::ipython-7.16.1-py36h5ca1d4c_0\n",
            "  ipython_genutils   pkgs/main/noarch::ipython_genutils-0.2.0-pyhd3eb1b0_1\n",
            "  jedi               pkgs/main/linux-64::jedi-0.17.0-py36_0\n",
            "  jupyter_client     pkgs/main/noarch::jupyter_client-6.1.12-pyhd3eb1b0_0\n",
            "  jupyter_core       pkgs/main/linux-64::jupyter_core-4.7.1-py36h06a4308_0\n",
            "  libsodium          pkgs/main/linux-64::libsodium-1.0.18-h7b6447c_0\n",
            "  parso              pkgs/main/noarch::parso-0.8.2-pyhd3eb1b0_0\n",
            "  pexpect            pkgs/main/noarch::pexpect-4.8.0-pyhd3eb1b0_3\n",
            "  pickleshare        pkgs/main/noarch::pickleshare-0.7.5-pyhd3eb1b0_1003\n",
            "  prompt-toolkit     pkgs/main/noarch::prompt-toolkit-3.0.17-pyh06a4308_0\n",
            "  ptyprocess         pkgs/main/noarch::ptyprocess-0.7.0-pyhd3eb1b0_2\n",
            "  pygments           pkgs/main/noarch::pygments-2.9.0-pyhd3eb1b0_0\n",
            "  pyzmq              pkgs/main/linux-64::pyzmq-20.0.0-py36h2531618_1\n",
            "  traitlets          pkgs/main/linux-64::traitlets-4.3.3-py36_0\n",
            "  wcwidth            pkgs/main/noarch::wcwidth-0.2.5-py_0\n",
            "  zeromq             pkgs/main/linux-64::zeromq-4.3.4-h2531618_0\n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "ptyprocess-0.7.0     | 17 KB     | : 100% 1.0/1 [00:00<00:00, 11.42it/s]\n",
            "jupyter_client-6.1.1 | 88 KB     | : 100% 1.0/1 [00:00<00:00, 15.55it/s]\n",
            "ipykernel-5.3.4      | 181 KB    | : 100% 1.0/1 [00:00<00:00, 16.08it/s]\n",
            "pexpect-4.8.0        | 53 KB     | : 100% 1.0/1 [00:00<00:00, 23.30it/s]\n",
            "traitlets-4.3.3      | 140 KB    | : 100% 1.0/1 [00:00<00:00, 19.89it/s]\n",
            "jupyter_core-4.7.1   | 68 KB     | : 100% 1.0/1 [00:00<00:00, 23.32it/s]\n",
            "zeromq-4.3.4         | 331 KB    | : 100% 1.0/1 [00:00<00:00, 18.40it/s]\n",
            "wcwidth-0.2.5        | 29 KB     | : 100% 1.0/1 [00:00<00:00, 24.95it/s]\n",
            "jedi-0.17.0          | 780 KB    | : 100% 1.0/1 [00:00<00:00,  5.33it/s]\n",
            "ipython-7.16.1       | 999 KB    | : 100% 1.0/1 [00:00<00:00,  8.18it/s]\n",
            "backcall-0.2.0       | 13 KB     | : 100% 1.0/1 [00:00<00:00, 39.27it/s]\n",
            "libsodium-1.0.18     | 244 KB    | : 100% 1.0/1 [00:00<00:00, 12.74it/s]\n",
            "pygments-2.9.0       | 721 KB    | : 100% 1.0/1 [00:00<00:00, 12.47it/s]\n",
            "parso-0.8.2          | 69 KB     | : 100% 1.0/1 [00:00<00:00, 22.38it/s]\n",
            "pyzmq-20.0.0         | 438 KB    | : 100% 1.0/1 [00:00<00:00, 14.50it/s]\n",
            "ipython_genutils-0.2 | 27 KB     | : 100% 1.0/1 [00:00<00:00, 24.05it/s]\n",
            "prompt-toolkit-3.0.1 | 256 KB    | : 100% 1.0/1 [00:00<00:00, 19.10it/s]\n",
            "decorator-5.0.9      | 12 KB     | : 100% 1.0/1 [00:00<00:00, 25.79it/s]\n",
            "pickleshare-0.7.5    | 13 KB     | : 100% 1.0/1 [00:00<00:00, 23.54it/s]\n",
            "Preparing transaction: - \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Installed kernelspec python3 in /root/.local/share/jupyter/kernels/python3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSXEM1ivzuQF"
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAgOCqPs9S97"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}